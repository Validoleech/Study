{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgoi8i6plWlO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### О ЛР:\n",
    "\n",
    "- Coding Gradient boosting\n",
    "\n",
    "----\n",
    "\n",
    "#### Самостоятельная оценка результатов\n",
    "\n",
    "Для удобства проверки, исходя из набора решенных задач, посчитайте свою максимальную оценку (Она тут равняется 6).\n",
    "\n",
    "**Оценка**:\n",
    "\n",
    "***DeadLine - 09.01.2025 23:59***\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через lms. Вы прикрепляете **ССЫЛКУ НА ПУБЛИЧНЫЙ РЕПОЗИТОРИЙ**, где выполнено ваше задание. Иначе задание не проверяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1XDUNTn4lWlP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yvolL0KvlWlQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = load_npz(\"x.npz\")\n",
    "y = np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRMjj9ZslWlQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Разделим на обучающую, валидационную и тестовую выборки (`random_state` оставьте равным 666 для воспроизводимости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hme6Cf0HlWlR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18825, 169), (2354, 169), (2353, 169))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=666\n",
    ")\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(\n",
    "    x_test, y_test, test_size=0.5, random_state=666\n",
    ")\n",
    "\n",
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHaBuXarlWlR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация градиентного бустинга [2 балла]\n",
    "\n",
    "Необходимо дописать код в файле `boosting.py`. Уже создан шаблон класса `Boosting`, который можно модифицировать по своему усмотрению.\n",
    "\n",
    "### Описание функций:\n",
    "\n",
    "#### `__init__`\n",
    "\n",
    "Конструктор класса принимает следующие параметры:\n",
    "\n",
    "- `base_model_class` — класс базовой модели для бустинга.\n",
    "- `base_model_params` — словарь гиперпараметров для базовой модели.\n",
    "- `n_estimators` — количество базовых моделей для обучения.\n",
    "- `learning_rate` — темп обучения, должен быть в диапазоне (0, 1].\n",
    "- `subsample` — доля обучающей выборки для тренировки базовой модели (размер бутстрап-выборки относительно исходной).\n",
    "- `early_stopping_rounds` — число итераций без улучшения на валидационной выборке, после которых обучение прекращается.\n",
    "- `plot` — флаг для построения графика качества моделей после обучения.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "Метод `fit` принимает обучающую и валидационную выборки.\n",
    "\n",
    "1. Инициализируем нулевую модель и делаем предсказания (например, все нули) для обеих выборок.\n",
    "2. Обучаем `n_estimators` базовых моделей:\n",
    "   - Обучаем новую базовую модель на текущих остатках.\n",
    "   - Обновляем предсказания на обучающей и валидационной выборках.\n",
    "   - Рассчитываем ошибки на обеих выборках с помощью `loss_fn`.\n",
    "   - Проверяем условия для ранней остановки.\n",
    "\n",
    "3. Если флаг `plot` установлен, строим график качества после обучения всех моделей.\n",
    "\n",
    "#### `fit_new_base_model`\n",
    "\n",
    "Метод `fit_new_base_model` принимает обучающую выборку и текущие предсказания для неё.\n",
    "\n",
    "1. Генерируем бутстрап-выборку.\n",
    "2. Обучаем базовую модель на этой выборке.\n",
    "3. Оптимизируем значение гаммы.\n",
    "4. Добавляем новую базовую модель и гамму в соответствующие списки (учитывая `learning_rate`).\n",
    "\n",
    "#### `predict_proba`\n",
    "\n",
    "Метод `predict_proba` принимает выборку для предсказания вероятностей.\n",
    "\n",
    "1. Суммируем предсказания базовых моделей (учитывая гамму и `learning_rate`).\n",
    "2. Применяем сигмоидальную функцию для получения вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xh3nawbUlWlS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "reqHbUEBlWlS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from boosting import Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7wWK5RplWlT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Проверка кода\n",
    "\n",
    "У автора задания всё учится около одной секунды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "00lZjzI3lWlT",
    "outputId": "c1aa6886-069b-433f-f0c1-5cf70bdb15cb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6551; Validation Loss = 0.6590\n",
      "Iteration 2: Train Loss = 0.6219; Validation Loss = 0.6298\n",
      "Iteration 3: Train Loss = 0.5932; Validation Loss = 0.6051\n",
      "Iteration 4: Train Loss = 0.5681; Validation Loss = 0.5840\n",
      "Iteration 5: Train Loss = 0.5463; Validation Loss = 0.5648\n",
      "Iteration 6: Train Loss = 0.5271; Validation Loss = 0.5490\n",
      "Iteration 7: Train Loss = 0.5101; Validation Loss = 0.5354\n",
      "Iteration 8: Train Loss = 0.4951; Validation Loss = 0.5221\n",
      "Iteration 9: Train Loss = 0.4817; Validation Loss = 0.5111\n",
      "Iteration 10: Train Loss = 0.4699; Validation Loss = 0.5019\n",
      "CPU times: total: 1.44 s\n",
      "Wall time: 1.43 s\n",
      "Train ROC-AUC 0.9881\n",
      "Valid ROC-AUC 0.9408\n",
      "Test ROC-AUC 0.9434\n"
     ]
    }
   ],
   "source": [
    "boosting = Boosting()\n",
    "\n",
    "%time boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "assert len(boosting.models) == boosting.n_estimators\n",
    "assert len(boosting.gammas) == boosting.n_estimators\n",
    "\n",
    "assert boosting.predict_proba(x_test).shape == (x_test.shape[0], 2)\n",
    "\n",
    "print(f'Train ROC-AUC {boosting.score(x_train, y_train):.4f}')\n",
    "print(f'Valid ROC-AUC {boosting.score(x_valid, y_valid):.4f}')\n",
    "print(f'Test ROC-AUC {boosting.score(x_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlU-c9CxlWlU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 2. Обучение градиентного бустинга [0.5 балла]\n",
    "\n",
    "Оцените качество вашей реализации градиентного бустинга на тестовой выборке, используя базовые модели — решающие деревья с различной максимальной глубиной. Метрикой будет ROC-AUC.\n",
    "\n",
    "**Инструкция:**\n",
    "1. Перебирайте значения максимальной глубины деревьев от 1 до 30 с шагом 2.\n",
    "2. Оставьте остальные параметры бустинга по умолчанию.\n",
    "3. Постройте график зависимости качества на обучающей и тестовой выборке от максимальной глубины деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-IjO9FqelWlU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6708; Validation Loss = 0.6714\n",
      "Iteration 2: Train Loss = 0.6517; Validation Loss = 0.6528\n",
      "Iteration 3: Train Loss = 0.6354; Validation Loss = 0.6373\n",
      "Iteration 4: Train Loss = 0.6211; Validation Loss = 0.6234\n",
      "Iteration 5: Train Loss = 0.6090; Validation Loss = 0.6115\n",
      "Iteration 6: Train Loss = 0.5979; Validation Loss = 0.6007\n",
      "Iteration 7: Train Loss = 0.5885; Validation Loss = 0.5915\n",
      "Iteration 8: Train Loss = 0.5802; Validation Loss = 0.5837\n",
      "Iteration 9: Train Loss = 0.5727; Validation Loss = 0.5762\n",
      "Iteration 10: Train Loss = 0.5660; Validation Loss = 0.5697\n",
      "Iteration 1: Train Loss = 0.6646; Validation Loss = 0.6652\n",
      "Iteration 2: Train Loss = 0.6402; Validation Loss = 0.6415\n",
      "Iteration 3: Train Loss = 0.6188; Validation Loss = 0.6203\n",
      "Iteration 4: Train Loss = 0.6003; Validation Loss = 0.6022\n",
      "Iteration 5: Train Loss = 0.5838; Validation Loss = 0.5864\n",
      "Iteration 6: Train Loss = 0.5695; Validation Loss = 0.5722\n",
      "Iteration 7: Train Loss = 0.5574; Validation Loss = 0.5605\n",
      "Iteration 8: Train Loss = 0.5467; Validation Loss = 0.5501\n",
      "Iteration 9: Train Loss = 0.5370; Validation Loss = 0.5405\n",
      "Iteration 10: Train Loss = 0.5283; Validation Loss = 0.5320\n",
      "Iteration 1: Train Loss = 0.6618; Validation Loss = 0.6623\n",
      "Iteration 2: Train Loss = 0.6348; Validation Loss = 0.6360\n",
      "Iteration 3: Train Loss = 0.6117; Validation Loss = 0.6134\n",
      "Iteration 4: Train Loss = 0.5909; Validation Loss = 0.5931\n",
      "Iteration 5: Train Loss = 0.5732; Validation Loss = 0.5757\n",
      "Iteration 6: Train Loss = 0.5574; Validation Loss = 0.5604\n",
      "Iteration 7: Train Loss = 0.5435; Validation Loss = 0.5470\n",
      "Iteration 8: Train Loss = 0.5312; Validation Loss = 0.5353\n",
      "Iteration 9: Train Loss = 0.5205; Validation Loss = 0.5248\n",
      "Iteration 10: Train Loss = 0.5111; Validation Loss = 0.5157\n",
      "Iteration 1: Train Loss = 0.6601; Validation Loss = 0.6611\n",
      "Iteration 2: Train Loss = 0.6314; Validation Loss = 0.6333\n",
      "Iteration 3: Train Loss = 0.6065; Validation Loss = 0.6093\n",
      "Iteration 4: Train Loss = 0.5853; Validation Loss = 0.5886\n",
      "Iteration 5: Train Loss = 0.5659; Validation Loss = 0.5701\n",
      "Iteration 6: Train Loss = 0.5495; Validation Loss = 0.5541\n",
      "Iteration 7: Train Loss = 0.5352; Validation Loss = 0.5408\n",
      "Iteration 8: Train Loss = 0.5223; Validation Loss = 0.5287\n",
      "Iteration 9: Train Loss = 0.5110; Validation Loss = 0.5182\n",
      "Iteration 10: Train Loss = 0.5011; Validation Loss = 0.5083\n",
      "Iteration 1: Train Loss = 0.6581; Validation Loss = 0.6598\n",
      "Iteration 2: Train Loss = 0.6279; Validation Loss = 0.6309\n",
      "Iteration 3: Train Loss = 0.6022; Validation Loss = 0.6066\n",
      "Iteration 4: Train Loss = 0.5797; Validation Loss = 0.5853\n",
      "Iteration 5: Train Loss = 0.5601; Validation Loss = 0.5666\n",
      "Iteration 6: Train Loss = 0.5434; Validation Loss = 0.5506\n",
      "Iteration 7: Train Loss = 0.5285; Validation Loss = 0.5364\n",
      "Iteration 8: Train Loss = 0.5151; Validation Loss = 0.5243\n",
      "Iteration 9: Train Loss = 0.5034; Validation Loss = 0.5139\n",
      "Iteration 10: Train Loss = 0.4930; Validation Loss = 0.5045\n",
      "Iteration 1: Train Loss = 0.6575; Validation Loss = 0.6598\n",
      "Iteration 2: Train Loss = 0.6266; Validation Loss = 0.6307\n",
      "Iteration 3: Train Loss = 0.5990; Validation Loss = 0.6049\n",
      "Iteration 4: Train Loss = 0.5755; Validation Loss = 0.5831\n",
      "Iteration 5: Train Loss = 0.5552; Validation Loss = 0.5641\n",
      "Iteration 6: Train Loss = 0.5386; Validation Loss = 0.5483\n",
      "Iteration 7: Train Loss = 0.5229; Validation Loss = 0.5341\n",
      "Iteration 8: Train Loss = 0.5093; Validation Loss = 0.5214\n",
      "Iteration 9: Train Loss = 0.4969; Validation Loss = 0.5107\n",
      "Iteration 10: Train Loss = 0.4858; Validation Loss = 0.5007\n",
      "Iteration 1: Train Loss = 0.6567; Validation Loss = 0.6593\n",
      "Iteration 2: Train Loss = 0.6249; Validation Loss = 0.6303\n",
      "Iteration 3: Train Loss = 0.5972; Validation Loss = 0.6052\n",
      "Iteration 4: Train Loss = 0.5736; Validation Loss = 0.5835\n",
      "Iteration 5: Train Loss = 0.5525; Validation Loss = 0.5646\n",
      "Iteration 6: Train Loss = 0.5341; Validation Loss = 0.5483\n",
      "Iteration 7: Train Loss = 0.5178; Validation Loss = 0.5333\n",
      "Iteration 8: Train Loss = 0.5039; Validation Loss = 0.5214\n",
      "Iteration 9: Train Loss = 0.4913; Validation Loss = 0.5095\n",
      "Iteration 10: Train Loss = 0.4804; Validation Loss = 0.5008\n",
      "Iteration 1: Train Loss = 0.6560; Validation Loss = 0.6590\n",
      "Iteration 2: Train Loss = 0.6241; Validation Loss = 0.6298\n",
      "Iteration 3: Train Loss = 0.5963; Validation Loss = 0.6052\n",
      "Iteration 4: Train Loss = 0.5719; Validation Loss = 0.5833\n",
      "Iteration 5: Train Loss = 0.5503; Validation Loss = 0.5642\n",
      "Iteration 6: Train Loss = 0.5321; Validation Loss = 0.5484\n",
      "Iteration 7: Train Loss = 0.5161; Validation Loss = 0.5343\n",
      "Iteration 8: Train Loss = 0.5018; Validation Loss = 0.5218\n",
      "Iteration 9: Train Loss = 0.4889; Validation Loss = 0.5112\n",
      "Iteration 10: Train Loss = 0.4778; Validation Loss = 0.5022\n",
      "Iteration 1: Train Loss = 0.6556; Validation Loss = 0.6588\n",
      "Iteration 2: Train Loss = 0.6231; Validation Loss = 0.6300\n",
      "Iteration 3: Train Loss = 0.5946; Validation Loss = 0.6049\n",
      "Iteration 4: Train Loss = 0.5701; Validation Loss = 0.5831\n",
      "Iteration 5: Train Loss = 0.5489; Validation Loss = 0.5644\n",
      "Iteration 6: Train Loss = 0.5304; Validation Loss = 0.5482\n",
      "Iteration 7: Train Loss = 0.5137; Validation Loss = 0.5337\n",
      "Iteration 8: Train Loss = 0.4992; Validation Loss = 0.5214\n",
      "Iteration 9: Train Loss = 0.4867; Validation Loss = 0.5107\n",
      "Iteration 10: Train Loss = 0.4753; Validation Loss = 0.5011\n",
      "Iteration 1: Train Loss = 0.6554; Validation Loss = 0.6588\n",
      "Iteration 2: Train Loss = 0.6226; Validation Loss = 0.6294\n",
      "Iteration 3: Train Loss = 0.5941; Validation Loss = 0.6035\n",
      "Iteration 4: Train Loss = 0.5697; Validation Loss = 0.5817\n",
      "Iteration 5: Train Loss = 0.5481; Validation Loss = 0.5624\n",
      "Iteration 6: Train Loss = 0.5295; Validation Loss = 0.5460\n",
      "Iteration 7: Train Loss = 0.5128; Validation Loss = 0.5314\n",
      "Iteration 8: Train Loss = 0.4981; Validation Loss = 0.5194\n",
      "Iteration 9: Train Loss = 0.4849; Validation Loss = 0.5077\n",
      "Iteration 10: Train Loss = 0.4735; Validation Loss = 0.4975\n",
      "Iteration 1: Train Loss = 0.6552; Validation Loss = 0.6580\n",
      "Iteration 2: Train Loss = 0.6226; Validation Loss = 0.6284\n",
      "Iteration 3: Train Loss = 0.5941; Validation Loss = 0.6030\n",
      "Iteration 4: Train Loss = 0.5695; Validation Loss = 0.5817\n",
      "Iteration 5: Train Loss = 0.5480; Validation Loss = 0.5630\n",
      "Iteration 6: Train Loss = 0.5287; Validation Loss = 0.5469\n",
      "Iteration 7: Train Loss = 0.5115; Validation Loss = 0.5326\n",
      "Iteration 8: Train Loss = 0.4970; Validation Loss = 0.5198\n",
      "Iteration 9: Train Loss = 0.4841; Validation Loss = 0.5091\n",
      "Iteration 10: Train Loss = 0.4723; Validation Loss = 0.4990\n",
      "Iteration 1: Train Loss = 0.6555; Validation Loss = 0.6592\n",
      "Iteration 2: Train Loss = 0.6221; Validation Loss = 0.6284\n",
      "Iteration 3: Train Loss = 0.5932; Validation Loss = 0.6031\n",
      "Iteration 4: Train Loss = 0.5681; Validation Loss = 0.5802\n",
      "Iteration 5: Train Loss = 0.5463; Validation Loss = 0.5613\n",
      "Iteration 6: Train Loss = 0.5270; Validation Loss = 0.5443\n",
      "Iteration 7: Train Loss = 0.5102; Validation Loss = 0.5303\n",
      "Iteration 8: Train Loss = 0.4956; Validation Loss = 0.5185\n",
      "Iteration 9: Train Loss = 0.4820; Validation Loss = 0.5077\n",
      "Iteration 10: Train Loss = 0.4704; Validation Loss = 0.4987\n",
      "Iteration 1: Train Loss = 0.6552; Validation Loss = 0.6590\n",
      "Iteration 2: Train Loss = 0.6225; Validation Loss = 0.6293\n",
      "Iteration 3: Train Loss = 0.5937; Validation Loss = 0.6035\n",
      "Iteration 4: Train Loss = 0.5690; Validation Loss = 0.5821\n",
      "Iteration 5: Train Loss = 0.5470; Validation Loss = 0.5633\n",
      "Iteration 6: Train Loss = 0.5281; Validation Loss = 0.5478\n",
      "Iteration 7: Train Loss = 0.5110; Validation Loss = 0.5335\n",
      "Iteration 8: Train Loss = 0.4961; Validation Loss = 0.5207\n",
      "Iteration 9: Train Loss = 0.4829; Validation Loss = 0.5099\n",
      "Iteration 10: Train Loss = 0.4711; Validation Loss = 0.5010\n",
      "Iteration 1: Train Loss = 0.6553; Validation Loss = 0.6596\n",
      "Iteration 2: Train Loss = 0.6226; Validation Loss = 0.6308\n",
      "Iteration 3: Train Loss = 0.5935; Validation Loss = 0.6049\n",
      "Iteration 4: Train Loss = 0.5687; Validation Loss = 0.5836\n",
      "Iteration 5: Train Loss = 0.5465; Validation Loss = 0.5647\n",
      "Iteration 6: Train Loss = 0.5275; Validation Loss = 0.5489\n",
      "Iteration 7: Train Loss = 0.5107; Validation Loss = 0.5347\n",
      "Iteration 8: Train Loss = 0.4955; Validation Loss = 0.5223\n",
      "Iteration 9: Train Loss = 0.4820; Validation Loss = 0.5103\n",
      "Iteration 10: Train Loss = 0.4699; Validation Loss = 0.5004\n",
      "Iteration 1: Train Loss = 0.6551; Validation Loss = 0.6583\n",
      "Iteration 2: Train Loss = 0.6220; Validation Loss = 0.6284\n",
      "Iteration 3: Train Loss = 0.5932; Validation Loss = 0.6036\n",
      "Iteration 4: Train Loss = 0.5682; Validation Loss = 0.5821\n",
      "Iteration 5: Train Loss = 0.5459; Validation Loss = 0.5624\n",
      "Iteration 6: Train Loss = 0.5265; Validation Loss = 0.5454\n",
      "Iteration 7: Train Loss = 0.5095; Validation Loss = 0.5309\n",
      "Iteration 8: Train Loss = 0.4947; Validation Loss = 0.5182\n",
      "Iteration 9: Train Loss = 0.4814; Validation Loss = 0.5078\n",
      "Iteration 10: Train Loss = 0.4696; Validation Loss = 0.4977\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "depths = range(1, 30, 2)\n",
    "\n",
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "for depth in depths:\n",
    "    boosting = Boosting(\n",
    "        base_model_params={'max_depth': depth}, \n",
    "        n_estimators=10, \n",
    "        learning_rate=0.1, \n",
    "        subsample=0.3, early_stopping_rounds=None, \n",
    "        plot=False\n",
    "        )\n",
    "\n",
    "    boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "    train_roc_auc = boosting.score(x_train, y_train)\n",
    "    test_roc_auc = boosting.score(x_test, y_test)\n",
    "\n",
    "    results[depth] = (train_roc_auc, test_roc_auc)\n",
    "\n",
    "train_scores = [results[d][0] for d in depths]\n",
    "test_scores = [results[d][1] for d in depths]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_scores, label='Train AUC')\n",
    "plt.plot(depths, test_scores, label='Test AUC')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.title('Gradient Boosting Performance vs Max Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bshz0JV6lWlV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Какая из моделей имеет лучшее качество? Как вы можете это объяснить?**\n",
    "\n",
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "Лучшее качество на тестовой выборке достигается при максимальной глубине дерева около 10, так как до этого она растёт.\n",
    "\n",
    "Где-то после 11-13 тестовая выборка начинает падение, что показывает, что у нас начинается оверфиттинг и подстраивание под шум и специфические данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FwaUsmqlWlV"
   },
   "source": [
    "## Задание 3. Подбор гиперпараметров и поиск оптимальной модели [3 балла]\n",
    "\n",
    "Настройте основные гиперпараметры вашей модели градиентного бустинга, используя валидационную выборку. Подберите параметры как для самого бустинга, так и для базовых моделей.\n",
    "\n",
    "**Рекомендации:**\n",
    "- Используйте библиотеки для автоматизированного подбора гиперпараметров, такие как [Hyperopt](https://github.com/hyperopt/hyperopt) или [Optuna](https://optuna.org/).\n",
    "- Подберите все основные параметры, чтобы найти лучшую модель на валидационной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rZq0rKpWlWlV",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:29:03,834] A new study created in memory with name: no-name-c7f75dbf-dc53-4a7d-bc1e-abf10ce4bb18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6381; Validation Loss = 0.6392\n",
      "Iteration 2: Train Loss = 0.5972; Validation Loss = 0.5992\n",
      "Iteration 3: Train Loss = 0.5660; Validation Loss = 0.5689\n",
      "Iteration 4: Train Loss = 0.5414; Validation Loss = 0.5450\n",
      "Iteration 5: Train Loss = 0.5220; Validation Loss = 0.5260\n",
      "Iteration 6: Train Loss = 0.5064; Validation Loss = 0.5105\n",
      "Iteration 7: Train Loss = 0.4945; Validation Loss = 0.4991\n",
      "Iteration 8: Train Loss = 0.4845; Validation Loss = 0.4890\n",
      "Iteration 9: Train Loss = 0.4763; Validation Loss = 0.4809\n",
      "Iteration 10: Train Loss = 0.4690; Validation Loss = 0.4737\n",
      "Iteration 11: Train Loss = 0.4630; Validation Loss = 0.4682\n",
      "Iteration 12: Train Loss = 0.4582; Validation Loss = 0.4638\n",
      "Iteration 13: Train Loss = 0.4536; Validation Loss = 0.4593\n",
      "Iteration 14: Train Loss = 0.4499; Validation Loss = 0.4558\n",
      "Iteration 15: Train Loss = 0.4467; Validation Loss = 0.4522\n",
      "Iteration 16: Train Loss = 0.4441; Validation Loss = 0.4494\n",
      "Iteration 17: Train Loss = 0.4420; Validation Loss = 0.4470\n",
      "Iteration 18: Train Loss = 0.4407; Validation Loss = 0.4458\n",
      "Iteration 19: Train Loss = 0.4390; Validation Loss = 0.4441\n",
      "Iteration 20: Train Loss = 0.4373; Validation Loss = 0.4426\n",
      "Iteration 21: Train Loss = 0.4361; Validation Loss = 0.4416\n",
      "Iteration 22: Train Loss = 0.4349; Validation Loss = 0.4406\n",
      "Iteration 23: Train Loss = 0.4336; Validation Loss = 0.4394\n",
      "Iteration 24: Train Loss = 0.4329; Validation Loss = 0.4386\n",
      "Iteration 25: Train Loss = 0.4316; Validation Loss = 0.4376\n",
      "Iteration 26: Train Loss = 0.4312; Validation Loss = 0.4372\n",
      "Iteration 27: Train Loss = 0.4306; Validation Loss = 0.4366\n",
      "Iteration 28: Train Loss = 0.4300; Validation Loss = 0.4360\n",
      "Iteration 29: Train Loss = 0.4297; Validation Loss = 0.4360\n",
      "Iteration 30: Train Loss = 0.4293; Validation Loss = 0.4356\n",
      "Iteration 31: Train Loss = 0.4282; Validation Loss = 0.4345\n",
      "Iteration 32: Train Loss = 0.4280; Validation Loss = 0.4342\n",
      "Iteration 33: Train Loss = 0.4272; Validation Loss = 0.4337\n",
      "Iteration 34: Train Loss = 0.4269; Validation Loss = 0.4335\n",
      "Iteration 35: Train Loss = 0.4267; Validation Loss = 0.4334\n",
      "Iteration 36: Train Loss = 0.4263; Validation Loss = 0.4331\n",
      "Iteration 37: Train Loss = 0.4260; Validation Loss = 0.4329\n",
      "Iteration 38: Train Loss = 0.4252; Validation Loss = 0.4321\n",
      "Iteration 39: Train Loss = 0.4250; Validation Loss = 0.4323\n",
      "Iteration 40: Train Loss = 0.4249; Validation Loss = 0.4321\n",
      "Iteration 41: Train Loss = 0.4247; Validation Loss = 0.4319\n",
      "Iteration 42: Train Loss = 0.4243; Validation Loss = 0.4315\n",
      "Iteration 43: Train Loss = 0.4239; Validation Loss = 0.4314\n",
      "Iteration 44: Train Loss = 0.4238; Validation Loss = 0.4312\n",
      "Iteration 45: Train Loss = 0.4232; Validation Loss = 0.4308\n",
      "Iteration 46: Train Loss = 0.4231; Validation Loss = 0.4309\n",
      "Iteration 47: Train Loss = 0.4226; Validation Loss = 0.4306\n",
      "Iteration 48: Train Loss = 0.4224; Validation Loss = 0.4304\n",
      "Iteration 49: Train Loss = 0.4216; Validation Loss = 0.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:29:09,059] Trial 0 finished with value: 0.9630955240611738 and parameters: {'max_depth': 4, 'min_samples_leaf': 7, 'n_estimators': 50, 'learning_rate': 0.18668681026667652, 'subsample': 0.9409123279722404}. Best is trial 0 with value: 0.9630955240611738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: Train Loss = 0.4215; Validation Loss = 0.4298\n",
      "Iteration 1: Train Loss = 0.6702; Validation Loss = 0.6710\n",
      "Iteration 2: Train Loss = 0.6495; Validation Loss = 0.6510\n",
      "Iteration 3: Train Loss = 0.6305; Validation Loss = 0.6326\n",
      "Iteration 4: Train Loss = 0.6136; Validation Loss = 0.6160\n",
      "Iteration 5: Train Loss = 0.5983; Validation Loss = 0.6011\n",
      "Iteration 6: Train Loss = 0.5840; Validation Loss = 0.5874\n",
      "Iteration 7: Train Loss = 0.5713; Validation Loss = 0.5751\n",
      "Iteration 8: Train Loss = 0.5595; Validation Loss = 0.5637\n",
      "Iteration 9: Train Loss = 0.5487; Validation Loss = 0.5532\n",
      "Iteration 10: Train Loss = 0.5387; Validation Loss = 0.5434\n",
      "Iteration 11: Train Loss = 0.5295; Validation Loss = 0.5348\n",
      "Iteration 12: Train Loss = 0.5211; Validation Loss = 0.5268\n",
      "Iteration 13: Train Loss = 0.5135; Validation Loss = 0.5198\n",
      "Iteration 14: Train Loss = 0.5066; Validation Loss = 0.5132\n",
      "Iteration 15: Train Loss = 0.5001; Validation Loss = 0.5071\n",
      "Iteration 16: Train Loss = 0.4940; Validation Loss = 0.5013\n",
      "Iteration 17: Train Loss = 0.4884; Validation Loss = 0.4961\n",
      "Iteration 18: Train Loss = 0.4831; Validation Loss = 0.4911\n",
      "Iteration 19: Train Loss = 0.4784; Validation Loss = 0.4869\n",
      "Iteration 20: Train Loss = 0.4739; Validation Loss = 0.4828\n",
      "Iteration 21: Train Loss = 0.4697; Validation Loss = 0.4788\n",
      "Iteration 22: Train Loss = 0.4657; Validation Loss = 0.4751\n",
      "Iteration 23: Train Loss = 0.4622; Validation Loss = 0.4718\n",
      "Iteration 24: Train Loss = 0.4589; Validation Loss = 0.4688\n",
      "Iteration 25: Train Loss = 0.4559; Validation Loss = 0.4661\n",
      "Iteration 26: Train Loss = 0.4528; Validation Loss = 0.4633\n",
      "Iteration 27: Train Loss = 0.4502; Validation Loss = 0.4609\n",
      "Iteration 28: Train Loss = 0.4477; Validation Loss = 0.4585\n",
      "Iteration 29: Train Loss = 0.4454; Validation Loss = 0.4565\n",
      "Iteration 30: Train Loss = 0.4432; Validation Loss = 0.4547\n",
      "Iteration 31: Train Loss = 0.4411; Validation Loss = 0.4527\n",
      "Iteration 32: Train Loss = 0.4390; Validation Loss = 0.4509\n",
      "Iteration 33: Train Loss = 0.4370; Validation Loss = 0.4491\n",
      "Iteration 34: Train Loss = 0.4353; Validation Loss = 0.4476\n",
      "Iteration 35: Train Loss = 0.4336; Validation Loss = 0.4462\n",
      "Iteration 36: Train Loss = 0.4322; Validation Loss = 0.4451\n",
      "Iteration 37: Train Loss = 0.4307; Validation Loss = 0.4437\n",
      "Iteration 38: Train Loss = 0.4294; Validation Loss = 0.4426\n",
      "Iteration 39: Train Loss = 0.4281; Validation Loss = 0.4414\n",
      "Iteration 40: Train Loss = 0.4271; Validation Loss = 0.4405\n",
      "Iteration 41: Train Loss = 0.4260; Validation Loss = 0.4395\n",
      "Iteration 42: Train Loss = 0.4249; Validation Loss = 0.4384\n",
      "Iteration 43: Train Loss = 0.4239; Validation Loss = 0.4376\n",
      "Iteration 44: Train Loss = 0.4230; Validation Loss = 0.4369\n",
      "Iteration 45: Train Loss = 0.4220; Validation Loss = 0.4361\n",
      "Iteration 46: Train Loss = 0.4212; Validation Loss = 0.4356\n",
      "Iteration 47: Train Loss = 0.4205; Validation Loss = 0.4350\n",
      "Iteration 48: Train Loss = 0.4198; Validation Loss = 0.4344\n",
      "Iteration 49: Train Loss = 0.4192; Validation Loss = 0.4339\n",
      "Iteration 50: Train Loss = 0.4183; Validation Loss = 0.4330\n",
      "Iteration 51: Train Loss = 0.4175; Validation Loss = 0.4322\n",
      "Iteration 52: Train Loss = 0.4169; Validation Loss = 0.4318\n",
      "Iteration 53: Train Loss = 0.4164; Validation Loss = 0.4314\n",
      "Iteration 54: Train Loss = 0.4159; Validation Loss = 0.4311\n",
      "Iteration 55: Train Loss = 0.4152; Validation Loss = 0.4305\n",
      "Iteration 56: Train Loss = 0.4148; Validation Loss = 0.4302\n",
      "Iteration 57: Train Loss = 0.4142; Validation Loss = 0.4298\n",
      "Iteration 58: Train Loss = 0.4138; Validation Loss = 0.4293\n",
      "Iteration 59: Train Loss = 0.4133; Validation Loss = 0.4288\n",
      "Iteration 60: Train Loss = 0.4130; Validation Loss = 0.4285\n",
      "Iteration 61: Train Loss = 0.4127; Validation Loss = 0.4282\n",
      "Iteration 62: Train Loss = 0.4124; Validation Loss = 0.4281\n",
      "Iteration 63: Train Loss = 0.4120; Validation Loss = 0.4278\n",
      "Iteration 64: Train Loss = 0.4117; Validation Loss = 0.4275\n",
      "Iteration 65: Train Loss = 0.4115; Validation Loss = 0.4273\n",
      "Iteration 66: Train Loss = 0.4112; Validation Loss = 0.4270\n",
      "Iteration 67: Train Loss = 0.4110; Validation Loss = 0.4268\n",
      "Iteration 68: Train Loss = 0.4106; Validation Loss = 0.4264\n",
      "Iteration 69: Train Loss = 0.4105; Validation Loss = 0.4263\n",
      "Iteration 70: Train Loss = 0.4102; Validation Loss = 0.4260\n",
      "Iteration 71: Train Loss = 0.4101; Validation Loss = 0.4259\n",
      "Iteration 72: Train Loss = 0.4098; Validation Loss = 0.4256\n",
      "Iteration 73: Train Loss = 0.4097; Validation Loss = 0.4254\n",
      "Iteration 74: Train Loss = 0.4095; Validation Loss = 0.4254\n",
      "Iteration 75: Train Loss = 0.4094; Validation Loss = 0.4253\n",
      "Iteration 76: Train Loss = 0.4091; Validation Loss = 0.4252\n",
      "Iteration 77: Train Loss = 0.4088; Validation Loss = 0.4250\n",
      "Iteration 78: Train Loss = 0.4086; Validation Loss = 0.4248\n",
      "Iteration 79: Train Loss = 0.4085; Validation Loss = 0.4248\n",
      "Iteration 80: Train Loss = 0.4083; Validation Loss = 0.4246\n",
      "Iteration 81: Train Loss = 0.4081; Validation Loss = 0.4245\n",
      "Iteration 82: Train Loss = 0.4079; Validation Loss = 0.4244\n",
      "Iteration 83: Train Loss = 0.4076; Validation Loss = 0.4243\n",
      "Iteration 84: Train Loss = 0.4075; Validation Loss = 0.4241\n",
      "Iteration 85: Train Loss = 0.4073; Validation Loss = 0.4240\n",
      "Iteration 86: Train Loss = 0.4071; Validation Loss = 0.4238\n",
      "Iteration 87: Train Loss = 0.4070; Validation Loss = 0.4238\n",
      "Iteration 88: Train Loss = 0.4068; Validation Loss = 0.4237\n",
      "Iteration 89: Train Loss = 0.4067; Validation Loss = 0.4236\n",
      "Iteration 90: Train Loss = 0.4064; Validation Loss = 0.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:29:17,567] Trial 1 finished with value: 0.9648200892976295 and parameters: {'max_depth': 7, 'min_samples_leaf': 5, 'n_estimators': 90, 'learning_rate': 0.06830870104262245, 'subsample': 0.6198369399840657}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.5747; Validation Loss = 0.5776\n",
      "Iteration 2: Train Loss = 0.5197; Validation Loss = 0.5224\n",
      "Iteration 3: Train Loss = 0.4886; Validation Loss = 0.4913\n",
      "Iteration 4: Train Loss = 0.4715; Validation Loss = 0.4754\n",
      "Iteration 5: Train Loss = 0.4600; Validation Loss = 0.4628\n",
      "Iteration 6: Train Loss = 0.4513; Validation Loss = 0.4542\n",
      "Iteration 7: Train Loss = 0.4472; Validation Loss = 0.4503\n",
      "Iteration 8: Train Loss = 0.4432; Validation Loss = 0.4466\n",
      "Iteration 9: Train Loss = 0.4423; Validation Loss = 0.4451\n",
      "Iteration 10: Train Loss = 0.4412; Validation Loss = 0.4443\n",
      "Iteration 11: Train Loss = 0.4390; Validation Loss = 0.4423\n",
      "Iteration 12: Train Loss = 0.4367; Validation Loss = 0.4401\n",
      "Iteration 13: Train Loss = 0.4354; Validation Loss = 0.4389\n",
      "Iteration 14: Train Loss = 0.4351; Validation Loss = 0.4385\n",
      "Iteration 15: Train Loss = 0.4344; Validation Loss = 0.4380\n",
      "Iteration 16: Train Loss = 0.4336; Validation Loss = 0.4375\n",
      "Iteration 17: Train Loss = 0.4335; Validation Loss = 0.4380\n",
      "Iteration 18: Train Loss = 0.4332; Validation Loss = 0.4380\n",
      "Iteration 19: Train Loss = 0.4327; Validation Loss = 0.4380\n",
      "Iteration 20: Train Loss = 0.4327; Validation Loss = 0.4382\n",
      "Iteration 21: Train Loss = 0.4319; Validation Loss = 0.4377\n",
      "Iteration 22: Train Loss = 0.4313; Validation Loss = 0.4374\n",
      "Iteration 23: Train Loss = 0.4303; Validation Loss = 0.4359\n",
      "Iteration 24: Train Loss = 0.4296; Validation Loss = 0.4355\n",
      "Iteration 25: Train Loss = 0.4292; Validation Loss = 0.4349\n",
      "Iteration 26: Train Loss = 0.4279; Validation Loss = 0.4336\n",
      "Iteration 27: Train Loss = 0.4271; Validation Loss = 0.4330\n",
      "Iteration 28: Train Loss = 0.4261; Validation Loss = 0.4317\n",
      "Iteration 29: Train Loss = 0.4252; Validation Loss = 0.4309\n",
      "Iteration 30: Train Loss = 0.4234; Validation Loss = 0.4291\n",
      "Iteration 31: Train Loss = 0.4221; Validation Loss = 0.4279\n",
      "Iteration 32: Train Loss = 0.4218; Validation Loss = 0.4276\n",
      "Iteration 33: Train Loss = 0.4216; Validation Loss = 0.4275\n",
      "Iteration 34: Train Loss = 0.4216; Validation Loss = 0.4275\n",
      "Iteration 35: Train Loss = 0.4216; Validation Loss = 0.4275\n",
      "Iteration 36: Train Loss = 0.4216; Validation Loss = 0.4275\n",
      "Iteration 37: Train Loss = 0.4216; Validation Loss = 0.4275\n",
      "Iteration 38: Train Loss = 0.4216; Validation Loss = 0.4277\n",
      "Iteration 39: Train Loss = 0.4212; Validation Loss = 0.4271\n",
      "Iteration 40: Train Loss = 0.4212; Validation Loss = 0.4271\n",
      "Iteration 41: Train Loss = 0.4211; Validation Loss = 0.4270\n",
      "Iteration 42: Train Loss = 0.4211; Validation Loss = 0.4270\n",
      "Iteration 43: Train Loss = 0.4211; Validation Loss = 0.4269\n",
      "Iteration 44: Train Loss = 0.4213; Validation Loss = 0.4270\n",
      "Iteration 45: Train Loss = 0.4209; Validation Loss = 0.4268\n",
      "Iteration 46: Train Loss = 0.4206; Validation Loss = 0.4268\n",
      "Iteration 47: Train Loss = 0.4206; Validation Loss = 0.4269\n",
      "Iteration 48: Train Loss = 0.4208; Validation Loss = 0.4271\n",
      "Iteration 49: Train Loss = 0.4204; Validation Loss = 0.4266\n",
      "Iteration 50: Train Loss = 0.4205; Validation Loss = 0.4266\n",
      "Iteration 51: Train Loss = 0.4205; Validation Loss = 0.4266\n",
      "Iteration 52: Train Loss = 0.4204; Validation Loss = 0.4265\n",
      "Iteration 53: Train Loss = 0.4204; Validation Loss = 0.4265\n",
      "Iteration 54: Train Loss = 0.4204; Validation Loss = 0.4265\n",
      "Iteration 55: Train Loss = 0.4203; Validation Loss = 0.4265\n",
      "Iteration 56: Train Loss = 0.4203; Validation Loss = 0.4265\n",
      "Iteration 57: Train Loss = 0.4203; Validation Loss = 0.4266\n",
      "Iteration 58: Train Loss = 0.4204; Validation Loss = 0.4267\n",
      "Iteration 59: Train Loss = 0.4203; Validation Loss = 0.4265\n",
      "Iteration 60: Train Loss = 0.4203; Validation Loss = 0.4265\n",
      "Iteration 61: Train Loss = 0.4202; Validation Loss = 0.4263\n",
      "Iteration 62: Train Loss = 0.4202; Validation Loss = 0.4264\n",
      "Iteration 63: Train Loss = 0.4202; Validation Loss = 0.4262\n",
      "Iteration 64: Train Loss = 0.4200; Validation Loss = 0.4263\n",
      "Iteration 65: Train Loss = 0.4198; Validation Loss = 0.4260\n",
      "Iteration 66: Train Loss = 0.4198; Validation Loss = 0.4260\n",
      "Iteration 67: Train Loss = 0.4198; Validation Loss = 0.4261\n",
      "Iteration 68: Train Loss = 0.4199; Validation Loss = 0.4261\n",
      "Iteration 69: Train Loss = 0.4199; Validation Loss = 0.4261\n",
      "Iteration 70: Train Loss = 0.4199; Validation Loss = 0.4263\n",
      "Iteration 71: Train Loss = 0.4197; Validation Loss = 0.4259\n",
      "Iteration 72: Train Loss = 0.4197; Validation Loss = 0.4257\n",
      "Iteration 73: Train Loss = 0.4195; Validation Loss = 0.4257\n",
      "Iteration 74: Train Loss = 0.4197; Validation Loss = 0.4260\n",
      "Iteration 75: Train Loss = 0.4193; Validation Loss = 0.4254\n",
      "Iteration 76: Train Loss = 0.4193; Validation Loss = 0.4253\n",
      "Iteration 77: Train Loss = 0.4193; Validation Loss = 0.4253\n",
      "Iteration 78: Train Loss = 0.4193; Validation Loss = 0.4253\n",
      "Iteration 79: Train Loss = 0.4193; Validation Loss = 0.4253\n",
      "Iteration 80: Train Loss = 0.4193; Validation Loss = 0.4253\n",
      "Iteration 81: Train Loss = 0.4192; Validation Loss = 0.4253\n",
      "Iteration 82: Train Loss = 0.4192; Validation Loss = 0.4252\n",
      "Iteration 83: Train Loss = 0.4192; Validation Loss = 0.4253\n",
      "Iteration 84: Train Loss = 0.4194; Validation Loss = 0.4256\n",
      "Iteration 85: Train Loss = 0.4192; Validation Loss = 0.4253\n",
      "Iteration 86: Train Loss = 0.4192; Validation Loss = 0.4252\n",
      "Iteration 87: Train Loss = 0.4191; Validation Loss = 0.4253\n",
      "Iteration 88: Train Loss = 0.4191; Validation Loss = 0.4252\n",
      "Iteration 89: Train Loss = 0.4192; Validation Loss = 0.4253\n",
      "Iteration 90: Train Loss = 0.4193; Validation Loss = 0.4255\n",
      "Iteration 91: Train Loss = 0.4192; Validation Loss = 0.4254\n",
      "Iteration 92: Train Loss = 0.4191; Validation Loss = 0.4252\n",
      "Iteration 93: Train Loss = 0.4191; Validation Loss = 0.4252\n",
      "Iteration 94: Train Loss = 0.4190; Validation Loss = 0.4250\n",
      "Iteration 95: Train Loss = 0.4190; Validation Loss = 0.4250\n",
      "Iteration 96: Train Loss = 0.4191; Validation Loss = 0.4250\n",
      "Iteration 97: Train Loss = 0.4189; Validation Loss = 0.4250\n",
      "Iteration 98: Train Loss = 0.4188; Validation Loss = 0.4249\n",
      "Iteration 99: Train Loss = 0.4187; Validation Loss = 0.4249\n",
      "Iteration 100: Train Loss = 0.4188; Validation Loss = 0.4248\n",
      "Iteration 101: Train Loss = 0.4188; Validation Loss = 0.4249\n",
      "Iteration 102: Train Loss = 0.4188; Validation Loss = 0.4249\n",
      "Iteration 103: Train Loss = 0.4188; Validation Loss = 0.4249\n",
      "Iteration 104: Train Loss = 0.4186; Validation Loss = 0.4249\n",
      "Iteration 105: Train Loss = 0.4187; Validation Loss = 0.4250\n",
      "Iteration 106: Train Loss = 0.4186; Validation Loss = 0.4249\n",
      "Iteration 107: Train Loss = 0.4186; Validation Loss = 0.4249\n",
      "Iteration 108: Train Loss = 0.4186; Validation Loss = 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:29:27,807] Trial 2 finished with value: 0.9410094216243626 and parameters: {'max_depth': 3, 'min_samples_leaf': 5, 'n_estimators': 110, 'learning_rate': 0.4622395159111369, 'subsample': 0.8032335757862877}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109: Train Loss = 0.4186; Validation Loss = 0.4248\n",
      "Iteration 110: Train Loss = 0.4186; Validation Loss = 0.4248\n",
      "Iteration 1: Train Loss = 0.6927; Validation Loss = 0.6927\n",
      "Iteration 2: Train Loss = 0.6923; Validation Loss = 0.6923\n",
      "Iteration 3: Train Loss = 0.6918; Validation Loss = 0.6919\n",
      "Iteration 4: Train Loss = 0.6914; Validation Loss = 0.6915\n",
      "Iteration 5: Train Loss = 0.6910; Validation Loss = 0.6911\n",
      "Iteration 6: Train Loss = 0.6905; Validation Loss = 0.6907\n",
      "Iteration 7: Train Loss = 0.6901; Validation Loss = 0.6903\n",
      "Iteration 8: Train Loss = 0.6897; Validation Loss = 0.6899\n",
      "Iteration 9: Train Loss = 0.6892; Validation Loss = 0.6894\n",
      "Iteration 10: Train Loss = 0.6888; Validation Loss = 0.6890\n",
      "Iteration 11: Train Loss = 0.6884; Validation Loss = 0.6886\n",
      "Iteration 12: Train Loss = 0.6879; Validation Loss = 0.6882\n",
      "Iteration 13: Train Loss = 0.6875; Validation Loss = 0.6878\n",
      "Iteration 14: Train Loss = 0.6871; Validation Loss = 0.6874\n",
      "Iteration 15: Train Loss = 0.6867; Validation Loss = 0.6870\n",
      "Iteration 16: Train Loss = 0.6862; Validation Loss = 0.6866\n",
      "Iteration 17: Train Loss = 0.6858; Validation Loss = 0.6862\n",
      "Iteration 18: Train Loss = 0.6854; Validation Loss = 0.6858\n",
      "Iteration 19: Train Loss = 0.6850; Validation Loss = 0.6854\n",
      "Iteration 20: Train Loss = 0.6845; Validation Loss = 0.6850\n",
      "Iteration 21: Train Loss = 0.6841; Validation Loss = 0.6846\n",
      "Iteration 22: Train Loss = 0.6837; Validation Loss = 0.6842\n",
      "Iteration 23: Train Loss = 0.6833; Validation Loss = 0.6838\n",
      "Iteration 24: Train Loss = 0.6829; Validation Loss = 0.6834\n",
      "Iteration 25: Train Loss = 0.6824; Validation Loss = 0.6830\n",
      "Iteration 26: Train Loss = 0.6820; Validation Loss = 0.6826\n",
      "Iteration 27: Train Loss = 0.6816; Validation Loss = 0.6822\n",
      "Iteration 28: Train Loss = 0.6812; Validation Loss = 0.6818\n",
      "Iteration 29: Train Loss = 0.6808; Validation Loss = 0.6814\n",
      "Iteration 30: Train Loss = 0.6803; Validation Loss = 0.6810\n",
      "Iteration 31: Train Loss = 0.6799; Validation Loss = 0.6806\n",
      "Iteration 32: Train Loss = 0.6795; Validation Loss = 0.6802\n",
      "Iteration 33: Train Loss = 0.6791; Validation Loss = 0.6798\n",
      "Iteration 34: Train Loss = 0.6787; Validation Loss = 0.6794\n",
      "Iteration 35: Train Loss = 0.6783; Validation Loss = 0.6790\n",
      "Iteration 36: Train Loss = 0.6779; Validation Loss = 0.6786\n",
      "Iteration 37: Train Loss = 0.6775; Validation Loss = 0.6782\n",
      "Iteration 38: Train Loss = 0.6770; Validation Loss = 0.6779\n",
      "Iteration 39: Train Loss = 0.6766; Validation Loss = 0.6775\n",
      "Iteration 40: Train Loss = 0.6762; Validation Loss = 0.6771\n",
      "Iteration 41: Train Loss = 0.6758; Validation Loss = 0.6767\n",
      "Iteration 42: Train Loss = 0.6754; Validation Loss = 0.6763\n",
      "Iteration 43: Train Loss = 0.6750; Validation Loss = 0.6759\n",
      "Iteration 44: Train Loss = 0.6746; Validation Loss = 0.6755\n",
      "Iteration 45: Train Loss = 0.6742; Validation Loss = 0.6751\n",
      "Iteration 46: Train Loss = 0.6738; Validation Loss = 0.6747\n",
      "Iteration 47: Train Loss = 0.6734; Validation Loss = 0.6744\n",
      "Iteration 48: Train Loss = 0.6730; Validation Loss = 0.6740\n",
      "Iteration 49: Train Loss = 0.6726; Validation Loss = 0.6736\n",
      "Iteration 50: Train Loss = 0.6722; Validation Loss = 0.6732\n",
      "Iteration 51: Train Loss = 0.6718; Validation Loss = 0.6728\n",
      "Iteration 52: Train Loss = 0.6714; Validation Loss = 0.6724\n",
      "Iteration 53: Train Loss = 0.6710; Validation Loss = 0.6720\n",
      "Iteration 54: Train Loss = 0.6706; Validation Loss = 0.6717\n",
      "Iteration 55: Train Loss = 0.6702; Validation Loss = 0.6713\n",
      "Iteration 56: Train Loss = 0.6698; Validation Loss = 0.6709\n",
      "Iteration 57: Train Loss = 0.6694; Validation Loss = 0.6705\n",
      "Iteration 58: Train Loss = 0.6690; Validation Loss = 0.6701\n",
      "Iteration 59: Train Loss = 0.6686; Validation Loss = 0.6698\n",
      "Iteration 60: Train Loss = 0.6682; Validation Loss = 0.6694\n",
      "Iteration 61: Train Loss = 0.6678; Validation Loss = 0.6690\n",
      "Iteration 62: Train Loss = 0.6674; Validation Loss = 0.6686\n",
      "Iteration 63: Train Loss = 0.6670; Validation Loss = 0.6683\n",
      "Iteration 64: Train Loss = 0.6666; Validation Loss = 0.6679\n",
      "Iteration 65: Train Loss = 0.6662; Validation Loss = 0.6675\n",
      "Iteration 66: Train Loss = 0.6658; Validation Loss = 0.6672\n",
      "Iteration 67: Train Loss = 0.6654; Validation Loss = 0.6668\n",
      "Iteration 68: Train Loss = 0.6650; Validation Loss = 0.6664\n",
      "Iteration 69: Train Loss = 0.6647; Validation Loss = 0.6661\n",
      "Iteration 70: Train Loss = 0.6643; Validation Loss = 0.6657\n",
      "Iteration 71: Train Loss = 0.6639; Validation Loss = 0.6653\n",
      "Iteration 72: Train Loss = 0.6635; Validation Loss = 0.6649\n",
      "Iteration 73: Train Loss = 0.6631; Validation Loss = 0.6646\n",
      "Iteration 74: Train Loss = 0.6627; Validation Loss = 0.6642\n",
      "Iteration 75: Train Loss = 0.6623; Validation Loss = 0.6638\n",
      "Iteration 76: Train Loss = 0.6619; Validation Loss = 0.6635\n",
      "Iteration 77: Train Loss = 0.6616; Validation Loss = 0.6631\n",
      "Iteration 78: Train Loss = 0.6612; Validation Loss = 0.6627\n",
      "Iteration 79: Train Loss = 0.6608; Validation Loss = 0.6624\n",
      "Iteration 80: Train Loss = 0.6604; Validation Loss = 0.6620\n",
      "Iteration 81: Train Loss = 0.6600; Validation Loss = 0.6617\n",
      "Iteration 82: Train Loss = 0.6596; Validation Loss = 0.6613\n",
      "Iteration 83: Train Loss = 0.6593; Validation Loss = 0.6609\n",
      "Iteration 84: Train Loss = 0.6589; Validation Loss = 0.6606\n",
      "Iteration 85: Train Loss = 0.6585; Validation Loss = 0.6602\n",
      "Iteration 86: Train Loss = 0.6581; Validation Loss = 0.6599\n",
      "Iteration 87: Train Loss = 0.6577; Validation Loss = 0.6595\n",
      "Iteration 88: Train Loss = 0.6574; Validation Loss = 0.6591\n",
      "Iteration 89: Train Loss = 0.6570; Validation Loss = 0.6588\n",
      "Iteration 90: Train Loss = 0.6566; Validation Loss = 0.6584\n",
      "Iteration 91: Train Loss = 0.6562; Validation Loss = 0.6581\n",
      "Iteration 92: Train Loss = 0.6559; Validation Loss = 0.6577\n",
      "Iteration 93: Train Loss = 0.6555; Validation Loss = 0.6574\n",
      "Iteration 94: Train Loss = 0.6551; Validation Loss = 0.6570\n",
      "Iteration 95: Train Loss = 0.6548; Validation Loss = 0.6567\n",
      "Iteration 96: Train Loss = 0.6544; Validation Loss = 0.6563\n",
      "Iteration 97: Train Loss = 0.6540; Validation Loss = 0.6560\n",
      "Iteration 98: Train Loss = 0.6536; Validation Loss = 0.6556\n",
      "Iteration 99: Train Loss = 0.6533; Validation Loss = 0.6553\n",
      "Iteration 100: Train Loss = 0.6529; Validation Loss = 0.6549\n",
      "Iteration 101: Train Loss = 0.6525; Validation Loss = 0.6546\n",
      "Iteration 102: Train Loss = 0.6522; Validation Loss = 0.6542\n",
      "Iteration 103: Train Loss = 0.6518; Validation Loss = 0.6539\n",
      "Iteration 104: Train Loss = 0.6514; Validation Loss = 0.6535\n",
      "Iteration 105: Train Loss = 0.6511; Validation Loss = 0.6532\n",
      "Iteration 106: Train Loss = 0.6507; Validation Loss = 0.6529\n",
      "Iteration 107: Train Loss = 0.6503; Validation Loss = 0.6525\n",
      "Iteration 108: Train Loss = 0.6500; Validation Loss = 0.6522\n",
      "Iteration 109: Train Loss = 0.6496; Validation Loss = 0.6518\n",
      "Iteration 110: Train Loss = 0.6492; Validation Loss = 0.6515\n",
      "Iteration 111: Train Loss = 0.6489; Validation Loss = 0.6511\n",
      "Iteration 112: Train Loss = 0.6485; Validation Loss = 0.6508\n",
      "Iteration 113: Train Loss = 0.6481; Validation Loss = 0.6504\n",
      "Iteration 114: Train Loss = 0.6478; Validation Loss = 0.6501\n",
      "Iteration 115: Train Loss = 0.6474; Validation Loss = 0.6497\n",
      "Iteration 116: Train Loss = 0.6471; Validation Loss = 0.6494\n",
      "Iteration 117: Train Loss = 0.6467; Validation Loss = 0.6491\n",
      "Iteration 118: Train Loss = 0.6463; Validation Loss = 0.6487\n",
      "Iteration 119: Train Loss = 0.6460; Validation Loss = 0.6484\n",
      "Iteration 120: Train Loss = 0.6456; Validation Loss = 0.6481\n",
      "Iteration 121: Train Loss = 0.6453; Validation Loss = 0.6477\n",
      "Iteration 122: Train Loss = 0.6449; Validation Loss = 0.6474\n",
      "Iteration 123: Train Loss = 0.6445; Validation Loss = 0.6470\n",
      "Iteration 124: Train Loss = 0.6442; Validation Loss = 0.6467\n",
      "Iteration 125: Train Loss = 0.6438; Validation Loss = 0.6464\n",
      "Iteration 126: Train Loss = 0.6435; Validation Loss = 0.6460\n",
      "Iteration 127: Train Loss = 0.6431; Validation Loss = 0.6457\n",
      "Iteration 128: Train Loss = 0.6428; Validation Loss = 0.6454\n",
      "Iteration 129: Train Loss = 0.6424; Validation Loss = 0.6450\n",
      "Iteration 130: Train Loss = 0.6421; Validation Loss = 0.6447\n",
      "Iteration 131: Train Loss = 0.6417; Validation Loss = 0.6444\n",
      "Iteration 132: Train Loss = 0.6414; Validation Loss = 0.6440\n",
      "Iteration 133: Train Loss = 0.6410; Validation Loss = 0.6437\n",
      "Iteration 134: Train Loss = 0.6407; Validation Loss = 0.6434\n",
      "Iteration 135: Train Loss = 0.6403; Validation Loss = 0.6430\n",
      "Iteration 136: Train Loss = 0.6400; Validation Loss = 0.6427\n",
      "Iteration 137: Train Loss = 0.6396; Validation Loss = 0.6424\n",
      "Iteration 138: Train Loss = 0.6393; Validation Loss = 0.6420\n",
      "Iteration 139: Train Loss = 0.6389; Validation Loss = 0.6417\n",
      "Iteration 140: Train Loss = 0.6386; Validation Loss = 0.6414\n",
      "Iteration 141: Train Loss = 0.6383; Validation Loss = 0.6411\n",
      "Iteration 142: Train Loss = 0.6379; Validation Loss = 0.6407\n",
      "Iteration 143: Train Loss = 0.6376; Validation Loss = 0.6404\n",
      "Iteration 144: Train Loss = 0.6372; Validation Loss = 0.6401\n",
      "Iteration 145: Train Loss = 0.6369; Validation Loss = 0.6398\n",
      "Iteration 146: Train Loss = 0.6365; Validation Loss = 0.6394\n",
      "Iteration 147: Train Loss = 0.6362; Validation Loss = 0.6391\n",
      "Iteration 148: Train Loss = 0.6359; Validation Loss = 0.6388\n",
      "Iteration 149: Train Loss = 0.6355; Validation Loss = 0.6385\n",
      "Iteration 150: Train Loss = 0.6352; Validation Loss = 0.6381\n",
      "Iteration 151: Train Loss = 0.6348; Validation Loss = 0.6378\n",
      "Iteration 152: Train Loss = 0.6345; Validation Loss = 0.6375\n",
      "Iteration 153: Train Loss = 0.6342; Validation Loss = 0.6372\n",
      "Iteration 154: Train Loss = 0.6338; Validation Loss = 0.6369\n",
      "Iteration 155: Train Loss = 0.6335; Validation Loss = 0.6365\n",
      "Iteration 156: Train Loss = 0.6331; Validation Loss = 0.6362\n",
      "Iteration 157: Train Loss = 0.6328; Validation Loss = 0.6359\n",
      "Iteration 158: Train Loss = 0.6325; Validation Loss = 0.6356\n",
      "Iteration 159: Train Loss = 0.6321; Validation Loss = 0.6353\n",
      "Iteration 160: Train Loss = 0.6318; Validation Loss = 0.6350\n",
      "Iteration 161: Train Loss = 0.6315; Validation Loss = 0.6346\n",
      "Iteration 162: Train Loss = 0.6311; Validation Loss = 0.6343\n",
      "Iteration 163: Train Loss = 0.6308; Validation Loss = 0.6340\n",
      "Iteration 164: Train Loss = 0.6305; Validation Loss = 0.6337\n",
      "Iteration 165: Train Loss = 0.6301; Validation Loss = 0.6334\n",
      "Iteration 166: Train Loss = 0.6298; Validation Loss = 0.6331\n",
      "Iteration 167: Train Loss = 0.6295; Validation Loss = 0.6328\n",
      "Iteration 168: Train Loss = 0.6292; Validation Loss = 0.6324\n",
      "Iteration 169: Train Loss = 0.6288; Validation Loss = 0.6321\n",
      "Iteration 170: Train Loss = 0.6285; Validation Loss = 0.6318\n",
      "Iteration 171: Train Loss = 0.6282; Validation Loss = 0.6315\n",
      "Iteration 172: Train Loss = 0.6278; Validation Loss = 0.6312\n",
      "Iteration 173: Train Loss = 0.6275; Validation Loss = 0.6309\n",
      "Iteration 174: Train Loss = 0.6272; Validation Loss = 0.6306\n",
      "Iteration 175: Train Loss = 0.6269; Validation Loss = 0.6303\n",
      "Iteration 176: Train Loss = 0.6265; Validation Loss = 0.6300\n",
      "Iteration 177: Train Loss = 0.6262; Validation Loss = 0.6296\n",
      "Iteration 178: Train Loss = 0.6259; Validation Loss = 0.6293\n",
      "Iteration 179: Train Loss = 0.6256; Validation Loss = 0.6290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:29:59,815] Trial 3 finished with value: 0.961714880523432 and parameters: {'max_depth': 12, 'min_samples_leaf': 8, 'n_estimators': 180, 'learning_rate': 0.0012021620178382655, 'subsample': 0.654231431564571}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 180: Train Loss = 0.6252; Validation Loss = 0.6287\n",
      "Iteration 1: Train Loss = 0.6917; Validation Loss = 0.6917\n",
      "Iteration 2: Train Loss = 0.6903; Validation Loss = 0.6903\n",
      "Iteration 3: Train Loss = 0.6888; Validation Loss = 0.6889\n",
      "Iteration 4: Train Loss = 0.6874; Validation Loss = 0.6875\n",
      "Iteration 5: Train Loss = 0.6859; Validation Loss = 0.6861\n",
      "Iteration 6: Train Loss = 0.6845; Validation Loss = 0.6847\n",
      "Iteration 7: Train Loss = 0.6832; Validation Loss = 0.6834\n",
      "Iteration 8: Train Loss = 0.6818; Validation Loss = 0.6820\n",
      "Iteration 9: Train Loss = 0.6804; Validation Loss = 0.6807\n",
      "Iteration 10: Train Loss = 0.6790; Validation Loss = 0.6793\n",
      "Iteration 11: Train Loss = 0.6777; Validation Loss = 0.6780\n",
      "Iteration 12: Train Loss = 0.6763; Validation Loss = 0.6767\n",
      "Iteration 13: Train Loss = 0.6750; Validation Loss = 0.6754\n",
      "Iteration 14: Train Loss = 0.6736; Validation Loss = 0.6741\n",
      "Iteration 15: Train Loss = 0.6723; Validation Loss = 0.6728\n",
      "Iteration 16: Train Loss = 0.6710; Validation Loss = 0.6715\n",
      "Iteration 17: Train Loss = 0.6697; Validation Loss = 0.6703\n",
      "Iteration 18: Train Loss = 0.6684; Validation Loss = 0.6690\n",
      "Iteration 19: Train Loss = 0.6671; Validation Loss = 0.6678\n",
      "Iteration 20: Train Loss = 0.6659; Validation Loss = 0.6665\n",
      "Iteration 21: Train Loss = 0.6646; Validation Loss = 0.6653\n",
      "Iteration 22: Train Loss = 0.6633; Validation Loss = 0.6641\n",
      "Iteration 23: Train Loss = 0.6621; Validation Loss = 0.6629\n",
      "Iteration 24: Train Loss = 0.6608; Validation Loss = 0.6616\n",
      "Iteration 25: Train Loss = 0.6596; Validation Loss = 0.6604\n",
      "Iteration 26: Train Loss = 0.6584; Validation Loss = 0.6592\n",
      "Iteration 27: Train Loss = 0.6572; Validation Loss = 0.6581\n",
      "Iteration 28: Train Loss = 0.6560; Validation Loss = 0.6569\n",
      "Iteration 29: Train Loss = 0.6548; Validation Loss = 0.6557\n",
      "Iteration 30: Train Loss = 0.6536; Validation Loss = 0.6546\n",
      "Iteration 31: Train Loss = 0.6524; Validation Loss = 0.6534\n",
      "Iteration 32: Train Loss = 0.6512; Validation Loss = 0.6523\n",
      "Iteration 33: Train Loss = 0.6501; Validation Loss = 0.6511\n",
      "Iteration 34: Train Loss = 0.6489; Validation Loss = 0.6500\n",
      "Iteration 35: Train Loss = 0.6477; Validation Loss = 0.6488\n",
      "Iteration 36: Train Loss = 0.6466; Validation Loss = 0.6477\n",
      "Iteration 37: Train Loss = 0.6455; Validation Loss = 0.6466\n",
      "Iteration 38: Train Loss = 0.6444; Validation Loss = 0.6455\n",
      "Iteration 39: Train Loss = 0.6432; Validation Loss = 0.6444\n",
      "Iteration 40: Train Loss = 0.6421; Validation Loss = 0.6433\n",
      "Iteration 41: Train Loss = 0.6410; Validation Loss = 0.6423\n",
      "Iteration 42: Train Loss = 0.6399; Validation Loss = 0.6412\n",
      "Iteration 43: Train Loss = 0.6388; Validation Loss = 0.6401\n",
      "Iteration 44: Train Loss = 0.6377; Validation Loss = 0.6391\n",
      "Iteration 45: Train Loss = 0.6367; Validation Loss = 0.6380\n",
      "Iteration 46: Train Loss = 0.6356; Validation Loss = 0.6370\n",
      "Iteration 47: Train Loss = 0.6345; Validation Loss = 0.6359\n",
      "Iteration 48: Train Loss = 0.6335; Validation Loss = 0.6349\n",
      "Iteration 49: Train Loss = 0.6325; Validation Loss = 0.6339\n",
      "Iteration 50: Train Loss = 0.6314; Validation Loss = 0.6329\n",
      "Iteration 51: Train Loss = 0.6304; Validation Loss = 0.6319\n",
      "Iteration 52: Train Loss = 0.6294; Validation Loss = 0.6309\n",
      "Iteration 53: Train Loss = 0.6283; Validation Loss = 0.6299\n",
      "Iteration 54: Train Loss = 0.6273; Validation Loss = 0.6289\n",
      "Iteration 55: Train Loss = 0.6263; Validation Loss = 0.6280\n",
      "Iteration 56: Train Loss = 0.6254; Validation Loss = 0.6270\n",
      "Iteration 57: Train Loss = 0.6244; Validation Loss = 0.6260\n",
      "Iteration 58: Train Loss = 0.6234; Validation Loss = 0.6251\n",
      "Iteration 59: Train Loss = 0.6224; Validation Loss = 0.6241\n",
      "Iteration 60: Train Loss = 0.6214; Validation Loss = 0.6232\n",
      "Iteration 61: Train Loss = 0.6205; Validation Loss = 0.6222\n",
      "Iteration 62: Train Loss = 0.6195; Validation Loss = 0.6213\n",
      "Iteration 63: Train Loss = 0.6186; Validation Loss = 0.6204\n",
      "Iteration 64: Train Loss = 0.6176; Validation Loss = 0.6195\n",
      "Iteration 65: Train Loss = 0.6167; Validation Loss = 0.6186\n",
      "Iteration 66: Train Loss = 0.6158; Validation Loss = 0.6177\n",
      "Iteration 67: Train Loss = 0.6149; Validation Loss = 0.6168\n",
      "Iteration 68: Train Loss = 0.6140; Validation Loss = 0.6159\n",
      "Iteration 69: Train Loss = 0.6131; Validation Loss = 0.6150\n",
      "Iteration 70: Train Loss = 0.6122; Validation Loss = 0.6141\n",
      "Iteration 71: Train Loss = 0.6113; Validation Loss = 0.6133\n",
      "Iteration 72: Train Loss = 0.6104; Validation Loss = 0.6124\n",
      "Iteration 73: Train Loss = 0.6095; Validation Loss = 0.6115\n",
      "Iteration 74: Train Loss = 0.6086; Validation Loss = 0.6107\n",
      "Iteration 75: Train Loss = 0.6077; Validation Loss = 0.6098\n",
      "Iteration 76: Train Loss = 0.6069; Validation Loss = 0.6090\n",
      "Iteration 77: Train Loss = 0.6060; Validation Loss = 0.6081\n",
      "Iteration 78: Train Loss = 0.6052; Validation Loss = 0.6073\n",
      "Iteration 79: Train Loss = 0.6043; Validation Loss = 0.6065\n",
      "Iteration 80: Train Loss = 0.6035; Validation Loss = 0.6056\n",
      "Iteration 81: Train Loss = 0.6026; Validation Loss = 0.6048\n",
      "Iteration 82: Train Loss = 0.6018; Validation Loss = 0.6040\n",
      "Iteration 83: Train Loss = 0.6010; Validation Loss = 0.6032\n",
      "Iteration 84: Train Loss = 0.6001; Validation Loss = 0.6024\n",
      "Iteration 85: Train Loss = 0.5993; Validation Loss = 0.6016\n",
      "Iteration 86: Train Loss = 0.5985; Validation Loss = 0.6008\n",
      "Iteration 87: Train Loss = 0.5977; Validation Loss = 0.6000\n",
      "Iteration 88: Train Loss = 0.5969; Validation Loss = 0.5992\n",
      "Iteration 89: Train Loss = 0.5961; Validation Loss = 0.5984\n",
      "Iteration 90: Train Loss = 0.5952; Validation Loss = 0.5976\n",
      "Iteration 91: Train Loss = 0.5944; Validation Loss = 0.5968\n",
      "Iteration 92: Train Loss = 0.5936; Validation Loss = 0.5961\n",
      "Iteration 93: Train Loss = 0.5929; Validation Loss = 0.5953\n",
      "Iteration 94: Train Loss = 0.5921; Validation Loss = 0.5945\n",
      "Iteration 95: Train Loss = 0.5913; Validation Loss = 0.5938\n",
      "Iteration 96: Train Loss = 0.5905; Validation Loss = 0.5930\n",
      "Iteration 97: Train Loss = 0.5898; Validation Loss = 0.5923\n",
      "Iteration 98: Train Loss = 0.5890; Validation Loss = 0.5915\n",
      "Iteration 99: Train Loss = 0.5883; Validation Loss = 0.5908\n",
      "Iteration 100: Train Loss = 0.5875; Validation Loss = 0.5901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:30:08,804] Trial 4 finished with value: 0.93708940057727 and parameters: {'max_depth': 3, 'min_samples_leaf': 2, 'n_estimators': 100, 'learning_rate': 0.005038414854203554, 'subsample': 0.715065225040602}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6920; Validation Loss = 0.6921\n",
      "Iteration 2: Train Loss = 0.6909; Validation Loss = 0.6910\n",
      "Iteration 3: Train Loss = 0.6898; Validation Loss = 0.6899\n",
      "Iteration 4: Train Loss = 0.6887; Validation Loss = 0.6888\n",
      "Iteration 5: Train Loss = 0.6876; Validation Loss = 0.6877\n",
      "Iteration 6: Train Loss = 0.6865; Validation Loss = 0.6867\n",
      "Iteration 7: Train Loss = 0.6854; Validation Loss = 0.6856\n",
      "Iteration 8: Train Loss = 0.6843; Validation Loss = 0.6845\n",
      "Iteration 9: Train Loss = 0.6833; Validation Loss = 0.6835\n",
      "Iteration 10: Train Loss = 0.6822; Validation Loss = 0.6824\n",
      "Iteration 11: Train Loss = 0.6811; Validation Loss = 0.6814\n",
      "Iteration 12: Train Loss = 0.6800; Validation Loss = 0.6804\n",
      "Iteration 13: Train Loss = 0.6790; Validation Loss = 0.6793\n",
      "Iteration 14: Train Loss = 0.6779; Validation Loss = 0.6783\n",
      "Iteration 15: Train Loss = 0.6769; Validation Loss = 0.6773\n",
      "Iteration 16: Train Loss = 0.6758; Validation Loss = 0.6763\n",
      "Iteration 17: Train Loss = 0.6748; Validation Loss = 0.6752\n",
      "Iteration 18: Train Loss = 0.6737; Validation Loss = 0.6742\n",
      "Iteration 19: Train Loss = 0.6727; Validation Loss = 0.6732\n",
      "Iteration 20: Train Loss = 0.6717; Validation Loss = 0.6722\n",
      "Iteration 21: Train Loss = 0.6707; Validation Loss = 0.6712\n",
      "Iteration 22: Train Loss = 0.6697; Validation Loss = 0.6702\n",
      "Iteration 23: Train Loss = 0.6687; Validation Loss = 0.6692\n",
      "Iteration 24: Train Loss = 0.6676; Validation Loss = 0.6683\n",
      "Iteration 25: Train Loss = 0.6667; Validation Loss = 0.6673\n",
      "Iteration 26: Train Loss = 0.6657; Validation Loss = 0.6663\n",
      "Iteration 27: Train Loss = 0.6647; Validation Loss = 0.6654\n",
      "Iteration 28: Train Loss = 0.6637; Validation Loss = 0.6644\n",
      "Iteration 29: Train Loss = 0.6627; Validation Loss = 0.6634\n",
      "Iteration 30: Train Loss = 0.6617; Validation Loss = 0.6625\n",
      "Iteration 31: Train Loss = 0.6607; Validation Loss = 0.6615\n",
      "Iteration 32: Train Loss = 0.6598; Validation Loss = 0.6606\n",
      "Iteration 33: Train Loss = 0.6588; Validation Loss = 0.6597\n",
      "Iteration 34: Train Loss = 0.6578; Validation Loss = 0.6587\n",
      "Iteration 35: Train Loss = 0.6569; Validation Loss = 0.6578\n",
      "Iteration 36: Train Loss = 0.6560; Validation Loss = 0.6569\n",
      "Iteration 37: Train Loss = 0.6550; Validation Loss = 0.6560\n",
      "Iteration 38: Train Loss = 0.6541; Validation Loss = 0.6550\n",
      "Iteration 39: Train Loss = 0.6531; Validation Loss = 0.6541\n",
      "Iteration 40: Train Loss = 0.6522; Validation Loss = 0.6532\n",
      "Iteration 41: Train Loss = 0.6513; Validation Loss = 0.6523\n",
      "Iteration 42: Train Loss = 0.6504; Validation Loss = 0.6514\n",
      "Iteration 43: Train Loss = 0.6495; Validation Loss = 0.6506\n",
      "Iteration 44: Train Loss = 0.6486; Validation Loss = 0.6497\n",
      "Iteration 45: Train Loss = 0.6476; Validation Loss = 0.6488\n",
      "Iteration 46: Train Loss = 0.6467; Validation Loss = 0.6479\n",
      "Iteration 47: Train Loss = 0.6459; Validation Loss = 0.6470\n",
      "Iteration 48: Train Loss = 0.6450; Validation Loss = 0.6461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:30:13,576] Trial 5 finished with value: 0.9545794584949041 and parameters: {'max_depth': 6, 'min_samples_leaf': 8, 'n_estimators': 50, 'learning_rate': 0.0033790333357318573, 'subsample': 0.5589970535686868}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49: Train Loss = 0.6441; Validation Loss = 0.6453\n",
      "Iteration 50: Train Loss = 0.6432; Validation Loss = 0.6444\n",
      "Iteration 1: Train Loss = 0.6656; Validation Loss = 0.6678\n",
      "Iteration 2: Train Loss = 0.6409; Validation Loss = 0.6446\n",
      "Iteration 3: Train Loss = 0.6187; Validation Loss = 0.6238\n",
      "Iteration 4: Train Loss = 0.5988; Validation Loss = 0.6050\n",
      "Iteration 5: Train Loss = 0.5810; Validation Loss = 0.5886\n",
      "Iteration 6: Train Loss = 0.5649; Validation Loss = 0.5738\n",
      "Iteration 7: Train Loss = 0.5502; Validation Loss = 0.5604\n",
      "Iteration 8: Train Loss = 0.5370; Validation Loss = 0.5485\n",
      "Iteration 9: Train Loss = 0.5250; Validation Loss = 0.5374\n",
      "Iteration 10: Train Loss = 0.5141; Validation Loss = 0.5275\n",
      "Iteration 11: Train Loss = 0.5040; Validation Loss = 0.5186\n",
      "Iteration 12: Train Loss = 0.4948; Validation Loss = 0.5105\n",
      "Iteration 13: Train Loss = 0.4867; Validation Loss = 0.5031\n",
      "Iteration 14: Train Loss = 0.4792; Validation Loss = 0.4961\n",
      "Iteration 15: Train Loss = 0.4720; Validation Loss = 0.4898\n",
      "Iteration 16: Train Loss = 0.4658; Validation Loss = 0.4846\n",
      "Iteration 17: Train Loss = 0.4598; Validation Loss = 0.4797\n",
      "Iteration 18: Train Loss = 0.4544; Validation Loss = 0.4751\n",
      "Iteration 19: Train Loss = 0.4491; Validation Loss = 0.4707\n",
      "Iteration 20: Train Loss = 0.4446; Validation Loss = 0.4666\n",
      "Iteration 21: Train Loss = 0.4404; Validation Loss = 0.4630\n",
      "Iteration 22: Train Loss = 0.4363; Validation Loss = 0.4597\n",
      "Iteration 23: Train Loss = 0.4326; Validation Loss = 0.4571\n",
      "Iteration 24: Train Loss = 0.4294; Validation Loss = 0.4544\n",
      "Iteration 25: Train Loss = 0.4263; Validation Loss = 0.4520\n",
      "Iteration 26: Train Loss = 0.4235; Validation Loss = 0.4497\n",
      "Iteration 27: Train Loss = 0.4209; Validation Loss = 0.4475\n",
      "Iteration 28: Train Loss = 0.4186; Validation Loss = 0.4455\n",
      "Iteration 29: Train Loss = 0.4164; Validation Loss = 0.4438\n",
      "Iteration 30: Train Loss = 0.4144; Validation Loss = 0.4422\n",
      "Iteration 31: Train Loss = 0.4124; Validation Loss = 0.4407\n",
      "Iteration 32: Train Loss = 0.4106; Validation Loss = 0.4390\n",
      "Iteration 33: Train Loss = 0.4089; Validation Loss = 0.4376\n",
      "Iteration 34: Train Loss = 0.4075; Validation Loss = 0.4365\n",
      "Iteration 35: Train Loss = 0.4060; Validation Loss = 0.4352\n",
      "Iteration 36: Train Loss = 0.4046; Validation Loss = 0.4341\n",
      "Iteration 37: Train Loss = 0.4031; Validation Loss = 0.4329\n",
      "Iteration 38: Train Loss = 0.4018; Validation Loss = 0.4320\n",
      "Iteration 39: Train Loss = 0.4008; Validation Loss = 0.4312\n",
      "Iteration 40: Train Loss = 0.3998; Validation Loss = 0.4305\n",
      "Iteration 41: Train Loss = 0.3988; Validation Loss = 0.4298\n",
      "Iteration 42: Train Loss = 0.3981; Validation Loss = 0.4292\n",
      "Iteration 43: Train Loss = 0.3972; Validation Loss = 0.4287\n",
      "Iteration 44: Train Loss = 0.3962; Validation Loss = 0.4279\n",
      "Iteration 45: Train Loss = 0.3952; Validation Loss = 0.4275\n",
      "Iteration 46: Train Loss = 0.3945; Validation Loss = 0.4268\n",
      "Iteration 47: Train Loss = 0.3937; Validation Loss = 0.4263\n",
      "Iteration 48: Train Loss = 0.3931; Validation Loss = 0.4260\n",
      "Iteration 49: Train Loss = 0.3923; Validation Loss = 0.4255\n",
      "Iteration 50: Train Loss = 0.3917; Validation Loss = 0.4251\n",
      "Iteration 51: Train Loss = 0.3911; Validation Loss = 0.4246\n",
      "Iteration 52: Train Loss = 0.3904; Validation Loss = 0.4243\n",
      "Iteration 53: Train Loss = 0.3900; Validation Loss = 0.4238\n",
      "Iteration 54: Train Loss = 0.3895; Validation Loss = 0.4236\n",
      "Iteration 55: Train Loss = 0.3889; Validation Loss = 0.4233\n",
      "Iteration 56: Train Loss = 0.3885; Validation Loss = 0.4230\n",
      "Iteration 57: Train Loss = 0.3880; Validation Loss = 0.4228\n",
      "Iteration 58: Train Loss = 0.3876; Validation Loss = 0.4226\n",
      "Iteration 59: Train Loss = 0.3874; Validation Loss = 0.4225\n",
      "Iteration 60: Train Loss = 0.3870; Validation Loss = 0.4224\n",
      "Iteration 61: Train Loss = 0.3866; Validation Loss = 0.4222\n",
      "Iteration 62: Train Loss = 0.3863; Validation Loss = 0.4220\n",
      "Iteration 63: Train Loss = 0.3861; Validation Loss = 0.4219\n",
      "Iteration 64: Train Loss = 0.3859; Validation Loss = 0.4216\n",
      "Iteration 65: Train Loss = 0.3856; Validation Loss = 0.4215\n",
      "Iteration 66: Train Loss = 0.3854; Validation Loss = 0.4213\n",
      "Iteration 67: Train Loss = 0.3852; Validation Loss = 0.4213\n",
      "Iteration 68: Train Loss = 0.3848; Validation Loss = 0.4212\n",
      "Iteration 69: Train Loss = 0.3844; Validation Loss = 0.4212\n",
      "Iteration 70: Train Loss = 0.3842; Validation Loss = 0.4211\n",
      "Iteration 71: Train Loss = 0.3839; Validation Loss = 0.4210\n",
      "Iteration 72: Train Loss = 0.3836; Validation Loss = 0.4209\n",
      "Iteration 73: Train Loss = 0.3835; Validation Loss = 0.4208\n",
      "Iteration 74: Train Loss = 0.3834; Validation Loss = 0.4208\n",
      "Iteration 75: Train Loss = 0.3831; Validation Loss = 0.4206\n",
      "Iteration 76: Train Loss = 0.3829; Validation Loss = 0.4206\n",
      "Iteration 77: Train Loss = 0.3826; Validation Loss = 0.4204\n",
      "Iteration 78: Train Loss = 0.3824; Validation Loss = 0.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:30:25,147] Trial 6 finished with value: 0.9585883196260329 and parameters: {'max_depth': 13, 'min_samples_leaf': 5, 'n_estimators': 80, 'learning_rate': 0.07533328468412463, 'subsample': 0.6266469614407265}. Best is trial 1 with value: 0.9648200892976295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79: Train Loss = 0.3822; Validation Loss = 0.4203\n",
      "Iteration 80: Train Loss = 0.3820; Validation Loss = 0.4203\n",
      "Iteration 1: Train Loss = 0.6823; Validation Loss = 0.6828\n",
      "Iteration 2: Train Loss = 0.6718; Validation Loss = 0.6728\n",
      "Iteration 3: Train Loss = 0.6618; Validation Loss = 0.6633\n",
      "Iteration 4: Train Loss = 0.6523; Validation Loss = 0.6541\n",
      "Iteration 5: Train Loss = 0.6432; Validation Loss = 0.6454\n",
      "Iteration 6: Train Loss = 0.6346; Validation Loss = 0.6371\n",
      "Iteration 7: Train Loss = 0.6263; Validation Loss = 0.6292\n",
      "Iteration 8: Train Loss = 0.6182; Validation Loss = 0.6215\n",
      "Iteration 9: Train Loss = 0.6105; Validation Loss = 0.6143\n",
      "Iteration 10: Train Loss = 0.6032; Validation Loss = 0.6073\n",
      "Iteration 11: Train Loss = 0.5962; Validation Loss = 0.6007\n",
      "Iteration 12: Train Loss = 0.5895; Validation Loss = 0.5944\n",
      "Iteration 13: Train Loss = 0.5831; Validation Loss = 0.5882\n",
      "Iteration 14: Train Loss = 0.5768; Validation Loss = 0.5823\n",
      "Iteration 15: Train Loss = 0.5708; Validation Loss = 0.5766\n",
      "Iteration 16: Train Loss = 0.5651; Validation Loss = 0.5712\n",
      "Iteration 17: Train Loss = 0.5596; Validation Loss = 0.5659\n",
      "Iteration 18: Train Loss = 0.5543; Validation Loss = 0.5609\n",
      "Iteration 19: Train Loss = 0.5493; Validation Loss = 0.5562\n",
      "Iteration 20: Train Loss = 0.5445; Validation Loss = 0.5516\n",
      "Iteration 21: Train Loss = 0.5398; Validation Loss = 0.5472\n",
      "Iteration 22: Train Loss = 0.5353; Validation Loss = 0.5429\n",
      "Iteration 23: Train Loss = 0.5309; Validation Loss = 0.5388\n",
      "Iteration 24: Train Loss = 0.5268; Validation Loss = 0.5348\n",
      "Iteration 25: Train Loss = 0.5228; Validation Loss = 0.5310\n",
      "Iteration 26: Train Loss = 0.5189; Validation Loss = 0.5273\n",
      "Iteration 27: Train Loss = 0.5153; Validation Loss = 0.5240\n",
      "Iteration 28: Train Loss = 0.5117; Validation Loss = 0.5206\n",
      "Iteration 29: Train Loss = 0.5082; Validation Loss = 0.5173\n",
      "Iteration 30: Train Loss = 0.5049; Validation Loss = 0.5142\n",
      "Iteration 31: Train Loss = 0.5017; Validation Loss = 0.5112\n",
      "Iteration 32: Train Loss = 0.4986; Validation Loss = 0.5083\n",
      "Iteration 33: Train Loss = 0.4957; Validation Loss = 0.5056\n",
      "Iteration 34: Train Loss = 0.4928; Validation Loss = 0.5028\n",
      "Iteration 35: Train Loss = 0.4900; Validation Loss = 0.5002\n",
      "Iteration 36: Train Loss = 0.4873; Validation Loss = 0.4976\n",
      "Iteration 37: Train Loss = 0.4847; Validation Loss = 0.4953\n",
      "Iteration 38: Train Loss = 0.4823; Validation Loss = 0.4930\n",
      "Iteration 39: Train Loss = 0.4799; Validation Loss = 0.4909\n",
      "Iteration 40: Train Loss = 0.4775; Validation Loss = 0.4887\n",
      "Iteration 41: Train Loss = 0.4753; Validation Loss = 0.4867\n",
      "Iteration 42: Train Loss = 0.4730; Validation Loss = 0.4846\n",
      "Iteration 43: Train Loss = 0.4710; Validation Loss = 0.4827\n",
      "Iteration 44: Train Loss = 0.4689; Validation Loss = 0.4807\n",
      "Iteration 45: Train Loss = 0.4669; Validation Loss = 0.4790\n",
      "Iteration 46: Train Loss = 0.4650; Validation Loss = 0.4772\n",
      "Iteration 47: Train Loss = 0.4633; Validation Loss = 0.4757\n",
      "Iteration 48: Train Loss = 0.4615; Validation Loss = 0.4740\n",
      "Iteration 49: Train Loss = 0.4599; Validation Loss = 0.4725\n",
      "Iteration 50: Train Loss = 0.4581; Validation Loss = 0.4709\n",
      "Iteration 51: Train Loss = 0.4565; Validation Loss = 0.4694\n",
      "Iteration 52: Train Loss = 0.4549; Validation Loss = 0.4681\n",
      "Iteration 53: Train Loss = 0.4533; Validation Loss = 0.4667\n",
      "Iteration 54: Train Loss = 0.4519; Validation Loss = 0.4654\n",
      "Iteration 55: Train Loss = 0.4504; Validation Loss = 0.4641\n",
      "Iteration 56: Train Loss = 0.4490; Validation Loss = 0.4628\n",
      "Iteration 57: Train Loss = 0.4477; Validation Loss = 0.4616\n",
      "Iteration 58: Train Loss = 0.4464; Validation Loss = 0.4604\n",
      "Iteration 59: Train Loss = 0.4452; Validation Loss = 0.4593\n",
      "Iteration 60: Train Loss = 0.4439; Validation Loss = 0.4581\n",
      "Iteration 61: Train Loss = 0.4426; Validation Loss = 0.4570\n",
      "Iteration 62: Train Loss = 0.4414; Validation Loss = 0.4560\n",
      "Iteration 63: Train Loss = 0.4402; Validation Loss = 0.4549\n",
      "Iteration 64: Train Loss = 0.4391; Validation Loss = 0.4540\n",
      "Iteration 65: Train Loss = 0.4381; Validation Loss = 0.4531\n",
      "Iteration 66: Train Loss = 0.4370; Validation Loss = 0.4522\n",
      "Iteration 67: Train Loss = 0.4360; Validation Loss = 0.4513\n",
      "Iteration 68: Train Loss = 0.4350; Validation Loss = 0.4505\n",
      "Iteration 69: Train Loss = 0.4340; Validation Loss = 0.4496\n",
      "Iteration 70: Train Loss = 0.4332; Validation Loss = 0.4488\n",
      "Iteration 71: Train Loss = 0.4322; Validation Loss = 0.4480\n",
      "Iteration 72: Train Loss = 0.4314; Validation Loss = 0.4472\n",
      "Iteration 73: Train Loss = 0.4306; Validation Loss = 0.4465\n",
      "Iteration 74: Train Loss = 0.4298; Validation Loss = 0.4459\n",
      "Iteration 75: Train Loss = 0.4290; Validation Loss = 0.4452\n",
      "Iteration 76: Train Loss = 0.4282; Validation Loss = 0.4444\n",
      "Iteration 77: Train Loss = 0.4274; Validation Loss = 0.4438\n",
      "Iteration 78: Train Loss = 0.4267; Validation Loss = 0.4432\n",
      "Iteration 79: Train Loss = 0.4259; Validation Loss = 0.4425\n",
      "Iteration 80: Train Loss = 0.4252; Validation Loss = 0.4420\n",
      "Iteration 81: Train Loss = 0.4246; Validation Loss = 0.4414\n",
      "Iteration 82: Train Loss = 0.4240; Validation Loss = 0.4409\n",
      "Iteration 83: Train Loss = 0.4235; Validation Loss = 0.4404\n",
      "Iteration 84: Train Loss = 0.4228; Validation Loss = 0.4399\n",
      "Iteration 85: Train Loss = 0.4221; Validation Loss = 0.4394\n",
      "Iteration 86: Train Loss = 0.4216; Validation Loss = 0.4390\n",
      "Iteration 87: Train Loss = 0.4210; Validation Loss = 0.4385\n",
      "Iteration 88: Train Loss = 0.4204; Validation Loss = 0.4381\n",
      "Iteration 89: Train Loss = 0.4200; Validation Loss = 0.4376\n",
      "Iteration 90: Train Loss = 0.4194; Validation Loss = 0.4372\n",
      "Iteration 91: Train Loss = 0.4190; Validation Loss = 0.4368\n",
      "Iteration 92: Train Loss = 0.4185; Validation Loss = 0.4363\n",
      "Iteration 93: Train Loss = 0.4180; Validation Loss = 0.4360\n",
      "Iteration 94: Train Loss = 0.4176; Validation Loss = 0.4356\n",
      "Iteration 95: Train Loss = 0.4171; Validation Loss = 0.4352\n",
      "Iteration 96: Train Loss = 0.4166; Validation Loss = 0.4347\n",
      "Iteration 97: Train Loss = 0.4161; Validation Loss = 0.4343\n",
      "Iteration 98: Train Loss = 0.4156; Validation Loss = 0.4338\n",
      "Iteration 99: Train Loss = 0.4152; Validation Loss = 0.4335\n",
      "Iteration 100: Train Loss = 0.4149; Validation Loss = 0.4332\n",
      "Iteration 101: Train Loss = 0.4144; Validation Loss = 0.4328\n",
      "Iteration 102: Train Loss = 0.4141; Validation Loss = 0.4326\n",
      "Iteration 103: Train Loss = 0.4137; Validation Loss = 0.4322\n",
      "Iteration 104: Train Loss = 0.4133; Validation Loss = 0.4319\n",
      "Iteration 105: Train Loss = 0.4130; Validation Loss = 0.4316\n",
      "Iteration 106: Train Loss = 0.4126; Validation Loss = 0.4313\n",
      "Iteration 107: Train Loss = 0.4122; Validation Loss = 0.4310\n",
      "Iteration 108: Train Loss = 0.4119; Validation Loss = 0.4307\n",
      "Iteration 109: Train Loss = 0.4116; Validation Loss = 0.4304\n",
      "Iteration 110: Train Loss = 0.4112; Validation Loss = 0.4302\n",
      "Iteration 111: Train Loss = 0.4109; Validation Loss = 0.4299\n",
      "Iteration 112: Train Loss = 0.4106; Validation Loss = 0.4296\n",
      "Iteration 113: Train Loss = 0.4103; Validation Loss = 0.4294\n",
      "Iteration 114: Train Loss = 0.4100; Validation Loss = 0.4292\n",
      "Iteration 115: Train Loss = 0.4097; Validation Loss = 0.4289\n",
      "Iteration 116: Train Loss = 0.4094; Validation Loss = 0.4287\n",
      "Iteration 117: Train Loss = 0.4091; Validation Loss = 0.4284\n",
      "Iteration 118: Train Loss = 0.4089; Validation Loss = 0.4282\n",
      "Iteration 119: Train Loss = 0.4085; Validation Loss = 0.4280\n",
      "Iteration 120: Train Loss = 0.4083; Validation Loss = 0.4278\n",
      "Iteration 121: Train Loss = 0.4081; Validation Loss = 0.4276\n",
      "Iteration 122: Train Loss = 0.4078; Validation Loss = 0.4274\n",
      "Iteration 123: Train Loss = 0.4076; Validation Loss = 0.4272\n",
      "Iteration 124: Train Loss = 0.4074; Validation Loss = 0.4271\n",
      "Iteration 125: Train Loss = 0.4072; Validation Loss = 0.4269\n",
      "Iteration 126: Train Loss = 0.4069; Validation Loss = 0.4267\n",
      "Iteration 127: Train Loss = 0.4067; Validation Loss = 0.4265\n",
      "Iteration 128: Train Loss = 0.4065; Validation Loss = 0.4264\n",
      "Iteration 129: Train Loss = 0.4064; Validation Loss = 0.4263\n",
      "Iteration 130: Train Loss = 0.4061; Validation Loss = 0.4261\n",
      "Iteration 131: Train Loss = 0.4059; Validation Loss = 0.4259\n",
      "Iteration 132: Train Loss = 0.4057; Validation Loss = 0.4258\n",
      "Iteration 133: Train Loss = 0.4056; Validation Loss = 0.4257\n",
      "Iteration 134: Train Loss = 0.4054; Validation Loss = 0.4255\n",
      "Iteration 135: Train Loss = 0.4052; Validation Loss = 0.4254\n",
      "Iteration 136: Train Loss = 0.4050; Validation Loss = 0.4253\n",
      "Iteration 137: Train Loss = 0.4047; Validation Loss = 0.4251\n",
      "Iteration 138: Train Loss = 0.4046; Validation Loss = 0.4250\n",
      "Iteration 139: Train Loss = 0.4044; Validation Loss = 0.4249\n",
      "Iteration 140: Train Loss = 0.4043; Validation Loss = 0.4248\n",
      "Iteration 141: Train Loss = 0.4041; Validation Loss = 0.4247\n",
      "Iteration 142: Train Loss = 0.4040; Validation Loss = 0.4246\n",
      "Iteration 143: Train Loss = 0.4038; Validation Loss = 0.4245\n",
      "Iteration 144: Train Loss = 0.4036; Validation Loss = 0.4244\n",
      "Iteration 145: Train Loss = 0.4035; Validation Loss = 0.4242\n",
      "Iteration 146: Train Loss = 0.4033; Validation Loss = 0.4241\n",
      "Iteration 147: Train Loss = 0.4031; Validation Loss = 0.4240\n",
      "Iteration 148: Train Loss = 0.4029; Validation Loss = 0.4239\n",
      "Iteration 149: Train Loss = 0.4028; Validation Loss = 0.4238\n",
      "Iteration 150: Train Loss = 0.4027; Validation Loss = 0.4237\n",
      "Iteration 151: Train Loss = 0.4026; Validation Loss = 0.4237\n",
      "Iteration 152: Train Loss = 0.4025; Validation Loss = 0.4235\n",
      "Iteration 153: Train Loss = 0.4024; Validation Loss = 0.4235\n",
      "Iteration 154: Train Loss = 0.4023; Validation Loss = 0.4234\n",
      "Iteration 155: Train Loss = 0.4022; Validation Loss = 0.4234\n",
      "Iteration 156: Train Loss = 0.4020; Validation Loss = 0.4233\n",
      "Iteration 157: Train Loss = 0.4019; Validation Loss = 0.4232\n",
      "Iteration 158: Train Loss = 0.4018; Validation Loss = 0.4232\n",
      "Iteration 159: Train Loss = 0.4017; Validation Loss = 0.4231\n",
      "Iteration 160: Train Loss = 0.4015; Validation Loss = 0.4230\n",
      "Iteration 161: Train Loss = 0.4014; Validation Loss = 0.4230\n",
      "Iteration 162: Train Loss = 0.4013; Validation Loss = 0.4229\n",
      "Iteration 163: Train Loss = 0.4012; Validation Loss = 0.4228\n",
      "Iteration 164: Train Loss = 0.4012; Validation Loss = 0.4228\n",
      "Iteration 165: Train Loss = 0.4010; Validation Loss = 0.4227\n",
      "Iteration 166: Train Loss = 0.4009; Validation Loss = 0.4226\n",
      "Iteration 167: Train Loss = 0.4009; Validation Loss = 0.4226\n",
      "Iteration 168: Train Loss = 0.4008; Validation Loss = 0.4225\n",
      "Iteration 169: Train Loss = 0.4007; Validation Loss = 0.4225\n",
      "Iteration 170: Train Loss = 0.4005; Validation Loss = 0.4224\n",
      "Iteration 171: Train Loss = 0.4004; Validation Loss = 0.4223\n",
      "Iteration 172: Train Loss = 0.4003; Validation Loss = 0.4223\n",
      "Iteration 173: Train Loss = 0.4002; Validation Loss = 0.4222\n",
      "Iteration 174: Train Loss = 0.4001; Validation Loss = 0.4222\n",
      "Iteration 175: Train Loss = 0.4000; Validation Loss = 0.4222\n",
      "Iteration 176: Train Loss = 0.3999; Validation Loss = 0.4221\n",
      "Iteration 177: Train Loss = 0.3998; Validation Loss = 0.4220\n",
      "Iteration 178: Train Loss = 0.3998; Validation Loss = 0.4220\n",
      "Iteration 179: Train Loss = 0.3997; Validation Loss = 0.4220\n",
      "Iteration 180: Train Loss = 0.3996; Validation Loss = 0.4220\n",
      "Iteration 181: Train Loss = 0.3995; Validation Loss = 0.4219\n",
      "Iteration 182: Train Loss = 0.3994; Validation Loss = 0.4218\n",
      "Iteration 183: Train Loss = 0.3994; Validation Loss = 0.4218\n",
      "Iteration 184: Train Loss = 0.3993; Validation Loss = 0.4217\n",
      "Iteration 185: Train Loss = 0.3992; Validation Loss = 0.4217\n",
      "Iteration 186: Train Loss = 0.3991; Validation Loss = 0.4216\n",
      "Iteration 187: Train Loss = 0.3990; Validation Loss = 0.4216\n",
      "Iteration 188: Train Loss = 0.3990; Validation Loss = 0.4216\n",
      "Iteration 189: Train Loss = 0.3989; Validation Loss = 0.4216\n",
      "Iteration 190: Train Loss = 0.3989; Validation Loss = 0.4216\n",
      "Iteration 191: Train Loss = 0.3988; Validation Loss = 0.4216\n",
      "Iteration 192: Train Loss = 0.3987; Validation Loss = 0.4215\n",
      "Iteration 193: Train Loss = 0.3986; Validation Loss = 0.4215\n",
      "Iteration 194: Train Loss = 0.3985; Validation Loss = 0.4215\n",
      "Iteration 195: Train Loss = 0.3985; Validation Loss = 0.4214\n",
      "Iteration 196: Train Loss = 0.3985; Validation Loss = 0.4214\n",
      "Iteration 197: Train Loss = 0.3984; Validation Loss = 0.4214\n",
      "Iteration 198: Train Loss = 0.3983; Validation Loss = 0.4213\n",
      "Iteration 199: Train Loss = 0.3982; Validation Loss = 0.4213\n",
      "Iteration 200: Train Loss = 0.3981; Validation Loss = 0.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:30:56,981] Trial 7 finished with value: 0.9650713580331645 and parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 200, 'learning_rate': 0.031227245340080506, 'subsample': 0.9780260989099555}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6685; Validation Loss = 0.6699\n",
      "Iteration 2: Train Loss = 0.6460; Validation Loss = 0.6493\n",
      "Iteration 3: Train Loss = 0.6258; Validation Loss = 0.6303\n",
      "Iteration 4: Train Loss = 0.6072; Validation Loss = 0.6131\n",
      "Iteration 5: Train Loss = 0.5905; Validation Loss = 0.5972\n",
      "Iteration 6: Train Loss = 0.5753; Validation Loss = 0.5831\n",
      "Iteration 7: Train Loss = 0.5615; Validation Loss = 0.5704\n",
      "Iteration 8: Train Loss = 0.5487; Validation Loss = 0.5587\n",
      "Iteration 9: Train Loss = 0.5372; Validation Loss = 0.5477\n",
      "Iteration 10: Train Loss = 0.5265; Validation Loss = 0.5378\n",
      "Iteration 11: Train Loss = 0.5168; Validation Loss = 0.5289\n",
      "Iteration 12: Train Loss = 0.5078; Validation Loss = 0.5207\n",
      "Iteration 13: Train Loss = 0.4996; Validation Loss = 0.5133\n",
      "Iteration 14: Train Loss = 0.4922; Validation Loss = 0.5066\n",
      "Iteration 15: Train Loss = 0.4852; Validation Loss = 0.5004\n",
      "Iteration 16: Train Loss = 0.4787; Validation Loss = 0.4945\n",
      "Iteration 17: Train Loss = 0.4728; Validation Loss = 0.4891\n",
      "Iteration 18: Train Loss = 0.4672; Validation Loss = 0.4842\n",
      "Iteration 19: Train Loss = 0.4621; Validation Loss = 0.4797\n",
      "Iteration 20: Train Loss = 0.4575; Validation Loss = 0.4756\n",
      "Iteration 21: Train Loss = 0.4530; Validation Loss = 0.4717\n",
      "Iteration 22: Train Loss = 0.4491; Validation Loss = 0.4681\n",
      "Iteration 23: Train Loss = 0.4452; Validation Loss = 0.4648\n",
      "Iteration 24: Train Loss = 0.4415; Validation Loss = 0.4620\n",
      "Iteration 25: Train Loss = 0.4382; Validation Loss = 0.4592\n",
      "Iteration 26: Train Loss = 0.4352; Validation Loss = 0.4567\n",
      "Iteration 27: Train Loss = 0.4324; Validation Loss = 0.4542\n",
      "Iteration 28: Train Loss = 0.4295; Validation Loss = 0.4516\n",
      "Iteration 29: Train Loss = 0.4271; Validation Loss = 0.4497\n",
      "Iteration 30: Train Loss = 0.4250; Validation Loss = 0.4479\n",
      "Iteration 31: Train Loss = 0.4228; Validation Loss = 0.4460\n",
      "Iteration 32: Train Loss = 0.4207; Validation Loss = 0.4442\n",
      "Iteration 33: Train Loss = 0.4190; Validation Loss = 0.4425\n",
      "Iteration 34: Train Loss = 0.4172; Validation Loss = 0.4410\n",
      "Iteration 35: Train Loss = 0.4155; Validation Loss = 0.4398\n",
      "Iteration 36: Train Loss = 0.4140; Validation Loss = 0.4384\n",
      "Iteration 37: Train Loss = 0.4123; Validation Loss = 0.4374\n",
      "Iteration 38: Train Loss = 0.4110; Validation Loss = 0.4362\n",
      "Iteration 39: Train Loss = 0.4097; Validation Loss = 0.4352\n",
      "Iteration 40: Train Loss = 0.4087; Validation Loss = 0.4344\n",
      "Iteration 41: Train Loss = 0.4074; Validation Loss = 0.4334\n",
      "Iteration 42: Train Loss = 0.4063; Validation Loss = 0.4325\n",
      "Iteration 43: Train Loss = 0.4052; Validation Loss = 0.4316\n",
      "Iteration 44: Train Loss = 0.4042; Validation Loss = 0.4310\n",
      "Iteration 45: Train Loss = 0.4034; Validation Loss = 0.4304\n",
      "Iteration 46: Train Loss = 0.4027; Validation Loss = 0.4300\n",
      "Iteration 47: Train Loss = 0.4019; Validation Loss = 0.4294\n",
      "Iteration 48: Train Loss = 0.4010; Validation Loss = 0.4287\n",
      "Iteration 49: Train Loss = 0.4001; Validation Loss = 0.4281\n",
      "Iteration 50: Train Loss = 0.3994; Validation Loss = 0.4275\n",
      "Iteration 51: Train Loss = 0.3987; Validation Loss = 0.4271\n",
      "Iteration 52: Train Loss = 0.3981; Validation Loss = 0.4266\n",
      "Iteration 53: Train Loss = 0.3977; Validation Loss = 0.4264\n",
      "Iteration 54: Train Loss = 0.3973; Validation Loss = 0.4261\n",
      "Iteration 55: Train Loss = 0.3966; Validation Loss = 0.4259\n",
      "Iteration 56: Train Loss = 0.3963; Validation Loss = 0.4256\n",
      "Iteration 57: Train Loss = 0.3958; Validation Loss = 0.4253\n",
      "Iteration 58: Train Loss = 0.3952; Validation Loss = 0.4249\n",
      "Iteration 59: Train Loss = 0.3949; Validation Loss = 0.4247\n",
      "Iteration 60: Train Loss = 0.3944; Validation Loss = 0.4244\n",
      "Iteration 61: Train Loss = 0.3941; Validation Loss = 0.4241\n",
      "Iteration 62: Train Loss = 0.3935; Validation Loss = 0.4237\n",
      "Iteration 63: Train Loss = 0.3931; Validation Loss = 0.4234\n",
      "Iteration 64: Train Loss = 0.3926; Validation Loss = 0.4232\n",
      "Iteration 65: Train Loss = 0.3923; Validation Loss = 0.4230\n",
      "Iteration 66: Train Loss = 0.3921; Validation Loss = 0.4229\n",
      "Iteration 67: Train Loss = 0.3918; Validation Loss = 0.4228\n",
      "Iteration 68: Train Loss = 0.3915; Validation Loss = 0.4226\n",
      "Iteration 69: Train Loss = 0.3912; Validation Loss = 0.4225\n",
      "Iteration 70: Train Loss = 0.3908; Validation Loss = 0.4224\n",
      "Iteration 71: Train Loss = 0.3905; Validation Loss = 0.4222\n",
      "Iteration 72: Train Loss = 0.3903; Validation Loss = 0.4220\n",
      "Iteration 73: Train Loss = 0.3899; Validation Loss = 0.4218\n",
      "Iteration 74: Train Loss = 0.3898; Validation Loss = 0.4216\n",
      "Iteration 75: Train Loss = 0.3895; Validation Loss = 0.4214\n",
      "Iteration 76: Train Loss = 0.3892; Validation Loss = 0.4215\n",
      "Iteration 77: Train Loss = 0.3890; Validation Loss = 0.4215\n",
      "Iteration 78: Train Loss = 0.3888; Validation Loss = 0.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:31:06,211] Trial 8 finished with value: 0.958900213139944 and parameters: {'max_depth': 11, 'min_samples_leaf': 3, 'n_estimators': 80, 'learning_rate': 0.0684777425734206, 'subsample': 0.5010670382474657}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79: Train Loss = 0.3885; Validation Loss = 0.4212\n",
      "Iteration 80: Train Loss = 0.3883; Validation Loss = 0.4210\n",
      "Iteration 1: Train Loss = 0.5228; Validation Loss = 0.5441\n",
      "Iteration 2: Train Loss = 0.4497; Validation Loss = 0.4831\n",
      "Iteration 3: Train Loss = 0.4143; Validation Loss = 0.4560\n",
      "Iteration 4: Train Loss = 0.3960; Validation Loss = 0.4424\n",
      "Iteration 5: Train Loss = 0.3861; Validation Loss = 0.4356\n",
      "Iteration 6: Train Loss = 0.3806; Validation Loss = 0.4310\n",
      "Iteration 7: Train Loss = 0.3771; Validation Loss = 0.4297\n",
      "Iteration 8: Train Loss = 0.3742; Validation Loss = 0.4290\n",
      "Iteration 9: Train Loss = 0.3723; Validation Loss = 0.4280\n",
      "Iteration 10: Train Loss = 0.3709; Validation Loss = 0.4278\n",
      "Iteration 11: Train Loss = 0.3692; Validation Loss = 0.4286\n",
      "Iteration 12: Train Loss = 0.3660; Validation Loss = 0.4298\n",
      "Iteration 13: Train Loss = 0.3649; Validation Loss = 0.4295\n",
      "Iteration 14: Train Loss = 0.3639; Validation Loss = 0.4298\n",
      "Iteration 15: Train Loss = 0.3634; Validation Loss = 0.4306\n",
      "Iteration 16: Train Loss = 0.3614; Validation Loss = 0.4314\n",
      "Iteration 17: Train Loss = 0.3602; Validation Loss = 0.4311\n",
      "Iteration 18: Train Loss = 0.3590; Validation Loss = 0.4302\n",
      "Iteration 19: Train Loss = 0.3583; Validation Loss = 0.4293\n",
      "Iteration 20: Train Loss = 0.3569; Validation Loss = 0.4304\n",
      "Iteration 21: Train Loss = 0.3555; Validation Loss = 0.4298\n",
      "Early stopping at iteration 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:31:10,332] Trial 9 finished with value: 0.9280566898871005 and parameters: {'max_depth': 14, 'min_samples_leaf': 2, 'n_estimators': 150, 'learning_rate': 0.4927362990810769, 'subsample': 0.8581110547498196}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6882; Validation Loss = 0.6884\n",
      "Iteration 2: Train Loss = 0.6834; Validation Loss = 0.6838\n",
      "Iteration 3: Train Loss = 0.6786; Validation Loss = 0.6792\n",
      "Iteration 4: Train Loss = 0.6740; Validation Loss = 0.6748\n",
      "Iteration 5: Train Loss = 0.6695; Validation Loss = 0.6704\n",
      "Iteration 6: Train Loss = 0.6650; Validation Loss = 0.6661\n",
      "Iteration 7: Train Loss = 0.6607; Validation Loss = 0.6619\n",
      "Iteration 8: Train Loss = 0.6564; Validation Loss = 0.6578\n",
      "Iteration 9: Train Loss = 0.6522; Validation Loss = 0.6538\n",
      "Iteration 10: Train Loss = 0.6481; Validation Loss = 0.6498\n",
      "Iteration 11: Train Loss = 0.6440; Validation Loss = 0.6459\n",
      "Iteration 12: Train Loss = 0.6401; Validation Loss = 0.6421\n",
      "Iteration 13: Train Loss = 0.6362; Validation Loss = 0.6384\n",
      "Iteration 14: Train Loss = 0.6324; Validation Loss = 0.6347\n",
      "Iteration 15: Train Loss = 0.6287; Validation Loss = 0.6312\n",
      "Iteration 16: Train Loss = 0.6251; Validation Loss = 0.6277\n",
      "Iteration 17: Train Loss = 0.6215; Validation Loss = 0.6244\n",
      "Iteration 18: Train Loss = 0.6180; Validation Loss = 0.6210\n",
      "Iteration 19: Train Loss = 0.6145; Validation Loss = 0.6178\n",
      "Iteration 20: Train Loss = 0.6111; Validation Loss = 0.6145\n",
      "Iteration 21: Train Loss = 0.6078; Validation Loss = 0.6114\n",
      "Iteration 22: Train Loss = 0.6045; Validation Loss = 0.6082\n",
      "Iteration 23: Train Loss = 0.6013; Validation Loss = 0.6052\n",
      "Iteration 24: Train Loss = 0.5981; Validation Loss = 0.6022\n",
      "Iteration 25: Train Loss = 0.5950; Validation Loss = 0.5993\n",
      "Iteration 26: Train Loss = 0.5920; Validation Loss = 0.5964\n",
      "Iteration 27: Train Loss = 0.5890; Validation Loss = 0.5935\n",
      "Iteration 28: Train Loss = 0.5861; Validation Loss = 0.5907\n",
      "Iteration 29: Train Loss = 0.5832; Validation Loss = 0.5880\n",
      "Iteration 30: Train Loss = 0.5804; Validation Loss = 0.5853\n",
      "Iteration 31: Train Loss = 0.5776; Validation Loss = 0.5827\n",
      "Iteration 32: Train Loss = 0.5750; Validation Loss = 0.5801\n",
      "Iteration 33: Train Loss = 0.5723; Validation Loss = 0.5775\n",
      "Iteration 34: Train Loss = 0.5697; Validation Loss = 0.5751\n",
      "Iteration 35: Train Loss = 0.5671; Validation Loss = 0.5727\n",
      "Iteration 36: Train Loss = 0.5645; Validation Loss = 0.5703\n",
      "Iteration 37: Train Loss = 0.5621; Validation Loss = 0.5679\n",
      "Iteration 38: Train Loss = 0.5597; Validation Loss = 0.5657\n",
      "Iteration 39: Train Loss = 0.5573; Validation Loss = 0.5634\n",
      "Iteration 40: Train Loss = 0.5549; Validation Loss = 0.5612\n",
      "Iteration 41: Train Loss = 0.5526; Validation Loss = 0.5590\n",
      "Iteration 42: Train Loss = 0.5503; Validation Loss = 0.5569\n",
      "Iteration 43: Train Loss = 0.5481; Validation Loss = 0.5548\n",
      "Iteration 44: Train Loss = 0.5459; Validation Loss = 0.5528\n",
      "Iteration 45: Train Loss = 0.5438; Validation Loss = 0.5507\n",
      "Iteration 46: Train Loss = 0.5416; Validation Loss = 0.5487\n",
      "Iteration 47: Train Loss = 0.5396; Validation Loss = 0.5468\n",
      "Iteration 48: Train Loss = 0.5376; Validation Loss = 0.5449\n",
      "Iteration 49: Train Loss = 0.5356; Validation Loss = 0.5430\n",
      "Iteration 50: Train Loss = 0.5336; Validation Loss = 0.5412\n",
      "Iteration 51: Train Loss = 0.5317; Validation Loss = 0.5393\n",
      "Iteration 52: Train Loss = 0.5298; Validation Loss = 0.5376\n",
      "Iteration 53: Train Loss = 0.5279; Validation Loss = 0.5358\n",
      "Iteration 54: Train Loss = 0.5261; Validation Loss = 0.5341\n",
      "Iteration 55: Train Loss = 0.5243; Validation Loss = 0.5324\n",
      "Iteration 56: Train Loss = 0.5225; Validation Loss = 0.5308\n",
      "Iteration 57: Train Loss = 0.5208; Validation Loss = 0.5292\n",
      "Iteration 58: Train Loss = 0.5190; Validation Loss = 0.5276\n",
      "Iteration 59: Train Loss = 0.5174; Validation Loss = 0.5260\n",
      "Iteration 60: Train Loss = 0.5157; Validation Loss = 0.5245\n",
      "Iteration 61: Train Loss = 0.5140; Validation Loss = 0.5229\n",
      "Iteration 62: Train Loss = 0.5124; Validation Loss = 0.5214\n",
      "Iteration 63: Train Loss = 0.5108; Validation Loss = 0.5199\n",
      "Iteration 64: Train Loss = 0.5093; Validation Loss = 0.5185\n",
      "Iteration 65: Train Loss = 0.5078; Validation Loss = 0.5170\n",
      "Iteration 66: Train Loss = 0.5063; Validation Loss = 0.5156\n",
      "Iteration 67: Train Loss = 0.5048; Validation Loss = 0.5143\n",
      "Iteration 68: Train Loss = 0.5033; Validation Loss = 0.5129\n",
      "Iteration 69: Train Loss = 0.5019; Validation Loss = 0.5116\n",
      "Iteration 70: Train Loss = 0.5005; Validation Loss = 0.5103\n",
      "Iteration 71: Train Loss = 0.4990; Validation Loss = 0.5089\n",
      "Iteration 72: Train Loss = 0.4976; Validation Loss = 0.5076\n",
      "Iteration 73: Train Loss = 0.4963; Validation Loss = 0.5064\n",
      "Iteration 74: Train Loss = 0.4949; Validation Loss = 0.5051\n",
      "Iteration 75: Train Loss = 0.4936; Validation Loss = 0.5039\n",
      "Iteration 76: Train Loss = 0.4923; Validation Loss = 0.5026\n",
      "Iteration 77: Train Loss = 0.4910; Validation Loss = 0.5015\n",
      "Iteration 78: Train Loss = 0.4898; Validation Loss = 0.5003\n",
      "Iteration 79: Train Loss = 0.4885; Validation Loss = 0.4991\n",
      "Iteration 80: Train Loss = 0.4873; Validation Loss = 0.4980\n",
      "Iteration 81: Train Loss = 0.4861; Validation Loss = 0.4969\n",
      "Iteration 82: Train Loss = 0.4849; Validation Loss = 0.4958\n",
      "Iteration 83: Train Loss = 0.4838; Validation Loss = 0.4947\n",
      "Iteration 84: Train Loss = 0.4826; Validation Loss = 0.4937\n",
      "Iteration 85: Train Loss = 0.4815; Validation Loss = 0.4926\n",
      "Iteration 86: Train Loss = 0.4804; Validation Loss = 0.4916\n",
      "Iteration 87: Train Loss = 0.4794; Validation Loss = 0.4906\n",
      "Iteration 88: Train Loss = 0.4783; Validation Loss = 0.4896\n",
      "Iteration 89: Train Loss = 0.4773; Validation Loss = 0.4887\n",
      "Iteration 90: Train Loss = 0.4763; Validation Loss = 0.4878\n",
      "Iteration 91: Train Loss = 0.4753; Validation Loss = 0.4869\n",
      "Iteration 92: Train Loss = 0.4743; Validation Loss = 0.4860\n",
      "Iteration 93: Train Loss = 0.4733; Validation Loss = 0.4851\n",
      "Iteration 94: Train Loss = 0.4724; Validation Loss = 0.4842\n",
      "Iteration 95: Train Loss = 0.4714; Validation Loss = 0.4834\n",
      "Iteration 96: Train Loss = 0.4705; Validation Loss = 0.4825\n",
      "Iteration 97: Train Loss = 0.4695; Validation Loss = 0.4817\n",
      "Iteration 98: Train Loss = 0.4686; Validation Loss = 0.4808\n",
      "Iteration 99: Train Loss = 0.4677; Validation Loss = 0.4800\n",
      "Iteration 100: Train Loss = 0.4668; Validation Loss = 0.4792\n",
      "Iteration 101: Train Loss = 0.4659; Validation Loss = 0.4784\n",
      "Iteration 102: Train Loss = 0.4650; Validation Loss = 0.4776\n",
      "Iteration 103: Train Loss = 0.4641; Validation Loss = 0.4768\n",
      "Iteration 104: Train Loss = 0.4633; Validation Loss = 0.4760\n",
      "Iteration 105: Train Loss = 0.4624; Validation Loss = 0.4753\n",
      "Iteration 106: Train Loss = 0.4616; Validation Loss = 0.4746\n",
      "Iteration 107: Train Loss = 0.4608; Validation Loss = 0.4739\n",
      "Iteration 108: Train Loss = 0.4600; Validation Loss = 0.4732\n",
      "Iteration 109: Train Loss = 0.4592; Validation Loss = 0.4725\n",
      "Iteration 110: Train Loss = 0.4585; Validation Loss = 0.4718\n",
      "Iteration 111: Train Loss = 0.4577; Validation Loss = 0.4711\n",
      "Iteration 112: Train Loss = 0.4570; Validation Loss = 0.4704\n",
      "Iteration 113: Train Loss = 0.4563; Validation Loss = 0.4697\n",
      "Iteration 114: Train Loss = 0.4555; Validation Loss = 0.4691\n",
      "Iteration 115: Train Loss = 0.4548; Validation Loss = 0.4684\n",
      "Iteration 116: Train Loss = 0.4541; Validation Loss = 0.4678\n",
      "Iteration 117: Train Loss = 0.4534; Validation Loss = 0.4672\n",
      "Iteration 118: Train Loss = 0.4528; Validation Loss = 0.4666\n",
      "Iteration 119: Train Loss = 0.4522; Validation Loss = 0.4660\n",
      "Iteration 120: Train Loss = 0.4515; Validation Loss = 0.4654\n",
      "Iteration 121: Train Loss = 0.4508; Validation Loss = 0.4648\n",
      "Iteration 122: Train Loss = 0.4502; Validation Loss = 0.4642\n",
      "Iteration 123: Train Loss = 0.4495; Validation Loss = 0.4636\n",
      "Iteration 124: Train Loss = 0.4488; Validation Loss = 0.4631\n",
      "Iteration 125: Train Loss = 0.4482; Validation Loss = 0.4625\n",
      "Iteration 126: Train Loss = 0.4475; Validation Loss = 0.4620\n",
      "Iteration 127: Train Loss = 0.4469; Validation Loss = 0.4614\n",
      "Iteration 128: Train Loss = 0.4463; Validation Loss = 0.4609\n",
      "Iteration 129: Train Loss = 0.4457; Validation Loss = 0.4604\n",
      "Iteration 130: Train Loss = 0.4451; Validation Loss = 0.4599\n",
      "Iteration 131: Train Loss = 0.4446; Validation Loss = 0.4594\n",
      "Iteration 132: Train Loss = 0.4440; Validation Loss = 0.4589\n",
      "Iteration 133: Train Loss = 0.4435; Validation Loss = 0.4584\n",
      "Iteration 134: Train Loss = 0.4429; Validation Loss = 0.4578\n",
      "Iteration 135: Train Loss = 0.4423; Validation Loss = 0.4573\n",
      "Iteration 136: Train Loss = 0.4418; Validation Loss = 0.4569\n",
      "Iteration 137: Train Loss = 0.4412; Validation Loss = 0.4564\n",
      "Iteration 138: Train Loss = 0.4407; Validation Loss = 0.4559\n",
      "Iteration 139: Train Loss = 0.4402; Validation Loss = 0.4555\n",
      "Iteration 140: Train Loss = 0.4397; Validation Loss = 0.4550\n",
      "Iteration 141: Train Loss = 0.4392; Validation Loss = 0.4546\n",
      "Iteration 142: Train Loss = 0.4387; Validation Loss = 0.4542\n",
      "Iteration 143: Train Loss = 0.4382; Validation Loss = 0.4538\n",
      "Iteration 144: Train Loss = 0.4378; Validation Loss = 0.4534\n",
      "Iteration 145: Train Loss = 0.4373; Validation Loss = 0.4530\n",
      "Iteration 146: Train Loss = 0.4368; Validation Loss = 0.4526\n",
      "Iteration 147: Train Loss = 0.4363; Validation Loss = 0.4522\n",
      "Iteration 148: Train Loss = 0.4358; Validation Loss = 0.4518\n",
      "Iteration 149: Train Loss = 0.4354; Validation Loss = 0.4514\n",
      "Iteration 150: Train Loss = 0.4349; Validation Loss = 0.4509\n",
      "Iteration 151: Train Loss = 0.4345; Validation Loss = 0.4506\n",
      "Iteration 152: Train Loss = 0.4340; Validation Loss = 0.4502\n",
      "Iteration 153: Train Loss = 0.4336; Validation Loss = 0.4498\n",
      "Iteration 154: Train Loss = 0.4332; Validation Loss = 0.4495\n",
      "Iteration 155: Train Loss = 0.4328; Validation Loss = 0.4491\n",
      "Iteration 156: Train Loss = 0.4323; Validation Loss = 0.4487\n",
      "Iteration 157: Train Loss = 0.4319; Validation Loss = 0.4484\n",
      "Iteration 158: Train Loss = 0.4315; Validation Loss = 0.4480\n",
      "Iteration 159: Train Loss = 0.4311; Validation Loss = 0.4476\n",
      "Iteration 160: Train Loss = 0.4307; Validation Loss = 0.4473\n",
      "Iteration 161: Train Loss = 0.4303; Validation Loss = 0.4469\n",
      "Iteration 162: Train Loss = 0.4299; Validation Loss = 0.4466\n",
      "Iteration 163: Train Loss = 0.4296; Validation Loss = 0.4463\n",
      "Iteration 164: Train Loss = 0.4292; Validation Loss = 0.4460\n",
      "Iteration 165: Train Loss = 0.4289; Validation Loss = 0.4457\n",
      "Iteration 166: Train Loss = 0.4285; Validation Loss = 0.4454\n",
      "Iteration 167: Train Loss = 0.4282; Validation Loss = 0.4451\n",
      "Iteration 168: Train Loss = 0.4278; Validation Loss = 0.4447\n",
      "Iteration 169: Train Loss = 0.4275; Validation Loss = 0.4445\n",
      "Iteration 170: Train Loss = 0.4271; Validation Loss = 0.4442\n",
      "Iteration 171: Train Loss = 0.4268; Validation Loss = 0.4439\n",
      "Iteration 172: Train Loss = 0.4264; Validation Loss = 0.4436\n",
      "Iteration 173: Train Loss = 0.4261; Validation Loss = 0.4433\n",
      "Iteration 174: Train Loss = 0.4258; Validation Loss = 0.4430\n",
      "Iteration 175: Train Loss = 0.4254; Validation Loss = 0.4427\n",
      "Iteration 176: Train Loss = 0.4251; Validation Loss = 0.4425\n",
      "Iteration 177: Train Loss = 0.4248; Validation Loss = 0.4422\n",
      "Iteration 178: Train Loss = 0.4245; Validation Loss = 0.4419\n",
      "Iteration 179: Train Loss = 0.4242; Validation Loss = 0.4417\n",
      "Iteration 180: Train Loss = 0.4239; Validation Loss = 0.4415\n",
      "Iteration 181: Train Loss = 0.4236; Validation Loss = 0.4412\n",
      "Iteration 182: Train Loss = 0.4233; Validation Loss = 0.4410\n",
      "Iteration 183: Train Loss = 0.4230; Validation Loss = 0.4408\n",
      "Iteration 184: Train Loss = 0.4228; Validation Loss = 0.4406\n",
      "Iteration 185: Train Loss = 0.4225; Validation Loss = 0.4403\n",
      "Iteration 186: Train Loss = 0.4222; Validation Loss = 0.4401\n",
      "Iteration 187: Train Loss = 0.4219; Validation Loss = 0.4398\n",
      "Iteration 188: Train Loss = 0.4217; Validation Loss = 0.4396\n",
      "Iteration 189: Train Loss = 0.4214; Validation Loss = 0.4394\n",
      "Iteration 190: Train Loss = 0.4212; Validation Loss = 0.4392\n",
      "Iteration 191: Train Loss = 0.4209; Validation Loss = 0.4389\n",
      "Iteration 192: Train Loss = 0.4207; Validation Loss = 0.4387\n",
      "Iteration 193: Train Loss = 0.4204; Validation Loss = 0.4385\n",
      "Iteration 194: Train Loss = 0.4201; Validation Loss = 0.4382\n",
      "Iteration 195: Train Loss = 0.4198; Validation Loss = 0.4380\n",
      "Iteration 196: Train Loss = 0.4196; Validation Loss = 0.4378\n",
      "Iteration 197: Train Loss = 0.4194; Validation Loss = 0.4376\n",
      "Iteration 198: Train Loss = 0.4191; Validation Loss = 0.4374\n",
      "Iteration 199: Train Loss = 0.4189; Validation Loss = 0.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:31:48,900] Trial 10 finished with value: 0.9638722075426377 and parameters: {'max_depth': 9, 'min_samples_leaf': 10, 'n_estimators': 200, 'learning_rate': 0.013887379469088829, 'subsample': 0.9811618687743707}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200: Train Loss = 0.4187; Validation Loss = 0.4371\n",
      "Iteration 1: Train Loss = 0.6858; Validation Loss = 0.6861\n",
      "Iteration 2: Train Loss = 0.6789; Validation Loss = 0.6794\n",
      "Iteration 3: Train Loss = 0.6721; Validation Loss = 0.6728\n",
      "Iteration 4: Train Loss = 0.6655; Validation Loss = 0.6664\n",
      "Iteration 5: Train Loss = 0.6591; Validation Loss = 0.6602\n",
      "Iteration 6: Train Loss = 0.6529; Validation Loss = 0.6542\n",
      "Iteration 7: Train Loss = 0.6468; Validation Loss = 0.6484\n",
      "Iteration 8: Train Loss = 0.6410; Validation Loss = 0.6428\n",
      "Iteration 9: Train Loss = 0.6352; Validation Loss = 0.6373\n",
      "Iteration 10: Train Loss = 0.6297; Validation Loss = 0.6319\n",
      "Iteration 11: Train Loss = 0.6244; Validation Loss = 0.6267\n",
      "Iteration 12: Train Loss = 0.6192; Validation Loss = 0.6217\n",
      "Iteration 13: Train Loss = 0.6141; Validation Loss = 0.6167\n",
      "Iteration 14: Train Loss = 0.6093; Validation Loss = 0.6120\n",
      "Iteration 15: Train Loss = 0.6045; Validation Loss = 0.6075\n",
      "Iteration 16: Train Loss = 0.5999; Validation Loss = 0.6029\n",
      "Iteration 17: Train Loss = 0.5954; Validation Loss = 0.5985\n",
      "Iteration 18: Train Loss = 0.5911; Validation Loss = 0.5943\n",
      "Iteration 19: Train Loss = 0.5868; Validation Loss = 0.5903\n",
      "Iteration 20: Train Loss = 0.5827; Validation Loss = 0.5863\n",
      "Iteration 21: Train Loss = 0.5788; Validation Loss = 0.5824\n",
      "Iteration 22: Train Loss = 0.5748; Validation Loss = 0.5787\n",
      "Iteration 23: Train Loss = 0.5711; Validation Loss = 0.5750\n",
      "Iteration 24: Train Loss = 0.5674; Validation Loss = 0.5715\n",
      "Iteration 25: Train Loss = 0.5638; Validation Loss = 0.5681\n",
      "Iteration 26: Train Loss = 0.5603; Validation Loss = 0.5647\n",
      "Iteration 27: Train Loss = 0.5569; Validation Loss = 0.5615\n",
      "Iteration 28: Train Loss = 0.5537; Validation Loss = 0.5584\n",
      "Iteration 29: Train Loss = 0.5504; Validation Loss = 0.5552\n",
      "Iteration 30: Train Loss = 0.5472; Validation Loss = 0.5522\n",
      "Iteration 31: Train Loss = 0.5441; Validation Loss = 0.5493\n",
      "Iteration 32: Train Loss = 0.5411; Validation Loss = 0.5464\n",
      "Iteration 33: Train Loss = 0.5382; Validation Loss = 0.5436\n",
      "Iteration 34: Train Loss = 0.5353; Validation Loss = 0.5408\n",
      "Iteration 35: Train Loss = 0.5325; Validation Loss = 0.5382\n",
      "Iteration 36: Train Loss = 0.5298; Validation Loss = 0.5356\n",
      "Iteration 37: Train Loss = 0.5271; Validation Loss = 0.5331\n",
      "Iteration 38: Train Loss = 0.5246; Validation Loss = 0.5306\n",
      "Iteration 39: Train Loss = 0.5221; Validation Loss = 0.5283\n",
      "Iteration 40: Train Loss = 0.5196; Validation Loss = 0.5260\n",
      "Iteration 41: Train Loss = 0.5173; Validation Loss = 0.5237\n",
      "Iteration 42: Train Loss = 0.5149; Validation Loss = 0.5215\n",
      "Iteration 43: Train Loss = 0.5127; Validation Loss = 0.5193\n",
      "Iteration 44: Train Loss = 0.5105; Validation Loss = 0.5172\n",
      "Iteration 45: Train Loss = 0.5084; Validation Loss = 0.5153\n",
      "Iteration 46: Train Loss = 0.5063; Validation Loss = 0.5133\n",
      "Iteration 47: Train Loss = 0.5043; Validation Loss = 0.5113\n",
      "Iteration 48: Train Loss = 0.5023; Validation Loss = 0.5094\n",
      "Iteration 49: Train Loss = 0.5004; Validation Loss = 0.5076\n",
      "Iteration 50: Train Loss = 0.4985; Validation Loss = 0.5058\n",
      "Iteration 51: Train Loss = 0.4966; Validation Loss = 0.5040\n",
      "Iteration 52: Train Loss = 0.4948; Validation Loss = 0.5023\n",
      "Iteration 53: Train Loss = 0.4930; Validation Loss = 0.5007\n",
      "Iteration 54: Train Loss = 0.4913; Validation Loss = 0.4991\n",
      "Iteration 55: Train Loss = 0.4895; Validation Loss = 0.4975\n",
      "Iteration 56: Train Loss = 0.4879; Validation Loss = 0.4959\n",
      "Iteration 57: Train Loss = 0.4862; Validation Loss = 0.4944\n",
      "Iteration 58: Train Loss = 0.4846; Validation Loss = 0.4929\n",
      "Iteration 59: Train Loss = 0.4831; Validation Loss = 0.4914\n",
      "Iteration 60: Train Loss = 0.4815; Validation Loss = 0.4900\n",
      "Iteration 61: Train Loss = 0.4801; Validation Loss = 0.4886\n",
      "Iteration 62: Train Loss = 0.4786; Validation Loss = 0.4873\n",
      "Iteration 63: Train Loss = 0.4772; Validation Loss = 0.4859\n",
      "Iteration 64: Train Loss = 0.4758; Validation Loss = 0.4846\n",
      "Iteration 65: Train Loss = 0.4745; Validation Loss = 0.4833\n",
      "Iteration 66: Train Loss = 0.4731; Validation Loss = 0.4821\n",
      "Iteration 67: Train Loss = 0.4719; Validation Loss = 0.4810\n",
      "Iteration 68: Train Loss = 0.4707; Validation Loss = 0.4797\n",
      "Iteration 69: Train Loss = 0.4695; Validation Loss = 0.4786\n",
      "Iteration 70: Train Loss = 0.4683; Validation Loss = 0.4775\n",
      "Iteration 71: Train Loss = 0.4672; Validation Loss = 0.4765\n",
      "Iteration 72: Train Loss = 0.4660; Validation Loss = 0.4755\n",
      "Iteration 73: Train Loss = 0.4650; Validation Loss = 0.4746\n",
      "Iteration 74: Train Loss = 0.4639; Validation Loss = 0.4735\n",
      "Iteration 75: Train Loss = 0.4627; Validation Loss = 0.4725\n",
      "Iteration 76: Train Loss = 0.4618; Validation Loss = 0.4716\n",
      "Iteration 77: Train Loss = 0.4607; Validation Loss = 0.4706\n",
      "Iteration 78: Train Loss = 0.4596; Validation Loss = 0.4697\n",
      "Iteration 79: Train Loss = 0.4587; Validation Loss = 0.4688\n",
      "Iteration 80: Train Loss = 0.4577; Validation Loss = 0.4678\n",
      "Iteration 81: Train Loss = 0.4567; Validation Loss = 0.4669\n",
      "Iteration 82: Train Loss = 0.4558; Validation Loss = 0.4661\n",
      "Iteration 83: Train Loss = 0.4549; Validation Loss = 0.4653\n",
      "Iteration 84: Train Loss = 0.4540; Validation Loss = 0.4644\n",
      "Iteration 85: Train Loss = 0.4531; Validation Loss = 0.4636\n",
      "Iteration 86: Train Loss = 0.4522; Validation Loss = 0.4628\n",
      "Iteration 87: Train Loss = 0.4514; Validation Loss = 0.4621\n",
      "Iteration 88: Train Loss = 0.4506; Validation Loss = 0.4613\n",
      "Iteration 89: Train Loss = 0.4497; Validation Loss = 0.4605\n",
      "Iteration 90: Train Loss = 0.4490; Validation Loss = 0.4599\n",
      "Iteration 91: Train Loss = 0.4483; Validation Loss = 0.4591\n",
      "Iteration 92: Train Loss = 0.4475; Validation Loss = 0.4584\n",
      "Iteration 93: Train Loss = 0.4467; Validation Loss = 0.4577\n",
      "Iteration 94: Train Loss = 0.4460; Validation Loss = 0.4571\n",
      "Iteration 95: Train Loss = 0.4453; Validation Loss = 0.4564\n",
      "Iteration 96: Train Loss = 0.4446; Validation Loss = 0.4558\n",
      "Iteration 97: Train Loss = 0.4438; Validation Loss = 0.4551\n",
      "Iteration 98: Train Loss = 0.4432; Validation Loss = 0.4544\n",
      "Iteration 99: Train Loss = 0.4425; Validation Loss = 0.4538\n",
      "Iteration 100: Train Loss = 0.4419; Validation Loss = 0.4532\n",
      "Iteration 101: Train Loss = 0.4413; Validation Loss = 0.4526\n",
      "Iteration 102: Train Loss = 0.4407; Validation Loss = 0.4521\n",
      "Iteration 103: Train Loss = 0.4400; Validation Loss = 0.4515\n",
      "Iteration 104: Train Loss = 0.4395; Validation Loss = 0.4510\n",
      "Iteration 105: Train Loss = 0.4388; Validation Loss = 0.4505\n",
      "Iteration 106: Train Loss = 0.4383; Validation Loss = 0.4499\n",
      "Iteration 107: Train Loss = 0.4377; Validation Loss = 0.4494\n",
      "Iteration 108: Train Loss = 0.4372; Validation Loss = 0.4489\n",
      "Iteration 109: Train Loss = 0.4366; Validation Loss = 0.4484\n",
      "Iteration 110: Train Loss = 0.4361; Validation Loss = 0.4479\n",
      "Iteration 111: Train Loss = 0.4356; Validation Loss = 0.4475\n",
      "Iteration 112: Train Loss = 0.4350; Validation Loss = 0.4470\n",
      "Iteration 113: Train Loss = 0.4345; Validation Loss = 0.4465\n",
      "Iteration 114: Train Loss = 0.4340; Validation Loss = 0.4461\n",
      "Iteration 115: Train Loss = 0.4335; Validation Loss = 0.4457\n",
      "Iteration 116: Train Loss = 0.4331; Validation Loss = 0.4452\n",
      "Iteration 117: Train Loss = 0.4326; Validation Loss = 0.4448\n",
      "Iteration 118: Train Loss = 0.4321; Validation Loss = 0.4443\n",
      "Iteration 119: Train Loss = 0.4316; Validation Loss = 0.4439\n",
      "Iteration 120: Train Loss = 0.4313; Validation Loss = 0.4436\n",
      "Iteration 121: Train Loss = 0.4309; Validation Loss = 0.4433\n",
      "Iteration 122: Train Loss = 0.4305; Validation Loss = 0.4429\n",
      "Iteration 123: Train Loss = 0.4300; Validation Loss = 0.4425\n",
      "Iteration 124: Train Loss = 0.4296; Validation Loss = 0.4422\n",
      "Iteration 125: Train Loss = 0.4292; Validation Loss = 0.4418\n",
      "Iteration 126: Train Loss = 0.4288; Validation Loss = 0.4414\n",
      "Iteration 127: Train Loss = 0.4284; Validation Loss = 0.4411\n",
      "Iteration 128: Train Loss = 0.4280; Validation Loss = 0.4407\n",
      "Iteration 129: Train Loss = 0.4276; Validation Loss = 0.4404\n",
      "Iteration 130: Train Loss = 0.4272; Validation Loss = 0.4401\n",
      "Iteration 131: Train Loss = 0.4268; Validation Loss = 0.4397\n",
      "Iteration 132: Train Loss = 0.4265; Validation Loss = 0.4395\n",
      "Iteration 133: Train Loss = 0.4261; Validation Loss = 0.4392\n",
      "Iteration 134: Train Loss = 0.4258; Validation Loss = 0.4389\n",
      "Iteration 135: Train Loss = 0.4254; Validation Loss = 0.4386\n",
      "Iteration 136: Train Loss = 0.4251; Validation Loss = 0.4383\n",
      "Iteration 137: Train Loss = 0.4248; Validation Loss = 0.4380\n",
      "Iteration 138: Train Loss = 0.4245; Validation Loss = 0.4377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:32:06,367] Trial 11 finished with value: 0.964370932134564 and parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'n_estimators': 140, 'learning_rate': 0.021182645968652707, 'subsample': 0.7415538006702035}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 139: Train Loss = 0.4242; Validation Loss = 0.4375\n",
      "Iteration 140: Train Loss = 0.4239; Validation Loss = 0.4372\n",
      "Iteration 1: Train Loss = 0.6650; Validation Loss = 0.6664\n",
      "Iteration 2: Train Loss = 0.6399; Validation Loss = 0.6427\n",
      "Iteration 3: Train Loss = 0.6180; Validation Loss = 0.6216\n",
      "Iteration 4: Train Loss = 0.5980; Validation Loss = 0.6024\n",
      "Iteration 5: Train Loss = 0.5804; Validation Loss = 0.5856\n",
      "Iteration 6: Train Loss = 0.5644; Validation Loss = 0.5705\n",
      "Iteration 7: Train Loss = 0.5501; Validation Loss = 0.5570\n",
      "Iteration 8: Train Loss = 0.5372; Validation Loss = 0.5446\n",
      "Iteration 9: Train Loss = 0.5255; Validation Loss = 0.5340\n",
      "Iteration 10: Train Loss = 0.5151; Validation Loss = 0.5244\n",
      "Iteration 11: Train Loss = 0.5056; Validation Loss = 0.5156\n",
      "Iteration 12: Train Loss = 0.4970; Validation Loss = 0.5078\n",
      "Iteration 13: Train Loss = 0.4890; Validation Loss = 0.5004\n",
      "Iteration 14: Train Loss = 0.4819; Validation Loss = 0.4937\n",
      "Iteration 15: Train Loss = 0.4753; Validation Loss = 0.4878\n",
      "Iteration 16: Train Loss = 0.4695; Validation Loss = 0.4824\n",
      "Iteration 17: Train Loss = 0.4638; Validation Loss = 0.4776\n",
      "Iteration 18: Train Loss = 0.4588; Validation Loss = 0.4733\n",
      "Iteration 19: Train Loss = 0.4542; Validation Loss = 0.4690\n",
      "Iteration 20: Train Loss = 0.4500; Validation Loss = 0.4651\n",
      "Iteration 21: Train Loss = 0.4462; Validation Loss = 0.4618\n",
      "Iteration 22: Train Loss = 0.4425; Validation Loss = 0.4588\n",
      "Iteration 23: Train Loss = 0.4393; Validation Loss = 0.4561\n",
      "Iteration 24: Train Loss = 0.4363; Validation Loss = 0.4534\n",
      "Iteration 25: Train Loss = 0.4337; Validation Loss = 0.4510\n",
      "Iteration 26: Train Loss = 0.4312; Validation Loss = 0.4488\n",
      "Iteration 27: Train Loss = 0.4288; Validation Loss = 0.4468\n",
      "Iteration 28: Train Loss = 0.4267; Validation Loss = 0.4449\n",
      "Iteration 29: Train Loss = 0.4249; Validation Loss = 0.4435\n",
      "Iteration 30: Train Loss = 0.4230; Validation Loss = 0.4419\n",
      "Iteration 31: Train Loss = 0.4213; Validation Loss = 0.4405\n",
      "Iteration 32: Train Loss = 0.4195; Validation Loss = 0.4391\n",
      "Iteration 33: Train Loss = 0.4180; Validation Loss = 0.4378\n",
      "Iteration 34: Train Loss = 0.4166; Validation Loss = 0.4367\n",
      "Iteration 35: Train Loss = 0.4152; Validation Loss = 0.4356\n",
      "Iteration 36: Train Loss = 0.4139; Validation Loss = 0.4344\n",
      "Iteration 37: Train Loss = 0.4130; Validation Loss = 0.4338\n",
      "Iteration 38: Train Loss = 0.4119; Validation Loss = 0.4331\n",
      "Iteration 39: Train Loss = 0.4108; Validation Loss = 0.4323\n",
      "Iteration 40: Train Loss = 0.4098; Validation Loss = 0.4315\n",
      "Iteration 41: Train Loss = 0.4089; Validation Loss = 0.4307\n",
      "Iteration 42: Train Loss = 0.4080; Validation Loss = 0.4300\n",
      "Iteration 43: Train Loss = 0.4072; Validation Loss = 0.4293\n",
      "Iteration 44: Train Loss = 0.4065; Validation Loss = 0.4287\n",
      "Iteration 45: Train Loss = 0.4057; Validation Loss = 0.4282\n",
      "Iteration 46: Train Loss = 0.4052; Validation Loss = 0.4277\n",
      "Iteration 47: Train Loss = 0.4045; Validation Loss = 0.4271\n",
      "Iteration 48: Train Loss = 0.4039; Validation Loss = 0.4267\n",
      "Iteration 49: Train Loss = 0.4034; Validation Loss = 0.4264\n",
      "Iteration 50: Train Loss = 0.4028; Validation Loss = 0.4261\n",
      "Iteration 51: Train Loss = 0.4020; Validation Loss = 0.4256\n",
      "Iteration 52: Train Loss = 0.4016; Validation Loss = 0.4251\n",
      "Iteration 53: Train Loss = 0.4010; Validation Loss = 0.4247\n",
      "Iteration 54: Train Loss = 0.4007; Validation Loss = 0.4244\n",
      "Iteration 55: Train Loss = 0.4004; Validation Loss = 0.4243\n",
      "Iteration 56: Train Loss = 0.3999; Validation Loss = 0.4237\n",
      "Iteration 57: Train Loss = 0.3997; Validation Loss = 0.4237\n",
      "Iteration 58: Train Loss = 0.3995; Validation Loss = 0.4236\n",
      "Iteration 59: Train Loss = 0.3991; Validation Loss = 0.4233\n",
      "Iteration 60: Train Loss = 0.3987; Validation Loss = 0.4230\n",
      "Iteration 61: Train Loss = 0.3986; Validation Loss = 0.4229\n",
      "Iteration 62: Train Loss = 0.3983; Validation Loss = 0.4228\n",
      "Iteration 63: Train Loss = 0.3979; Validation Loss = 0.4226\n",
      "Iteration 64: Train Loss = 0.3976; Validation Loss = 0.4226\n",
      "Iteration 65: Train Loss = 0.3973; Validation Loss = 0.4223\n",
      "Iteration 66: Train Loss = 0.3969; Validation Loss = 0.4222\n",
      "Iteration 67: Train Loss = 0.3965; Validation Loss = 0.4220\n",
      "Iteration 68: Train Loss = 0.3963; Validation Loss = 0.4219\n",
      "Iteration 69: Train Loss = 0.3960; Validation Loss = 0.4218\n",
      "Iteration 70: Train Loss = 0.3958; Validation Loss = 0.4216\n",
      "Iteration 71: Train Loss = 0.3957; Validation Loss = 0.4215\n",
      "Iteration 72: Train Loss = 0.3956; Validation Loss = 0.4215\n",
      "Iteration 73: Train Loss = 0.3953; Validation Loss = 0.4214\n",
      "Iteration 74: Train Loss = 0.3952; Validation Loss = 0.4214\n",
      "Iteration 75: Train Loss = 0.3950; Validation Loss = 0.4213\n",
      "Iteration 76: Train Loss = 0.3946; Validation Loss = 0.4211\n",
      "Iteration 77: Train Loss = 0.3944; Validation Loss = 0.4210\n",
      "Iteration 78: Train Loss = 0.3944; Validation Loss = 0.4209\n",
      "Iteration 79: Train Loss = 0.3942; Validation Loss = 0.4209\n",
      "Iteration 80: Train Loss = 0.3941; Validation Loss = 0.4209\n",
      "Iteration 81: Train Loss = 0.3940; Validation Loss = 0.4209\n",
      "Iteration 82: Train Loss = 0.3939; Validation Loss = 0.4209\n",
      "Iteration 83: Train Loss = 0.3938; Validation Loss = 0.4208\n",
      "Iteration 84: Train Loss = 0.3936; Validation Loss = 0.4208\n",
      "Iteration 85: Train Loss = 0.3934; Validation Loss = 0.4208\n",
      "Iteration 86: Train Loss = 0.3934; Validation Loss = 0.4207\n",
      "Iteration 87: Train Loss = 0.3931; Validation Loss = 0.4206\n",
      "Iteration 88: Train Loss = 0.3930; Validation Loss = 0.4206\n",
      "Iteration 89: Train Loss = 0.3929; Validation Loss = 0.4205\n",
      "Iteration 90: Train Loss = 0.3929; Validation Loss = 0.4205\n",
      "Iteration 91: Train Loss = 0.3927; Validation Loss = 0.4203\n",
      "Iteration 92: Train Loss = 0.3926; Validation Loss = 0.4204\n",
      "Iteration 93: Train Loss = 0.3924; Validation Loss = 0.4203\n",
      "Iteration 94: Train Loss = 0.3924; Validation Loss = 0.4204\n",
      "Iteration 95: Train Loss = 0.3922; Validation Loss = 0.4202\n",
      "Iteration 96: Train Loss = 0.3920; Validation Loss = 0.4203\n",
      "Iteration 97: Train Loss = 0.3917; Validation Loss = 0.4203\n",
      "Iteration 98: Train Loss = 0.3916; Validation Loss = 0.4202\n",
      "Iteration 99: Train Loss = 0.3915; Validation Loss = 0.4201\n",
      "Iteration 100: Train Loss = 0.3914; Validation Loss = 0.4200\n",
      "Iteration 101: Train Loss = 0.3912; Validation Loss = 0.4199\n",
      "Iteration 102: Train Loss = 0.3910; Validation Loss = 0.4197\n",
      "Iteration 103: Train Loss = 0.3907; Validation Loss = 0.4197\n",
      "Iteration 104: Train Loss = 0.3906; Validation Loss = 0.4197\n",
      "Iteration 105: Train Loss = 0.3906; Validation Loss = 0.4198\n",
      "Iteration 106: Train Loss = 0.3905; Validation Loss = 0.4199\n",
      "Iteration 107: Train Loss = 0.3904; Validation Loss = 0.4199\n",
      "Iteration 108: Train Loss = 0.3904; Validation Loss = 0.4199\n",
      "Iteration 109: Train Loss = 0.3903; Validation Loss = 0.4199\n",
      "Iteration 110: Train Loss = 0.3902; Validation Loss = 0.4199\n",
      "Iteration 111: Train Loss = 0.3900; Validation Loss = 0.4199\n",
      "Iteration 112: Train Loss = 0.3899; Validation Loss = 0.4198\n",
      "Iteration 113: Train Loss = 0.3897; Validation Loss = 0.4196\n",
      "Iteration 114: Train Loss = 0.3896; Validation Loss = 0.4197\n",
      "Iteration 115: Train Loss = 0.3895; Validation Loss = 0.4197\n",
      "Iteration 116: Train Loss = 0.3895; Validation Loss = 0.4198\n",
      "Iteration 117: Train Loss = 0.3893; Validation Loss = 0.4199\n",
      "Iteration 118: Train Loss = 0.3891; Validation Loss = 0.4199\n",
      "Iteration 119: Train Loss = 0.3890; Validation Loss = 0.4198\n",
      "Iteration 120: Train Loss = 0.3889; Validation Loss = 0.4198\n",
      "Iteration 121: Train Loss = 0.3887; Validation Loss = 0.4198\n",
      "Iteration 122: Train Loss = 0.3886; Validation Loss = 0.4197\n",
      "Iteration 123: Train Loss = 0.3885; Validation Loss = 0.4198\n",
      "Iteration 124: Train Loss = 0.3884; Validation Loss = 0.4198\n",
      "Early stopping at iteration 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:32:24,447] Trial 12 finished with value: 0.9611570563047864 and parameters: {'max_depth': 9, 'min_samples_leaf': 6, 'n_estimators': 170, 'learning_rate': 0.08009083726813554, 'subsample': 0.8802083755691944}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6897; Validation Loss = 0.6898\n",
      "Iteration 2: Train Loss = 0.6864; Validation Loss = 0.6866\n",
      "Iteration 3: Train Loss = 0.6831; Validation Loss = 0.6834\n",
      "Iteration 4: Train Loss = 0.6797; Validation Loss = 0.6801\n",
      "Iteration 5: Train Loss = 0.6765; Validation Loss = 0.6770\n",
      "Iteration 6: Train Loss = 0.6733; Validation Loss = 0.6739\n",
      "Iteration 7: Train Loss = 0.6702; Validation Loss = 0.6708\n",
      "Iteration 8: Train Loss = 0.6670; Validation Loss = 0.6678\n",
      "Iteration 9: Train Loss = 0.6639; Validation Loss = 0.6648\n",
      "Iteration 10: Train Loss = 0.6609; Validation Loss = 0.6619\n",
      "Iteration 11: Train Loss = 0.6579; Validation Loss = 0.6590\n",
      "Iteration 12: Train Loss = 0.6550; Validation Loss = 0.6561\n",
      "Iteration 13: Train Loss = 0.6520; Validation Loss = 0.6533\n",
      "Iteration 14: Train Loss = 0.6492; Validation Loss = 0.6505\n",
      "Iteration 15: Train Loss = 0.6464; Validation Loss = 0.6477\n",
      "Iteration 16: Train Loss = 0.6436; Validation Loss = 0.6451\n",
      "Iteration 17: Train Loss = 0.6409; Validation Loss = 0.6424\n",
      "Iteration 18: Train Loss = 0.6381; Validation Loss = 0.6398\n",
      "Iteration 19: Train Loss = 0.6354; Validation Loss = 0.6372\n",
      "Iteration 20: Train Loss = 0.6328; Validation Loss = 0.6346\n",
      "Iteration 21: Train Loss = 0.6302; Validation Loss = 0.6322\n",
      "Iteration 22: Train Loss = 0.6277; Validation Loss = 0.6298\n",
      "Iteration 23: Train Loss = 0.6252; Validation Loss = 0.6274\n",
      "Iteration 24: Train Loss = 0.6227; Validation Loss = 0.6250\n",
      "Iteration 25: Train Loss = 0.6202; Validation Loss = 0.6226\n",
      "Iteration 26: Train Loss = 0.6178; Validation Loss = 0.6202\n",
      "Iteration 27: Train Loss = 0.6154; Validation Loss = 0.6179\n",
      "Iteration 28: Train Loss = 0.6131; Validation Loss = 0.6156\n",
      "Iteration 29: Train Loss = 0.6108; Validation Loss = 0.6134\n",
      "Iteration 30: Train Loss = 0.6085; Validation Loss = 0.6112\n",
      "Iteration 31: Train Loss = 0.6062; Validation Loss = 0.6090\n",
      "Iteration 32: Train Loss = 0.6040; Validation Loss = 0.6068\n",
      "Iteration 33: Train Loss = 0.6018; Validation Loss = 0.6047\n",
      "Iteration 34: Train Loss = 0.5996; Validation Loss = 0.6027\n",
      "Iteration 35: Train Loss = 0.5974; Validation Loss = 0.6006\n",
      "Iteration 36: Train Loss = 0.5953; Validation Loss = 0.5986\n",
      "Iteration 37: Train Loss = 0.5932; Validation Loss = 0.5965\n",
      "Iteration 38: Train Loss = 0.5912; Validation Loss = 0.5945\n",
      "Iteration 39: Train Loss = 0.5892; Validation Loss = 0.5926\n",
      "Iteration 40: Train Loss = 0.5872; Validation Loss = 0.5907\n",
      "Iteration 41: Train Loss = 0.5853; Validation Loss = 0.5888\n",
      "Iteration 42: Train Loss = 0.5833; Validation Loss = 0.5870\n",
      "Iteration 43: Train Loss = 0.5814; Validation Loss = 0.5851\n",
      "Iteration 44: Train Loss = 0.5795; Validation Loss = 0.5833\n",
      "Iteration 45: Train Loss = 0.5776; Validation Loss = 0.5814\n",
      "Iteration 46: Train Loss = 0.5758; Validation Loss = 0.5796\n",
      "Iteration 47: Train Loss = 0.5739; Validation Loss = 0.5780\n",
      "Iteration 48: Train Loss = 0.5721; Validation Loss = 0.5762\n",
      "Iteration 49: Train Loss = 0.5703; Validation Loss = 0.5745\n",
      "Iteration 50: Train Loss = 0.5686; Validation Loss = 0.5728\n",
      "Iteration 51: Train Loss = 0.5669; Validation Loss = 0.5712\n",
      "Iteration 52: Train Loss = 0.5652; Validation Loss = 0.5695\n",
      "Iteration 53: Train Loss = 0.5635; Validation Loss = 0.5679\n",
      "Iteration 54: Train Loss = 0.5618; Validation Loss = 0.5664\n",
      "Iteration 55: Train Loss = 0.5601; Validation Loss = 0.5648\n",
      "Iteration 56: Train Loss = 0.5585; Validation Loss = 0.5632\n",
      "Iteration 57: Train Loss = 0.5569; Validation Loss = 0.5617\n",
      "Iteration 58: Train Loss = 0.5553; Validation Loss = 0.5602\n",
      "Iteration 59: Train Loss = 0.5538; Validation Loss = 0.5587\n",
      "Iteration 60: Train Loss = 0.5523; Validation Loss = 0.5572\n",
      "Iteration 61: Train Loss = 0.5507; Validation Loss = 0.5558\n",
      "Iteration 62: Train Loss = 0.5493; Validation Loss = 0.5544\n",
      "Iteration 63: Train Loss = 0.5478; Validation Loss = 0.5529\n",
      "Iteration 64: Train Loss = 0.5463; Validation Loss = 0.5516\n",
      "Iteration 65: Train Loss = 0.5449; Validation Loss = 0.5502\n",
      "Iteration 66: Train Loss = 0.5435; Validation Loss = 0.5489\n",
      "Iteration 67: Train Loss = 0.5421; Validation Loss = 0.5475\n",
      "Iteration 68: Train Loss = 0.5406; Validation Loss = 0.5462\n",
      "Iteration 69: Train Loss = 0.5393; Validation Loss = 0.5449\n",
      "Iteration 70: Train Loss = 0.5379; Validation Loss = 0.5436\n",
      "Iteration 71: Train Loss = 0.5365; Validation Loss = 0.5423\n",
      "Iteration 72: Train Loss = 0.5352; Validation Loss = 0.5410\n",
      "Iteration 73: Train Loss = 0.5339; Validation Loss = 0.5398\n",
      "Iteration 74: Train Loss = 0.5326; Validation Loss = 0.5386\n",
      "Iteration 75: Train Loss = 0.5313; Validation Loss = 0.5373\n",
      "Iteration 76: Train Loss = 0.5301; Validation Loss = 0.5361\n",
      "Iteration 77: Train Loss = 0.5288; Validation Loss = 0.5349\n",
      "Iteration 78: Train Loss = 0.5276; Validation Loss = 0.5337\n",
      "Iteration 79: Train Loss = 0.5263; Validation Loss = 0.5326\n",
      "Iteration 80: Train Loss = 0.5252; Validation Loss = 0.5315\n",
      "Iteration 81: Train Loss = 0.5240; Validation Loss = 0.5303\n",
      "Iteration 82: Train Loss = 0.5228; Validation Loss = 0.5292\n",
      "Iteration 83: Train Loss = 0.5217; Validation Loss = 0.5282\n",
      "Iteration 84: Train Loss = 0.5205; Validation Loss = 0.5271\n",
      "Iteration 85: Train Loss = 0.5194; Validation Loss = 0.5260\n",
      "Iteration 86: Train Loss = 0.5182; Validation Loss = 0.5250\n",
      "Iteration 87: Train Loss = 0.5171; Validation Loss = 0.5239\n",
      "Iteration 88: Train Loss = 0.5160; Validation Loss = 0.5229\n",
      "Iteration 89: Train Loss = 0.5149; Validation Loss = 0.5218\n",
      "Iteration 90: Train Loss = 0.5139; Validation Loss = 0.5208\n",
      "Iteration 91: Train Loss = 0.5128; Validation Loss = 0.5198\n",
      "Iteration 92: Train Loss = 0.5118; Validation Loss = 0.5188\n",
      "Iteration 93: Train Loss = 0.5107; Validation Loss = 0.5178\n",
      "Iteration 94: Train Loss = 0.5097; Validation Loss = 0.5169\n",
      "Iteration 95: Train Loss = 0.5087; Validation Loss = 0.5159\n",
      "Iteration 96: Train Loss = 0.5077; Validation Loss = 0.5150\n",
      "Iteration 97: Train Loss = 0.5067; Validation Loss = 0.5141\n",
      "Iteration 98: Train Loss = 0.5058; Validation Loss = 0.5132\n",
      "Iteration 99: Train Loss = 0.5048; Validation Loss = 0.5123\n",
      "Iteration 100: Train Loss = 0.5038; Validation Loss = 0.5113\n",
      "Iteration 101: Train Loss = 0.5029; Validation Loss = 0.5105\n",
      "Iteration 102: Train Loss = 0.5020; Validation Loss = 0.5096\n",
      "Iteration 103: Train Loss = 0.5010; Validation Loss = 0.5087\n",
      "Iteration 104: Train Loss = 0.5001; Validation Loss = 0.5078\n",
      "Iteration 105: Train Loss = 0.4992; Validation Loss = 0.5070\n",
      "Iteration 106: Train Loss = 0.4983; Validation Loss = 0.5061\n",
      "Iteration 107: Train Loss = 0.4974; Validation Loss = 0.5053\n",
      "Iteration 108: Train Loss = 0.4965; Validation Loss = 0.5045\n",
      "Iteration 109: Train Loss = 0.4957; Validation Loss = 0.5037\n",
      "Iteration 110: Train Loss = 0.4949; Validation Loss = 0.5029\n",
      "Iteration 111: Train Loss = 0.4940; Validation Loss = 0.5021\n",
      "Iteration 112: Train Loss = 0.4932; Validation Loss = 0.5013\n",
      "Iteration 113: Train Loss = 0.4924; Validation Loss = 0.5005\n",
      "Iteration 114: Train Loss = 0.4916; Validation Loss = 0.4997\n",
      "Iteration 115: Train Loss = 0.4908; Validation Loss = 0.4990\n",
      "Iteration 116: Train Loss = 0.4899; Validation Loss = 0.4983\n",
      "Iteration 117: Train Loss = 0.4891; Validation Loss = 0.4975\n",
      "Iteration 118: Train Loss = 0.4884; Validation Loss = 0.4968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:32:38,007] Trial 13 finished with value: 0.9608108468785865 and parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'n_estimators': 120, 'learning_rate': 0.010062547257827332, 'subsample': 0.6353946478126551}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119: Train Loss = 0.4876; Validation Loss = 0.4961\n",
      "Iteration 120: Train Loss = 0.4869; Validation Loss = 0.4954\n",
      "Iteration 1: Train Loss = 0.6808; Validation Loss = 0.6811\n",
      "Iteration 2: Train Loss = 0.6691; Validation Loss = 0.6698\n",
      "Iteration 3: Train Loss = 0.6580; Validation Loss = 0.6589\n",
      "Iteration 4: Train Loss = 0.6475; Validation Loss = 0.6487\n",
      "Iteration 5: Train Loss = 0.6374; Validation Loss = 0.6388\n",
      "Iteration 6: Train Loss = 0.6280; Validation Loss = 0.6296\n",
      "Iteration 7: Train Loss = 0.6190; Validation Loss = 0.6209\n",
      "Iteration 8: Train Loss = 0.6104; Validation Loss = 0.6125\n",
      "Iteration 9: Train Loss = 0.6023; Validation Loss = 0.6047\n",
      "Iteration 10: Train Loss = 0.5946; Validation Loss = 0.5973\n",
      "Iteration 11: Train Loss = 0.5872; Validation Loss = 0.5902\n",
      "Iteration 12: Train Loss = 0.5802; Validation Loss = 0.5836\n",
      "Iteration 13: Train Loss = 0.5737; Validation Loss = 0.5773\n",
      "Iteration 14: Train Loss = 0.5673; Validation Loss = 0.5712\n",
      "Iteration 15: Train Loss = 0.5614; Validation Loss = 0.5653\n",
      "Iteration 16: Train Loss = 0.5557; Validation Loss = 0.5599\n",
      "Iteration 17: Train Loss = 0.5502; Validation Loss = 0.5546\n",
      "Iteration 18: Train Loss = 0.5451; Validation Loss = 0.5497\n",
      "Iteration 19: Train Loss = 0.5401; Validation Loss = 0.5448\n",
      "Iteration 20: Train Loss = 0.5354; Validation Loss = 0.5402\n",
      "Iteration 21: Train Loss = 0.5308; Validation Loss = 0.5358\n",
      "Iteration 22: Train Loss = 0.5265; Validation Loss = 0.5316\n",
      "Iteration 23: Train Loss = 0.5223; Validation Loss = 0.5275\n",
      "Iteration 24: Train Loss = 0.5184; Validation Loss = 0.5236\n",
      "Iteration 25: Train Loss = 0.5146; Validation Loss = 0.5200\n",
      "Iteration 26: Train Loss = 0.5109; Validation Loss = 0.5165\n",
      "Iteration 27: Train Loss = 0.5073; Validation Loss = 0.5132\n",
      "Iteration 28: Train Loss = 0.5039; Validation Loss = 0.5101\n",
      "Iteration 29: Train Loss = 0.5007; Validation Loss = 0.5070\n",
      "Iteration 30: Train Loss = 0.4977; Validation Loss = 0.5042\n",
      "Iteration 31: Train Loss = 0.4947; Validation Loss = 0.5015\n",
      "Iteration 32: Train Loss = 0.4918; Validation Loss = 0.4988\n",
      "Iteration 33: Train Loss = 0.4891; Validation Loss = 0.4962\n",
      "Iteration 34: Train Loss = 0.4865; Validation Loss = 0.4937\n",
      "Iteration 35: Train Loss = 0.4839; Validation Loss = 0.4913\n",
      "Iteration 36: Train Loss = 0.4815; Validation Loss = 0.4890\n",
      "Iteration 37: Train Loss = 0.4792; Validation Loss = 0.4868\n",
      "Iteration 38: Train Loss = 0.4769; Validation Loss = 0.4846\n",
      "Iteration 39: Train Loss = 0.4748; Validation Loss = 0.4826\n",
      "Iteration 40: Train Loss = 0.4727; Validation Loss = 0.4806\n",
      "Iteration 41: Train Loss = 0.4707; Validation Loss = 0.4789\n",
      "Iteration 42: Train Loss = 0.4689; Validation Loss = 0.4771\n",
      "Iteration 43: Train Loss = 0.4670; Validation Loss = 0.4753\n",
      "Iteration 44: Train Loss = 0.4651; Validation Loss = 0.4735\n",
      "Iteration 45: Train Loss = 0.4634; Validation Loss = 0.4718\n",
      "Iteration 46: Train Loss = 0.4618; Validation Loss = 0.4702\n",
      "Iteration 47: Train Loss = 0.4602; Validation Loss = 0.4687\n",
      "Iteration 48: Train Loss = 0.4588; Validation Loss = 0.4674\n",
      "Iteration 49: Train Loss = 0.4574; Validation Loss = 0.4662\n",
      "Iteration 50: Train Loss = 0.4560; Validation Loss = 0.4649\n",
      "Iteration 51: Train Loss = 0.4546; Validation Loss = 0.4636\n",
      "Iteration 52: Train Loss = 0.4532; Validation Loss = 0.4624\n",
      "Iteration 53: Train Loss = 0.4519; Validation Loss = 0.4611\n",
      "Iteration 54: Train Loss = 0.4506; Validation Loss = 0.4600\n",
      "Iteration 55: Train Loss = 0.4494; Validation Loss = 0.4589\n",
      "Iteration 56: Train Loss = 0.4482; Validation Loss = 0.4577\n",
      "Iteration 57: Train Loss = 0.4471; Validation Loss = 0.4567\n",
      "Iteration 58: Train Loss = 0.4460; Validation Loss = 0.4557\n",
      "Iteration 59: Train Loss = 0.4448; Validation Loss = 0.4546\n",
      "Iteration 60: Train Loss = 0.4438; Validation Loss = 0.4537\n",
      "Iteration 61: Train Loss = 0.4428; Validation Loss = 0.4528\n",
      "Iteration 62: Train Loss = 0.4419; Validation Loss = 0.4520\n",
      "Iteration 63: Train Loss = 0.4408; Validation Loss = 0.4510\n",
      "Iteration 64: Train Loss = 0.4398; Validation Loss = 0.4501\n",
      "Iteration 65: Train Loss = 0.4389; Validation Loss = 0.4492\n",
      "Iteration 66: Train Loss = 0.4382; Validation Loss = 0.4485\n",
      "Iteration 67: Train Loss = 0.4373; Validation Loss = 0.4478\n",
      "Iteration 68: Train Loss = 0.4365; Validation Loss = 0.4471\n",
      "Iteration 69: Train Loss = 0.4358; Validation Loss = 0.4464\n",
      "Iteration 70: Train Loss = 0.4351; Validation Loss = 0.4458\n",
      "Iteration 71: Train Loss = 0.4344; Validation Loss = 0.4452\n",
      "Iteration 72: Train Loss = 0.4338; Validation Loss = 0.4446\n",
      "Iteration 73: Train Loss = 0.4331; Validation Loss = 0.4441\n",
      "Iteration 74: Train Loss = 0.4325; Validation Loss = 0.4436\n",
      "Iteration 75: Train Loss = 0.4320; Validation Loss = 0.4431\n",
      "Iteration 76: Train Loss = 0.4313; Validation Loss = 0.4425\n",
      "Iteration 77: Train Loss = 0.4308; Validation Loss = 0.4419\n",
      "Iteration 78: Train Loss = 0.4302; Validation Loss = 0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:32:47,528] Trial 14 finished with value: 0.963124501942662 and parameters: {'max_depth': 6, 'min_samples_leaf': 1, 'n_estimators': 80, 'learning_rate': 0.037388445021054234, 'subsample': 0.796360272082524}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79: Train Loss = 0.4297; Validation Loss = 0.4409\n",
      "Iteration 80: Train Loss = 0.4292; Validation Loss = 0.4404\n",
      "Iteration 1: Train Loss = 0.6333; Validation Loss = 0.6363\n",
      "Iteration 2: Train Loss = 0.5875; Validation Loss = 0.5932\n",
      "Iteration 3: Train Loss = 0.5518; Validation Loss = 0.5599\n",
      "Iteration 4: Train Loss = 0.5237; Validation Loss = 0.5343\n",
      "Iteration 5: Train Loss = 0.5017; Validation Loss = 0.5135\n",
      "Iteration 6: Train Loss = 0.4834; Validation Loss = 0.4969\n",
      "Iteration 7: Train Loss = 0.4685; Validation Loss = 0.4832\n",
      "Iteration 8: Train Loss = 0.4570; Validation Loss = 0.4721\n",
      "Iteration 9: Train Loss = 0.4471; Validation Loss = 0.4634\n",
      "Iteration 10: Train Loss = 0.4390; Validation Loss = 0.4562\n",
      "Iteration 11: Train Loss = 0.4319; Validation Loss = 0.4500\n",
      "Iteration 12: Train Loss = 0.4267; Validation Loss = 0.4454\n",
      "Iteration 13: Train Loss = 0.4221; Validation Loss = 0.4416\n",
      "Iteration 14: Train Loss = 0.4187; Validation Loss = 0.4387\n",
      "Iteration 15: Train Loss = 0.4159; Validation Loss = 0.4364\n",
      "Iteration 16: Train Loss = 0.4127; Validation Loss = 0.4341\n",
      "Iteration 17: Train Loss = 0.4105; Validation Loss = 0.4324\n",
      "Iteration 18: Train Loss = 0.4084; Validation Loss = 0.4305\n",
      "Iteration 19: Train Loss = 0.4063; Validation Loss = 0.4285\n",
      "Iteration 20: Train Loss = 0.4044; Validation Loss = 0.4272\n",
      "Iteration 21: Train Loss = 0.4031; Validation Loss = 0.4264\n",
      "Iteration 22: Train Loss = 0.4022; Validation Loss = 0.4258\n",
      "Iteration 23: Train Loss = 0.4009; Validation Loss = 0.4249\n",
      "Iteration 24: Train Loss = 0.3999; Validation Loss = 0.4242\n",
      "Iteration 25: Train Loss = 0.3991; Validation Loss = 0.4237\n",
      "Iteration 26: Train Loss = 0.3983; Validation Loss = 0.4232\n",
      "Iteration 27: Train Loss = 0.3981; Validation Loss = 0.4231\n",
      "Iteration 28: Train Loss = 0.3976; Validation Loss = 0.4228\n",
      "Iteration 29: Train Loss = 0.3972; Validation Loss = 0.4224\n",
      "Iteration 30: Train Loss = 0.3970; Validation Loss = 0.4225\n",
      "Iteration 31: Train Loss = 0.3963; Validation Loss = 0.4222\n",
      "Iteration 32: Train Loss = 0.3956; Validation Loss = 0.4216\n",
      "Iteration 33: Train Loss = 0.3953; Validation Loss = 0.4216\n",
      "Iteration 34: Train Loss = 0.3948; Validation Loss = 0.4213\n",
      "Iteration 35: Train Loss = 0.3944; Validation Loss = 0.4208\n",
      "Iteration 36: Train Loss = 0.3939; Validation Loss = 0.4206\n",
      "Iteration 37: Train Loss = 0.3933; Validation Loss = 0.4203\n",
      "Iteration 38: Train Loss = 0.3926; Validation Loss = 0.4202\n",
      "Iteration 39: Train Loss = 0.3925; Validation Loss = 0.4202\n",
      "Iteration 40: Train Loss = 0.3918; Validation Loss = 0.4199\n",
      "Iteration 41: Train Loss = 0.3911; Validation Loss = 0.4196\n",
      "Iteration 42: Train Loss = 0.3908; Validation Loss = 0.4195\n",
      "Iteration 43: Train Loss = 0.3904; Validation Loss = 0.4197\n",
      "Iteration 44: Train Loss = 0.3899; Validation Loss = 0.4194\n",
      "Iteration 45: Train Loss = 0.3898; Validation Loss = 0.4197\n",
      "Iteration 46: Train Loss = 0.3895; Validation Loss = 0.4199\n",
      "Iteration 47: Train Loss = 0.3892; Validation Loss = 0.4198\n",
      "Iteration 48: Train Loss = 0.3888; Validation Loss = 0.4196\n",
      "Iteration 49: Train Loss = 0.3887; Validation Loss = 0.4194\n",
      "Iteration 50: Train Loss = 0.3886; Validation Loss = 0.4197\n",
      "Iteration 51: Train Loss = 0.3883; Validation Loss = 0.4195\n",
      "Iteration 52: Train Loss = 0.3880; Validation Loss = 0.4193\n",
      "Iteration 53: Train Loss = 0.3878; Validation Loss = 0.4192\n",
      "Iteration 54: Train Loss = 0.3877; Validation Loss = 0.4191\n",
      "Iteration 55: Train Loss = 0.3876; Validation Loss = 0.4191\n",
      "Iteration 56: Train Loss = 0.3873; Validation Loss = 0.4191\n",
      "Iteration 57: Train Loss = 0.3871; Validation Loss = 0.4190\n",
      "Iteration 58: Train Loss = 0.3871; Validation Loss = 0.4193\n",
      "Iteration 59: Train Loss = 0.3869; Validation Loss = 0.4193\n",
      "Iteration 60: Train Loss = 0.3868; Validation Loss = 0.4193\n",
      "Iteration 61: Train Loss = 0.3868; Validation Loss = 0.4197\n",
      "Iteration 62: Train Loss = 0.3863; Validation Loss = 0.4197\n",
      "Iteration 63: Train Loss = 0.3860; Validation Loss = 0.4195\n",
      "Iteration 64: Train Loss = 0.3860; Validation Loss = 0.4196\n",
      "Iteration 65: Train Loss = 0.3859; Validation Loss = 0.4196\n",
      "Iteration 66: Train Loss = 0.3858; Validation Loss = 0.4195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:32:55,729] Trial 15 finished with value: 0.9578112548566547 and parameters: {'max_depth': 10, 'min_samples_leaf': 6, 'n_estimators': 200, 'learning_rate': 0.17391156817271466, 'subsample': 0.6832685361365332}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67: Train Loss = 0.3857; Validation Loss = 0.4195\n",
      "Iteration 68: Train Loss = 0.3855; Validation Loss = 0.4194\n",
      "Early stopping at iteration 67\n",
      "Iteration 1: Train Loss = 0.6802; Validation Loss = 0.6806\n",
      "Iteration 2: Train Loss = 0.6682; Validation Loss = 0.6691\n",
      "Iteration 3: Train Loss = 0.6567; Validation Loss = 0.6579\n",
      "Iteration 4: Train Loss = 0.6458; Validation Loss = 0.6474\n",
      "Iteration 5: Train Loss = 0.6354; Validation Loss = 0.6373\n",
      "Iteration 6: Train Loss = 0.6254; Validation Loss = 0.6277\n",
      "Iteration 7: Train Loss = 0.6162; Validation Loss = 0.6189\n",
      "Iteration 8: Train Loss = 0.6072; Validation Loss = 0.6102\n",
      "Iteration 9: Train Loss = 0.5988; Validation Loss = 0.6023\n",
      "Iteration 10: Train Loss = 0.5909; Validation Loss = 0.5945\n",
      "Iteration 11: Train Loss = 0.5832; Validation Loss = 0.5871\n",
      "Iteration 12: Train Loss = 0.5758; Validation Loss = 0.5800\n",
      "Iteration 13: Train Loss = 0.5688; Validation Loss = 0.5734\n",
      "Iteration 14: Train Loss = 0.5622; Validation Loss = 0.5673\n",
      "Iteration 15: Train Loss = 0.5560; Validation Loss = 0.5614\n",
      "Iteration 16: Train Loss = 0.5500; Validation Loss = 0.5557\n",
      "Iteration 17: Train Loss = 0.5443; Validation Loss = 0.5503\n",
      "Iteration 18: Train Loss = 0.5388; Validation Loss = 0.5452\n",
      "Iteration 19: Train Loss = 0.5335; Validation Loss = 0.5402\n",
      "Iteration 20: Train Loss = 0.5286; Validation Loss = 0.5357\n",
      "Iteration 21: Train Loss = 0.5239; Validation Loss = 0.5312\n",
      "Iteration 22: Train Loss = 0.5194; Validation Loss = 0.5270\n",
      "Iteration 23: Train Loss = 0.5149; Validation Loss = 0.5228\n",
      "Iteration 24: Train Loss = 0.5106; Validation Loss = 0.5188\n",
      "Iteration 25: Train Loss = 0.5066; Validation Loss = 0.5150\n",
      "Iteration 26: Train Loss = 0.5028; Validation Loss = 0.5114\n",
      "Iteration 27: Train Loss = 0.4991; Validation Loss = 0.5080\n",
      "Iteration 28: Train Loss = 0.4956; Validation Loss = 0.5047\n",
      "Iteration 29: Train Loss = 0.4923; Validation Loss = 0.5017\n",
      "Iteration 30: Train Loss = 0.4891; Validation Loss = 0.4986\n",
      "Iteration 31: Train Loss = 0.4860; Validation Loss = 0.4958\n",
      "Iteration 32: Train Loss = 0.4831; Validation Loss = 0.4930\n",
      "Iteration 33: Train Loss = 0.4802; Validation Loss = 0.4904\n",
      "Iteration 34: Train Loss = 0.4774; Validation Loss = 0.4878\n",
      "Iteration 35: Train Loss = 0.4748; Validation Loss = 0.4854\n",
      "Iteration 36: Train Loss = 0.4724; Validation Loss = 0.4832\n",
      "Iteration 37: Train Loss = 0.4699; Validation Loss = 0.4810\n",
      "Iteration 38: Train Loss = 0.4675; Validation Loss = 0.4789\n",
      "Iteration 39: Train Loss = 0.4653; Validation Loss = 0.4768\n",
      "Iteration 40: Train Loss = 0.4632; Validation Loss = 0.4749\n",
      "Iteration 41: Train Loss = 0.4611; Validation Loss = 0.4730\n",
      "Iteration 42: Train Loss = 0.4591; Validation Loss = 0.4713\n",
      "Iteration 43: Train Loss = 0.4572; Validation Loss = 0.4695\n",
      "Iteration 44: Train Loss = 0.4553; Validation Loss = 0.4678\n",
      "Iteration 45: Train Loss = 0.4535; Validation Loss = 0.4661\n",
      "Iteration 46: Train Loss = 0.4518; Validation Loss = 0.4647\n",
      "Iteration 47: Train Loss = 0.4501; Validation Loss = 0.4632\n",
      "Iteration 48: Train Loss = 0.4485; Validation Loss = 0.4618\n",
      "Iteration 49: Train Loss = 0.4471; Validation Loss = 0.4605\n",
      "Iteration 50: Train Loss = 0.4456; Validation Loss = 0.4591\n",
      "Iteration 51: Train Loss = 0.4441; Validation Loss = 0.4580\n",
      "Iteration 52: Train Loss = 0.4428; Validation Loss = 0.4568\n",
      "Iteration 53: Train Loss = 0.4415; Validation Loss = 0.4556\n",
      "Iteration 54: Train Loss = 0.4403; Validation Loss = 0.4544\n",
      "Iteration 55: Train Loss = 0.4390; Validation Loss = 0.4533\n",
      "Iteration 56: Train Loss = 0.4378; Validation Loss = 0.4522\n",
      "Iteration 57: Train Loss = 0.4366; Validation Loss = 0.4512\n",
      "Iteration 58: Train Loss = 0.4355; Validation Loss = 0.4502\n",
      "Iteration 59: Train Loss = 0.4344; Validation Loss = 0.4492\n",
      "Iteration 60: Train Loss = 0.4334; Validation Loss = 0.4483\n",
      "Iteration 61: Train Loss = 0.4325; Validation Loss = 0.4474\n",
      "Iteration 62: Train Loss = 0.4315; Validation Loss = 0.4466\n",
      "Iteration 63: Train Loss = 0.4307; Validation Loss = 0.4458\n",
      "Iteration 64: Train Loss = 0.4297; Validation Loss = 0.4450\n",
      "Iteration 65: Train Loss = 0.4288; Validation Loss = 0.4442\n",
      "Iteration 66: Train Loss = 0.4279; Validation Loss = 0.4434\n",
      "Iteration 67: Train Loss = 0.4272; Validation Loss = 0.4427\n",
      "Iteration 68: Train Loss = 0.4264; Validation Loss = 0.4420\n",
      "Iteration 69: Train Loss = 0.4256; Validation Loss = 0.4413\n",
      "Iteration 70: Train Loss = 0.4248; Validation Loss = 0.4406\n",
      "Iteration 71: Train Loss = 0.4241; Validation Loss = 0.4401\n",
      "Iteration 72: Train Loss = 0.4234; Validation Loss = 0.4396\n",
      "Iteration 73: Train Loss = 0.4228; Validation Loss = 0.4390\n",
      "Iteration 74: Train Loss = 0.4221; Validation Loss = 0.4385\n",
      "Iteration 75: Train Loss = 0.4216; Validation Loss = 0.4380\n",
      "Iteration 76: Train Loss = 0.4210; Validation Loss = 0.4375\n",
      "Iteration 77: Train Loss = 0.4204; Validation Loss = 0.4370\n",
      "Iteration 78: Train Loss = 0.4199; Validation Loss = 0.4365\n",
      "Iteration 79: Train Loss = 0.4194; Validation Loss = 0.4361\n",
      "Iteration 80: Train Loss = 0.4188; Validation Loss = 0.4356\n",
      "Iteration 81: Train Loss = 0.4183; Validation Loss = 0.4353\n",
      "Iteration 82: Train Loss = 0.4178; Validation Loss = 0.4348\n",
      "Iteration 83: Train Loss = 0.4173; Validation Loss = 0.4343\n",
      "Iteration 84: Train Loss = 0.4167; Validation Loss = 0.4339\n",
      "Iteration 85: Train Loss = 0.4162; Validation Loss = 0.4335\n",
      "Iteration 86: Train Loss = 0.4158; Validation Loss = 0.4332\n",
      "Iteration 87: Train Loss = 0.4154; Validation Loss = 0.4329\n",
      "Iteration 88: Train Loss = 0.4150; Validation Loss = 0.4325\n",
      "Iteration 89: Train Loss = 0.4146; Validation Loss = 0.4322\n",
      "Iteration 90: Train Loss = 0.4142; Validation Loss = 0.4318\n",
      "Iteration 91: Train Loss = 0.4137; Validation Loss = 0.4315\n",
      "Iteration 92: Train Loss = 0.4134; Validation Loss = 0.4312\n",
      "Iteration 93: Train Loss = 0.4132; Validation Loss = 0.4311\n",
      "Iteration 94: Train Loss = 0.4128; Validation Loss = 0.4308\n",
      "Iteration 95: Train Loss = 0.4124; Validation Loss = 0.4305\n",
      "Iteration 96: Train Loss = 0.4121; Validation Loss = 0.4303\n",
      "Iteration 97: Train Loss = 0.4119; Validation Loss = 0.4301\n",
      "Iteration 98: Train Loss = 0.4116; Validation Loss = 0.4299\n",
      "Iteration 99: Train Loss = 0.4113; Validation Loss = 0.4296\n",
      "Iteration 100: Train Loss = 0.4110; Validation Loss = 0.4293\n",
      "Iteration 101: Train Loss = 0.4107; Validation Loss = 0.4291\n",
      "Iteration 102: Train Loss = 0.4104; Validation Loss = 0.4289\n",
      "Iteration 103: Train Loss = 0.4100; Validation Loss = 0.4287\n",
      "Iteration 104: Train Loss = 0.4097; Validation Loss = 0.4285\n",
      "Iteration 105: Train Loss = 0.4094; Validation Loss = 0.4283\n",
      "Iteration 106: Train Loss = 0.4091; Validation Loss = 0.4280\n",
      "Iteration 107: Train Loss = 0.4089; Validation Loss = 0.4279\n",
      "Iteration 108: Train Loss = 0.4086; Validation Loss = 0.4277\n",
      "Iteration 109: Train Loss = 0.4084; Validation Loss = 0.4276\n",
      "Iteration 110: Train Loss = 0.4082; Validation Loss = 0.4274\n",
      "Iteration 111: Train Loss = 0.4080; Validation Loss = 0.4272\n",
      "Iteration 112: Train Loss = 0.4077; Validation Loss = 0.4270\n",
      "Iteration 113: Train Loss = 0.4074; Validation Loss = 0.4268\n",
      "Iteration 114: Train Loss = 0.4072; Validation Loss = 0.4267\n",
      "Iteration 115: Train Loss = 0.4070; Validation Loss = 0.4265\n",
      "Iteration 116: Train Loss = 0.4068; Validation Loss = 0.4263\n",
      "Iteration 117: Train Loss = 0.4066; Validation Loss = 0.4262\n",
      "Iteration 118: Train Loss = 0.4064; Validation Loss = 0.4260\n",
      "Iteration 119: Train Loss = 0.4061; Validation Loss = 0.4259\n",
      "Iteration 120: Train Loss = 0.4060; Validation Loss = 0.4258\n",
      "Iteration 121: Train Loss = 0.4059; Validation Loss = 0.4257\n",
      "Iteration 122: Train Loss = 0.4056; Validation Loss = 0.4255\n",
      "Iteration 123: Train Loss = 0.4055; Validation Loss = 0.4254\n",
      "Iteration 124: Train Loss = 0.4053; Validation Loss = 0.4253\n",
      "Iteration 125: Train Loss = 0.4052; Validation Loss = 0.4252\n",
      "Iteration 126: Train Loss = 0.4050; Validation Loss = 0.4250\n",
      "Iteration 127: Train Loss = 0.4050; Validation Loss = 0.4250\n",
      "Iteration 128: Train Loss = 0.4049; Validation Loss = 0.4249\n",
      "Iteration 129: Train Loss = 0.4048; Validation Loss = 0.4249\n",
      "Iteration 130: Train Loss = 0.4046; Validation Loss = 0.4249\n",
      "Iteration 131: Train Loss = 0.4045; Validation Loss = 0.4248\n",
      "Iteration 132: Train Loss = 0.4044; Validation Loss = 0.4247\n",
      "Iteration 133: Train Loss = 0.4042; Validation Loss = 0.4245\n",
      "Iteration 134: Train Loss = 0.4042; Validation Loss = 0.4245\n",
      "Iteration 135: Train Loss = 0.4040; Validation Loss = 0.4244\n",
      "Iteration 136: Train Loss = 0.4040; Validation Loss = 0.4243\n",
      "Iteration 137: Train Loss = 0.4039; Validation Loss = 0.4243\n",
      "Iteration 138: Train Loss = 0.4038; Validation Loss = 0.4243\n",
      "Iteration 139: Train Loss = 0.4036; Validation Loss = 0.4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:33:09,178] Trial 16 finished with value: 0.9649253247619809 and parameters: {'max_depth': 8, 'min_samples_leaf': 4, 'n_estimators': 140, 'learning_rate': 0.03697780210044303, 'subsample': 0.5671617104841297}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140: Train Loss = 0.4035; Validation Loss = 0.4241\n",
      "Iteration 1: Train Loss = 0.6820; Validation Loss = 0.6828\n",
      "Iteration 2: Train Loss = 0.6713; Validation Loss = 0.6729\n",
      "Iteration 3: Train Loss = 0.6610; Validation Loss = 0.6634\n",
      "Iteration 4: Train Loss = 0.6510; Validation Loss = 0.6542\n",
      "Iteration 5: Train Loss = 0.6416; Validation Loss = 0.6456\n",
      "Iteration 6: Train Loss = 0.6325; Validation Loss = 0.6372\n",
      "Iteration 7: Train Loss = 0.6238; Validation Loss = 0.6293\n",
      "Iteration 8: Train Loss = 0.6155; Validation Loss = 0.6218\n",
      "Iteration 9: Train Loss = 0.6074; Validation Loss = 0.6146\n",
      "Iteration 10: Train Loss = 0.5997; Validation Loss = 0.6075\n",
      "Iteration 11: Train Loss = 0.5921; Validation Loss = 0.6009\n",
      "Iteration 12: Train Loss = 0.5850; Validation Loss = 0.5943\n",
      "Iteration 13: Train Loss = 0.5781; Validation Loss = 0.5880\n",
      "Iteration 14: Train Loss = 0.5715; Validation Loss = 0.5819\n",
      "Iteration 15: Train Loss = 0.5651; Validation Loss = 0.5761\n",
      "Iteration 16: Train Loss = 0.5590; Validation Loss = 0.5707\n",
      "Iteration 17: Train Loss = 0.5530; Validation Loss = 0.5655\n",
      "Iteration 18: Train Loss = 0.5473; Validation Loss = 0.5604\n",
      "Iteration 19: Train Loss = 0.5418; Validation Loss = 0.5557\n",
      "Iteration 20: Train Loss = 0.5365; Validation Loss = 0.5512\n",
      "Iteration 21: Train Loss = 0.5315; Validation Loss = 0.5467\n",
      "Iteration 22: Train Loss = 0.5266; Validation Loss = 0.5423\n",
      "Iteration 23: Train Loss = 0.5219; Validation Loss = 0.5382\n",
      "Iteration 24: Train Loss = 0.5174; Validation Loss = 0.5345\n",
      "Iteration 25: Train Loss = 0.5130; Validation Loss = 0.5307\n",
      "Iteration 26: Train Loss = 0.5087; Validation Loss = 0.5269\n",
      "Iteration 27: Train Loss = 0.5046; Validation Loss = 0.5233\n",
      "Iteration 28: Train Loss = 0.5006; Validation Loss = 0.5198\n",
      "Iteration 29: Train Loss = 0.4969; Validation Loss = 0.5166\n",
      "Iteration 30: Train Loss = 0.4931; Validation Loss = 0.5133\n",
      "Iteration 31: Train Loss = 0.4896; Validation Loss = 0.5103\n",
      "Iteration 32: Train Loss = 0.4863; Validation Loss = 0.5074\n",
      "Iteration 33: Train Loss = 0.4830; Validation Loss = 0.5045\n",
      "Iteration 34: Train Loss = 0.4798; Validation Loss = 0.5017\n",
      "Iteration 35: Train Loss = 0.4767; Validation Loss = 0.4991\n",
      "Iteration 36: Train Loss = 0.4738; Validation Loss = 0.4964\n",
      "Iteration 37: Train Loss = 0.4708; Validation Loss = 0.4941\n",
      "Iteration 38: Train Loss = 0.4681; Validation Loss = 0.4918\n",
      "Iteration 39: Train Loss = 0.4654; Validation Loss = 0.4895\n",
      "Iteration 40: Train Loss = 0.4627; Validation Loss = 0.4874\n",
      "Iteration 41: Train Loss = 0.4602; Validation Loss = 0.4854\n",
      "Iteration 42: Train Loss = 0.4577; Validation Loss = 0.4833\n",
      "Iteration 43: Train Loss = 0.4553; Validation Loss = 0.4813\n",
      "Iteration 44: Train Loss = 0.4530; Validation Loss = 0.4793\n",
      "Iteration 45: Train Loss = 0.4508; Validation Loss = 0.4776\n",
      "Iteration 46: Train Loss = 0.4486; Validation Loss = 0.4758\n",
      "Iteration 47: Train Loss = 0.4465; Validation Loss = 0.4739\n",
      "Iteration 48: Train Loss = 0.4445; Validation Loss = 0.4722\n",
      "Iteration 49: Train Loss = 0.4425; Validation Loss = 0.4705\n",
      "Iteration 50: Train Loss = 0.4407; Validation Loss = 0.4691\n",
      "Iteration 51: Train Loss = 0.4388; Validation Loss = 0.4677\n",
      "Iteration 52: Train Loss = 0.4370; Validation Loss = 0.4662\n",
      "Iteration 53: Train Loss = 0.4352; Validation Loss = 0.4647\n",
      "Iteration 54: Train Loss = 0.4336; Validation Loss = 0.4635\n",
      "Iteration 55: Train Loss = 0.4319; Validation Loss = 0.4622\n",
      "Iteration 56: Train Loss = 0.4303; Validation Loss = 0.4607\n",
      "Iteration 57: Train Loss = 0.4286; Validation Loss = 0.4595\n",
      "Iteration 58: Train Loss = 0.4271; Validation Loss = 0.4582\n",
      "Iteration 59: Train Loss = 0.4257; Validation Loss = 0.4572\n",
      "Iteration 60: Train Loss = 0.4242; Validation Loss = 0.4562\n",
      "Iteration 61: Train Loss = 0.4229; Validation Loss = 0.4551\n",
      "Iteration 62: Train Loss = 0.4215; Validation Loss = 0.4540\n",
      "Iteration 63: Train Loss = 0.4203; Validation Loss = 0.4530\n",
      "Iteration 64: Train Loss = 0.4190; Validation Loss = 0.4521\n",
      "Iteration 65: Train Loss = 0.4178; Validation Loss = 0.4511\n",
      "Iteration 66: Train Loss = 0.4166; Validation Loss = 0.4503\n",
      "Iteration 67: Train Loss = 0.4155; Validation Loss = 0.4493\n",
      "Iteration 68: Train Loss = 0.4143; Validation Loss = 0.4485\n",
      "Iteration 69: Train Loss = 0.4132; Validation Loss = 0.4476\n",
      "Iteration 70: Train Loss = 0.4121; Validation Loss = 0.4467\n",
      "Iteration 71: Train Loss = 0.4111; Validation Loss = 0.4458\n",
      "Iteration 72: Train Loss = 0.4101; Validation Loss = 0.4451\n",
      "Iteration 73: Train Loss = 0.4091; Validation Loss = 0.4443\n",
      "Iteration 74: Train Loss = 0.4082; Validation Loss = 0.4435\n",
      "Iteration 75: Train Loss = 0.4073; Validation Loss = 0.4429\n",
      "Iteration 76: Train Loss = 0.4064; Validation Loss = 0.4423\n",
      "Iteration 77: Train Loss = 0.4055; Validation Loss = 0.4417\n",
      "Iteration 78: Train Loss = 0.4047; Validation Loss = 0.4410\n",
      "Iteration 79: Train Loss = 0.4039; Validation Loss = 0.4404\n",
      "Iteration 80: Train Loss = 0.4031; Validation Loss = 0.4399\n",
      "Iteration 81: Train Loss = 0.4024; Validation Loss = 0.4393\n",
      "Iteration 82: Train Loss = 0.4017; Validation Loss = 0.4387\n",
      "Iteration 83: Train Loss = 0.4009; Validation Loss = 0.4381\n",
      "Iteration 84: Train Loss = 0.4001; Validation Loss = 0.4376\n",
      "Iteration 85: Train Loss = 0.3994; Validation Loss = 0.4371\n",
      "Iteration 86: Train Loss = 0.3988; Validation Loss = 0.4365\n",
      "Iteration 87: Train Loss = 0.3981; Validation Loss = 0.4360\n",
      "Iteration 88: Train Loss = 0.3975; Validation Loss = 0.4356\n",
      "Iteration 89: Train Loss = 0.3969; Validation Loss = 0.4351\n",
      "Iteration 90: Train Loss = 0.3962; Validation Loss = 0.4346\n",
      "Iteration 91: Train Loss = 0.3957; Validation Loss = 0.4341\n",
      "Iteration 92: Train Loss = 0.3951; Validation Loss = 0.4337\n",
      "Iteration 93: Train Loss = 0.3946; Validation Loss = 0.4333\n",
      "Iteration 94: Train Loss = 0.3940; Validation Loss = 0.4329\n",
      "Iteration 95: Train Loss = 0.3935; Validation Loss = 0.4326\n",
      "Iteration 96: Train Loss = 0.3929; Validation Loss = 0.4322\n",
      "Iteration 97: Train Loss = 0.3924; Validation Loss = 0.4320\n",
      "Iteration 98: Train Loss = 0.3918; Validation Loss = 0.4315\n",
      "Iteration 99: Train Loss = 0.3914; Validation Loss = 0.4312\n",
      "Iteration 100: Train Loss = 0.3909; Validation Loss = 0.4309\n",
      "Iteration 101: Train Loss = 0.3904; Validation Loss = 0.4305\n",
      "Iteration 102: Train Loss = 0.3900; Validation Loss = 0.4302\n",
      "Iteration 103: Train Loss = 0.3897; Validation Loss = 0.4301\n",
      "Iteration 104: Train Loss = 0.3893; Validation Loss = 0.4298\n",
      "Iteration 105: Train Loss = 0.3888; Validation Loss = 0.4295\n",
      "Iteration 106: Train Loss = 0.3884; Validation Loss = 0.4292\n",
      "Iteration 107: Train Loss = 0.3880; Validation Loss = 0.4288\n",
      "Iteration 108: Train Loss = 0.3875; Validation Loss = 0.4286\n",
      "Iteration 109: Train Loss = 0.3871; Validation Loss = 0.4283\n",
      "Iteration 110: Train Loss = 0.3868; Validation Loss = 0.4281\n",
      "Iteration 111: Train Loss = 0.3865; Validation Loss = 0.4279\n",
      "Iteration 112: Train Loss = 0.3861; Validation Loss = 0.4276\n",
      "Iteration 113: Train Loss = 0.3857; Validation Loss = 0.4274\n",
      "Iteration 114: Train Loss = 0.3854; Validation Loss = 0.4272\n",
      "Iteration 115: Train Loss = 0.3851; Validation Loss = 0.4271\n",
      "Iteration 116: Train Loss = 0.3847; Validation Loss = 0.4269\n",
      "Iteration 117: Train Loss = 0.3843; Validation Loss = 0.4267\n",
      "Iteration 118: Train Loss = 0.3840; Validation Loss = 0.4263\n",
      "Iteration 119: Train Loss = 0.3836; Validation Loss = 0.4261\n",
      "Iteration 120: Train Loss = 0.3833; Validation Loss = 0.4259\n",
      "Iteration 121: Train Loss = 0.3829; Validation Loss = 0.4257\n",
      "Iteration 122: Train Loss = 0.3827; Validation Loss = 0.4256\n",
      "Iteration 123: Train Loss = 0.3824; Validation Loss = 0.4254\n",
      "Iteration 124: Train Loss = 0.3820; Validation Loss = 0.4251\n",
      "Iteration 125: Train Loss = 0.3818; Validation Loss = 0.4250\n",
      "Iteration 126: Train Loss = 0.3815; Validation Loss = 0.4247\n",
      "Iteration 127: Train Loss = 0.3812; Validation Loss = 0.4246\n",
      "Iteration 128: Train Loss = 0.3810; Validation Loss = 0.4244\n",
      "Iteration 129: Train Loss = 0.3807; Validation Loss = 0.4243\n",
      "Iteration 130: Train Loss = 0.3805; Validation Loss = 0.4241\n",
      "Iteration 131: Train Loss = 0.3802; Validation Loss = 0.4239\n",
      "Iteration 132: Train Loss = 0.3800; Validation Loss = 0.4238\n",
      "Iteration 133: Train Loss = 0.3797; Validation Loss = 0.4236\n",
      "Iteration 134: Train Loss = 0.3795; Validation Loss = 0.4235\n",
      "Iteration 135: Train Loss = 0.3793; Validation Loss = 0.4234\n",
      "Iteration 136: Train Loss = 0.3790; Validation Loss = 0.4233\n",
      "Iteration 137: Train Loss = 0.3788; Validation Loss = 0.4231\n",
      "Iteration 138: Train Loss = 0.3786; Validation Loss = 0.4231\n",
      "Iteration 139: Train Loss = 0.3784; Validation Loss = 0.4230\n",
      "Iteration 140: Train Loss = 0.3782; Validation Loss = 0.4229\n",
      "Iteration 141: Train Loss = 0.3780; Validation Loss = 0.4228\n",
      "Iteration 142: Train Loss = 0.3778; Validation Loss = 0.4227\n",
      "Iteration 143: Train Loss = 0.3776; Validation Loss = 0.4226\n",
      "Iteration 144: Train Loss = 0.3774; Validation Loss = 0.4225\n",
      "Iteration 145: Train Loss = 0.3772; Validation Loss = 0.4224\n",
      "Iteration 146: Train Loss = 0.3771; Validation Loss = 0.4223\n",
      "Iteration 147: Train Loss = 0.3768; Validation Loss = 0.4223\n",
      "Iteration 148: Train Loss = 0.3767; Validation Loss = 0.4222\n",
      "Iteration 149: Train Loss = 0.3766; Validation Loss = 0.4222\n",
      "Iteration 150: Train Loss = 0.3764; Validation Loss = 0.4221\n",
      "Iteration 151: Train Loss = 0.3762; Validation Loss = 0.4219\n",
      "Iteration 152: Train Loss = 0.3760; Validation Loss = 0.4219\n",
      "Iteration 153: Train Loss = 0.3758; Validation Loss = 0.4217\n",
      "Iteration 154: Train Loss = 0.3757; Validation Loss = 0.4216\n",
      "Iteration 155: Train Loss = 0.3755; Validation Loss = 0.4216\n",
      "Iteration 156: Train Loss = 0.3754; Validation Loss = 0.4215\n",
      "Iteration 157: Train Loss = 0.3752; Validation Loss = 0.4215\n",
      "Iteration 158: Train Loss = 0.3751; Validation Loss = 0.4214\n",
      "Iteration 159: Train Loss = 0.3749; Validation Loss = 0.4213\n",
      "Iteration 160: Train Loss = 0.3748; Validation Loss = 0.4213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:33:33,118] Trial 17 finished with value: 0.9579652951740388 and parameters: {'max_depth': 15, 'min_samples_leaf': 3, 'n_estimators': 160, 'learning_rate': 0.02949911246261975, 'subsample': 0.557455360988736}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6907; Validation Loss = 0.6908\n",
      "Iteration 2: Train Loss = 0.6883; Validation Loss = 0.6885\n",
      "Iteration 3: Train Loss = 0.6859; Validation Loss = 0.6861\n",
      "Iteration 4: Train Loss = 0.6835; Validation Loss = 0.6838\n",
      "Iteration 5: Train Loss = 0.6811; Validation Loss = 0.6816\n",
      "Iteration 6: Train Loss = 0.6788; Validation Loss = 0.6794\n",
      "Iteration 7: Train Loss = 0.6765; Validation Loss = 0.6771\n",
      "Iteration 8: Train Loss = 0.6742; Validation Loss = 0.6749\n",
      "Iteration 9: Train Loss = 0.6719; Validation Loss = 0.6728\n",
      "Iteration 10: Train Loss = 0.6697; Validation Loss = 0.6706\n",
      "Iteration 11: Train Loss = 0.6675; Validation Loss = 0.6685\n",
      "Iteration 12: Train Loss = 0.6653; Validation Loss = 0.6664\n",
      "Iteration 13: Train Loss = 0.6631; Validation Loss = 0.6643\n",
      "Iteration 14: Train Loss = 0.6610; Validation Loss = 0.6622\n",
      "Iteration 15: Train Loss = 0.6588; Validation Loss = 0.6601\n",
      "Iteration 16: Train Loss = 0.6567; Validation Loss = 0.6581\n",
      "Iteration 17: Train Loss = 0.6546; Validation Loss = 0.6561\n",
      "Iteration 18: Train Loss = 0.6525; Validation Loss = 0.6541\n",
      "Iteration 19: Train Loss = 0.6505; Validation Loss = 0.6521\n",
      "Iteration 20: Train Loss = 0.6484; Validation Loss = 0.6502\n",
      "Iteration 21: Train Loss = 0.6464; Validation Loss = 0.6483\n",
      "Iteration 22: Train Loss = 0.6444; Validation Loss = 0.6464\n",
      "Iteration 23: Train Loss = 0.6424; Validation Loss = 0.6445\n",
      "Iteration 24: Train Loss = 0.6405; Validation Loss = 0.6426\n",
      "Iteration 25: Train Loss = 0.6386; Validation Loss = 0.6408\n",
      "Iteration 26: Train Loss = 0.6366; Validation Loss = 0.6389\n",
      "Iteration 27: Train Loss = 0.6347; Validation Loss = 0.6371\n",
      "Iteration 28: Train Loss = 0.6329; Validation Loss = 0.6353\n",
      "Iteration 29: Train Loss = 0.6310; Validation Loss = 0.6335\n",
      "Iteration 30: Train Loss = 0.6292; Validation Loss = 0.6318\n",
      "Iteration 31: Train Loss = 0.6273; Validation Loss = 0.6300\n",
      "Iteration 32: Train Loss = 0.6255; Validation Loss = 0.6283\n",
      "Iteration 33: Train Loss = 0.6238; Validation Loss = 0.6266\n",
      "Iteration 34: Train Loss = 0.6220; Validation Loss = 0.6249\n",
      "Iteration 35: Train Loss = 0.6202; Validation Loss = 0.6232\n",
      "Iteration 36: Train Loss = 0.6185; Validation Loss = 0.6216\n",
      "Iteration 37: Train Loss = 0.6168; Validation Loss = 0.6199\n",
      "Iteration 38: Train Loss = 0.6150; Validation Loss = 0.6183\n",
      "Iteration 39: Train Loss = 0.6133; Validation Loss = 0.6167\n",
      "Iteration 40: Train Loss = 0.6117; Validation Loss = 0.6150\n",
      "Iteration 41: Train Loss = 0.6100; Validation Loss = 0.6135\n",
      "Iteration 42: Train Loss = 0.6084; Validation Loss = 0.6119\n",
      "Iteration 43: Train Loss = 0.6068; Validation Loss = 0.6104\n",
      "Iteration 44: Train Loss = 0.6052; Validation Loss = 0.6088\n",
      "Iteration 45: Train Loss = 0.6036; Validation Loss = 0.6073\n",
      "Iteration 46: Train Loss = 0.6020; Validation Loss = 0.6058\n",
      "Iteration 47: Train Loss = 0.6004; Validation Loss = 0.6043\n",
      "Iteration 48: Train Loss = 0.5989; Validation Loss = 0.6028\n",
      "Iteration 49: Train Loss = 0.5973; Validation Loss = 0.6013\n",
      "Iteration 50: Train Loss = 0.5958; Validation Loss = 0.5999\n",
      "Iteration 51: Train Loss = 0.5943; Validation Loss = 0.5984\n",
      "Iteration 52: Train Loss = 0.5928; Validation Loss = 0.5970\n",
      "Iteration 53: Train Loss = 0.5913; Validation Loss = 0.5956\n",
      "Iteration 54: Train Loss = 0.5898; Validation Loss = 0.5942\n",
      "Iteration 55: Train Loss = 0.5884; Validation Loss = 0.5928\n",
      "Iteration 56: Train Loss = 0.5870; Validation Loss = 0.5914\n",
      "Iteration 57: Train Loss = 0.5855; Validation Loss = 0.5901\n",
      "Iteration 58: Train Loss = 0.5841; Validation Loss = 0.5887\n",
      "Iteration 59: Train Loss = 0.5827; Validation Loss = 0.5874\n",
      "Iteration 60: Train Loss = 0.5814; Validation Loss = 0.5861\n",
      "Iteration 61: Train Loss = 0.5800; Validation Loss = 0.5847\n",
      "Iteration 62: Train Loss = 0.5786; Validation Loss = 0.5834\n",
      "Iteration 63: Train Loss = 0.5773; Validation Loss = 0.5822\n",
      "Iteration 64: Train Loss = 0.5759; Validation Loss = 0.5809\n",
      "Iteration 65: Train Loss = 0.5746; Validation Loss = 0.5796\n",
      "Iteration 66: Train Loss = 0.5733; Validation Loss = 0.5784\n",
      "Iteration 67: Train Loss = 0.5720; Validation Loss = 0.5771\n",
      "Iteration 68: Train Loss = 0.5707; Validation Loss = 0.5759\n",
      "Iteration 69: Train Loss = 0.5694; Validation Loss = 0.5747\n",
      "Iteration 70: Train Loss = 0.5682; Validation Loss = 0.5736\n",
      "Iteration 71: Train Loss = 0.5669; Validation Loss = 0.5724\n",
      "Iteration 72: Train Loss = 0.5657; Validation Loss = 0.5712\n",
      "Iteration 73: Train Loss = 0.5644; Validation Loss = 0.5700\n",
      "Iteration 74: Train Loss = 0.5632; Validation Loss = 0.5688\n",
      "Iteration 75: Train Loss = 0.5620; Validation Loss = 0.5676\n",
      "Iteration 76: Train Loss = 0.5608; Validation Loss = 0.5665\n",
      "Iteration 77: Train Loss = 0.5596; Validation Loss = 0.5654\n",
      "Iteration 78: Train Loss = 0.5584; Validation Loss = 0.5643\n",
      "Iteration 79: Train Loss = 0.5572; Validation Loss = 0.5632\n",
      "Iteration 80: Train Loss = 0.5561; Validation Loss = 0.5620\n",
      "Iteration 81: Train Loss = 0.5549; Validation Loss = 0.5610\n",
      "Iteration 82: Train Loss = 0.5538; Validation Loss = 0.5599\n",
      "Iteration 83: Train Loss = 0.5526; Validation Loss = 0.5588\n",
      "Iteration 84: Train Loss = 0.5515; Validation Loss = 0.5578\n",
      "Iteration 85: Train Loss = 0.5504; Validation Loss = 0.5567\n",
      "Iteration 86: Train Loss = 0.5493; Validation Loss = 0.5557\n",
      "Iteration 87: Train Loss = 0.5482; Validation Loss = 0.5546\n",
      "Iteration 88: Train Loss = 0.5472; Validation Loss = 0.5536\n",
      "Iteration 89: Train Loss = 0.5461; Validation Loss = 0.5526\n",
      "Iteration 90: Train Loss = 0.5451; Validation Loss = 0.5516\n",
      "Iteration 91: Train Loss = 0.5440; Validation Loss = 0.5506\n",
      "Iteration 92: Train Loss = 0.5430; Validation Loss = 0.5497\n",
      "Iteration 93: Train Loss = 0.5419; Validation Loss = 0.5487\n",
      "Iteration 94: Train Loss = 0.5409; Validation Loss = 0.5477\n",
      "Iteration 95: Train Loss = 0.5399; Validation Loss = 0.5468\n",
      "Iteration 96: Train Loss = 0.5389; Validation Loss = 0.5458\n",
      "Iteration 97: Train Loss = 0.5379; Validation Loss = 0.5449\n",
      "Iteration 98: Train Loss = 0.5369; Validation Loss = 0.5439\n",
      "Iteration 99: Train Loss = 0.5359; Validation Loss = 0.5430\n",
      "Iteration 100: Train Loss = 0.5349; Validation Loss = 0.5421\n",
      "Iteration 101: Train Loss = 0.5339; Validation Loss = 0.5411\n",
      "Iteration 102: Train Loss = 0.5330; Validation Loss = 0.5402\n",
      "Iteration 103: Train Loss = 0.5320; Validation Loss = 0.5393\n",
      "Iteration 104: Train Loss = 0.5311; Validation Loss = 0.5384\n",
      "Iteration 105: Train Loss = 0.5302; Validation Loss = 0.5375\n",
      "Iteration 106: Train Loss = 0.5292; Validation Loss = 0.5366\n",
      "Iteration 107: Train Loss = 0.5283; Validation Loss = 0.5358\n",
      "Iteration 108: Train Loss = 0.5274; Validation Loss = 0.5349\n",
      "Iteration 109: Train Loss = 0.5265; Validation Loss = 0.5341\n",
      "Iteration 110: Train Loss = 0.5256; Validation Loss = 0.5333\n",
      "Iteration 111: Train Loss = 0.5247; Validation Loss = 0.5324\n",
      "Iteration 112: Train Loss = 0.5239; Validation Loss = 0.5316\n",
      "Iteration 113: Train Loss = 0.5230; Validation Loss = 0.5308\n",
      "Iteration 114: Train Loss = 0.5222; Validation Loss = 0.5300\n",
      "Iteration 115: Train Loss = 0.5213; Validation Loss = 0.5292\n",
      "Iteration 116: Train Loss = 0.5205; Validation Loss = 0.5284\n",
      "Iteration 117: Train Loss = 0.5197; Validation Loss = 0.5277\n",
      "Iteration 118: Train Loss = 0.5188; Validation Loss = 0.5269\n",
      "Iteration 119: Train Loss = 0.5180; Validation Loss = 0.5261\n",
      "Iteration 120: Train Loss = 0.5172; Validation Loss = 0.5253\n",
      "Iteration 121: Train Loss = 0.5164; Validation Loss = 0.5245\n",
      "Iteration 122: Train Loss = 0.5155; Validation Loss = 0.5238\n",
      "Iteration 123: Train Loss = 0.5147; Validation Loss = 0.5230\n",
      "Iteration 124: Train Loss = 0.5139; Validation Loss = 0.5222\n",
      "Iteration 125: Train Loss = 0.5131; Validation Loss = 0.5215\n",
      "Iteration 126: Train Loss = 0.5124; Validation Loss = 0.5208\n",
      "Iteration 127: Train Loss = 0.5116; Validation Loss = 0.5200\n",
      "Iteration 128: Train Loss = 0.5108; Validation Loss = 0.5193\n",
      "Iteration 129: Train Loss = 0.5101; Validation Loss = 0.5186\n",
      "Iteration 130: Train Loss = 0.5093; Validation Loss = 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:33:56,577] Trial 18 finished with value: 0.9611185462254402 and parameters: {'max_depth': 8, 'min_samples_leaf': 7, 'n_estimators': 130, 'learning_rate': 0.007006763011283539, 'subsample': 0.9943842051860647}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6926; Validation Loss = 0.6926\n",
      "Iteration 2: Train Loss = 0.6920; Validation Loss = 0.6921\n",
      "Iteration 3: Train Loss = 0.6915; Validation Loss = 0.6915\n",
      "Iteration 4: Train Loss = 0.6909; Validation Loss = 0.6910\n",
      "Iteration 5: Train Loss = 0.6904; Validation Loss = 0.6904\n",
      "Iteration 6: Train Loss = 0.6898; Validation Loss = 0.6899\n",
      "Iteration 7: Train Loss = 0.6893; Validation Loss = 0.6894\n",
      "Iteration 8: Train Loss = 0.6887; Validation Loss = 0.6888\n",
      "Iteration 9: Train Loss = 0.6882; Validation Loss = 0.6883\n",
      "Iteration 10: Train Loss = 0.6876; Validation Loss = 0.6878\n",
      "Iteration 11: Train Loss = 0.6871; Validation Loss = 0.6872\n",
      "Iteration 12: Train Loss = 0.6866; Validation Loss = 0.6867\n",
      "Iteration 13: Train Loss = 0.6860; Validation Loss = 0.6862\n",
      "Iteration 14: Train Loss = 0.6855; Validation Loss = 0.6856\n",
      "Iteration 15: Train Loss = 0.6849; Validation Loss = 0.6851\n",
      "Iteration 16: Train Loss = 0.6844; Validation Loss = 0.6846\n",
      "Iteration 17: Train Loss = 0.6839; Validation Loss = 0.6841\n",
      "Iteration 18: Train Loss = 0.6833; Validation Loss = 0.6836\n",
      "Iteration 19: Train Loss = 0.6828; Validation Loss = 0.6830\n",
      "Iteration 20: Train Loss = 0.6823; Validation Loss = 0.6825\n",
      "Iteration 21: Train Loss = 0.6817; Validation Loss = 0.6820\n",
      "Iteration 22: Train Loss = 0.6812; Validation Loss = 0.6815\n",
      "Iteration 23: Train Loss = 0.6807; Validation Loss = 0.6810\n",
      "Iteration 24: Train Loss = 0.6802; Validation Loss = 0.6805\n",
      "Iteration 25: Train Loss = 0.6796; Validation Loss = 0.6799\n",
      "Iteration 26: Train Loss = 0.6791; Validation Loss = 0.6794\n",
      "Iteration 27: Train Loss = 0.6786; Validation Loss = 0.6789\n",
      "Iteration 28: Train Loss = 0.6781; Validation Loss = 0.6784\n",
      "Iteration 29: Train Loss = 0.6775; Validation Loss = 0.6779\n",
      "Iteration 30: Train Loss = 0.6770; Validation Loss = 0.6774\n",
      "Iteration 31: Train Loss = 0.6765; Validation Loss = 0.6769\n",
      "Iteration 32: Train Loss = 0.6760; Validation Loss = 0.6764\n",
      "Iteration 33: Train Loss = 0.6755; Validation Loss = 0.6759\n",
      "Iteration 34: Train Loss = 0.6750; Validation Loss = 0.6754\n",
      "Iteration 35: Train Loss = 0.6745; Validation Loss = 0.6749\n",
      "Iteration 36: Train Loss = 0.6739; Validation Loss = 0.6744\n",
      "Iteration 37: Train Loss = 0.6734; Validation Loss = 0.6739\n",
      "Iteration 38: Train Loss = 0.6729; Validation Loss = 0.6734\n",
      "Iteration 39: Train Loss = 0.6724; Validation Loss = 0.6729\n",
      "Iteration 40: Train Loss = 0.6719; Validation Loss = 0.6724\n",
      "Iteration 41: Train Loss = 0.6714; Validation Loss = 0.6719\n",
      "Iteration 42: Train Loss = 0.6709; Validation Loss = 0.6714\n",
      "Iteration 43: Train Loss = 0.6704; Validation Loss = 0.6709\n",
      "Iteration 44: Train Loss = 0.6699; Validation Loss = 0.6705\n",
      "Iteration 45: Train Loss = 0.6694; Validation Loss = 0.6700\n",
      "Iteration 46: Train Loss = 0.6689; Validation Loss = 0.6695\n",
      "Iteration 47: Train Loss = 0.6684; Validation Loss = 0.6690\n",
      "Iteration 48: Train Loss = 0.6679; Validation Loss = 0.6685\n",
      "Iteration 49: Train Loss = 0.6674; Validation Loss = 0.6680\n",
      "Iteration 50: Train Loss = 0.6669; Validation Loss = 0.6675\n",
      "Iteration 51: Train Loss = 0.6664; Validation Loss = 0.6671\n",
      "Iteration 52: Train Loss = 0.6659; Validation Loss = 0.6666\n",
      "Iteration 53: Train Loss = 0.6654; Validation Loss = 0.6661\n",
      "Iteration 54: Train Loss = 0.6650; Validation Loss = 0.6656\n",
      "Iteration 55: Train Loss = 0.6645; Validation Loss = 0.6652\n",
      "Iteration 56: Train Loss = 0.6640; Validation Loss = 0.6647\n",
      "Iteration 57: Train Loss = 0.6635; Validation Loss = 0.6642\n",
      "Iteration 58: Train Loss = 0.6630; Validation Loss = 0.6637\n",
      "Iteration 59: Train Loss = 0.6625; Validation Loss = 0.6633\n",
      "Iteration 60: Train Loss = 0.6621; Validation Loss = 0.6628\n",
      "Iteration 61: Train Loss = 0.6616; Validation Loss = 0.6623\n",
      "Iteration 62: Train Loss = 0.6611; Validation Loss = 0.6619\n",
      "Iteration 63: Train Loss = 0.6606; Validation Loss = 0.6614\n",
      "Iteration 64: Train Loss = 0.6601; Validation Loss = 0.6609\n",
      "Iteration 65: Train Loss = 0.6597; Validation Loss = 0.6605\n",
      "Iteration 66: Train Loss = 0.6592; Validation Loss = 0.6600\n",
      "Iteration 67: Train Loss = 0.6587; Validation Loss = 0.6596\n",
      "Iteration 68: Train Loss = 0.6583; Validation Loss = 0.6591\n",
      "Iteration 69: Train Loss = 0.6578; Validation Loss = 0.6587\n",
      "Iteration 70: Train Loss = 0.6573; Validation Loss = 0.6582\n",
      "Iteration 71: Train Loss = 0.6569; Validation Loss = 0.6577\n",
      "Iteration 72: Train Loss = 0.6564; Validation Loss = 0.6573\n",
      "Iteration 73: Train Loss = 0.6559; Validation Loss = 0.6568\n",
      "Iteration 74: Train Loss = 0.6554; Validation Loss = 0.6564\n",
      "Iteration 75: Train Loss = 0.6550; Validation Loss = 0.6559\n",
      "Iteration 76: Train Loss = 0.6545; Validation Loss = 0.6555\n",
      "Iteration 77: Train Loss = 0.6541; Validation Loss = 0.6550\n",
      "Iteration 78: Train Loss = 0.6536; Validation Loss = 0.6546\n",
      "Iteration 79: Train Loss = 0.6531; Validation Loss = 0.6541\n",
      "Iteration 80: Train Loss = 0.6527; Validation Loss = 0.6537\n",
      "Iteration 81: Train Loss = 0.6522; Validation Loss = 0.6532\n",
      "Iteration 82: Train Loss = 0.6518; Validation Loss = 0.6528\n",
      "Iteration 83: Train Loss = 0.6513; Validation Loss = 0.6523\n",
      "Iteration 84: Train Loss = 0.6508; Validation Loss = 0.6519\n",
      "Iteration 85: Train Loss = 0.6504; Validation Loss = 0.6514\n",
      "Iteration 86: Train Loss = 0.6499; Validation Loss = 0.6510\n",
      "Iteration 87: Train Loss = 0.6495; Validation Loss = 0.6506\n",
      "Iteration 88: Train Loss = 0.6490; Validation Loss = 0.6501\n",
      "Iteration 89: Train Loss = 0.6486; Validation Loss = 0.6497\n",
      "Iteration 90: Train Loss = 0.6481; Validation Loss = 0.6492\n",
      "Iteration 91: Train Loss = 0.6477; Validation Loss = 0.6488\n",
      "Iteration 92: Train Loss = 0.6472; Validation Loss = 0.6484\n",
      "Iteration 93: Train Loss = 0.6468; Validation Loss = 0.6479\n",
      "Iteration 94: Train Loss = 0.6464; Validation Loss = 0.6475\n",
      "Iteration 95: Train Loss = 0.6459; Validation Loss = 0.6471\n",
      "Iteration 96: Train Loss = 0.6455; Validation Loss = 0.6466\n",
      "Iteration 97: Train Loss = 0.6450; Validation Loss = 0.6462\n",
      "Iteration 98: Train Loss = 0.6446; Validation Loss = 0.6458\n",
      "Iteration 99: Train Loss = 0.6442; Validation Loss = 0.6453\n",
      "Iteration 100: Train Loss = 0.6437; Validation Loss = 0.6449\n",
      "Iteration 101: Train Loss = 0.6433; Validation Loss = 0.6445\n",
      "Iteration 102: Train Loss = 0.6428; Validation Loss = 0.6441\n",
      "Iteration 103: Train Loss = 0.6424; Validation Loss = 0.6437\n",
      "Iteration 104: Train Loss = 0.6420; Validation Loss = 0.6432\n",
      "Iteration 105: Train Loss = 0.6416; Validation Loss = 0.6428\n",
      "Iteration 106: Train Loss = 0.6411; Validation Loss = 0.6424\n",
      "Iteration 107: Train Loss = 0.6407; Validation Loss = 0.6420\n",
      "Iteration 108: Train Loss = 0.6403; Validation Loss = 0.6415\n",
      "Iteration 109: Train Loss = 0.6398; Validation Loss = 0.6411\n",
      "Iteration 110: Train Loss = 0.6394; Validation Loss = 0.6407\n",
      "Iteration 111: Train Loss = 0.6390; Validation Loss = 0.6403\n",
      "Iteration 112: Train Loss = 0.6386; Validation Loss = 0.6399\n",
      "Iteration 113: Train Loss = 0.6381; Validation Loss = 0.6395\n",
      "Iteration 114: Train Loss = 0.6377; Validation Loss = 0.6391\n",
      "Iteration 115: Train Loss = 0.6373; Validation Loss = 0.6386\n",
      "Iteration 116: Train Loss = 0.6369; Validation Loss = 0.6382\n",
      "Iteration 117: Train Loss = 0.6364; Validation Loss = 0.6378\n",
      "Iteration 118: Train Loss = 0.6360; Validation Loss = 0.6374\n",
      "Iteration 119: Train Loss = 0.6356; Validation Loss = 0.6370\n",
      "Iteration 120: Train Loss = 0.6352; Validation Loss = 0.6366\n",
      "Iteration 121: Train Loss = 0.6348; Validation Loss = 0.6362\n",
      "Iteration 122: Train Loss = 0.6344; Validation Loss = 0.6358\n",
      "Iteration 123: Train Loss = 0.6339; Validation Loss = 0.6354\n",
      "Iteration 124: Train Loss = 0.6335; Validation Loss = 0.6350\n",
      "Iteration 125: Train Loss = 0.6331; Validation Loss = 0.6346\n",
      "Iteration 126: Train Loss = 0.6327; Validation Loss = 0.6342\n",
      "Iteration 127: Train Loss = 0.6323; Validation Loss = 0.6338\n",
      "Iteration 128: Train Loss = 0.6319; Validation Loss = 0.6334\n",
      "Iteration 129: Train Loss = 0.6315; Validation Loss = 0.6330\n",
      "Iteration 130: Train Loss = 0.6311; Validation Loss = 0.6326\n",
      "Iteration 131: Train Loss = 0.6307; Validation Loss = 0.6322\n",
      "Iteration 132: Train Loss = 0.6303; Validation Loss = 0.6318\n",
      "Iteration 133: Train Loss = 0.6299; Validation Loss = 0.6314\n",
      "Iteration 134: Train Loss = 0.6295; Validation Loss = 0.6310\n",
      "Iteration 135: Train Loss = 0.6290; Validation Loss = 0.6306\n",
      "Iteration 136: Train Loss = 0.6286; Validation Loss = 0.6302\n",
      "Iteration 137: Train Loss = 0.6282; Validation Loss = 0.6298\n",
      "Iteration 138: Train Loss = 0.6279; Validation Loss = 0.6294\n",
      "Iteration 139: Train Loss = 0.6275; Validation Loss = 0.6290\n",
      "Iteration 140: Train Loss = 0.6271; Validation Loss = 0.6287\n",
      "Iteration 141: Train Loss = 0.6267; Validation Loss = 0.6283\n",
      "Iteration 142: Train Loss = 0.6263; Validation Loss = 0.6279\n",
      "Iteration 143: Train Loss = 0.6259; Validation Loss = 0.6275\n",
      "Iteration 144: Train Loss = 0.6255; Validation Loss = 0.6271\n",
      "Iteration 145: Train Loss = 0.6251; Validation Loss = 0.6268\n",
      "Iteration 146: Train Loss = 0.6247; Validation Loss = 0.6264\n",
      "Iteration 147: Train Loss = 0.6243; Validation Loss = 0.6260\n",
      "Iteration 148: Train Loss = 0.6239; Validation Loss = 0.6256\n",
      "Iteration 149: Train Loss = 0.6235; Validation Loss = 0.6253\n",
      "Iteration 150: Train Loss = 0.6231; Validation Loss = 0.6249\n",
      "Iteration 151: Train Loss = 0.6228; Validation Loss = 0.6245\n",
      "Iteration 152: Train Loss = 0.6224; Validation Loss = 0.6241\n",
      "Iteration 153: Train Loss = 0.6220; Validation Loss = 0.6237\n",
      "Iteration 154: Train Loss = 0.6216; Validation Loss = 0.6234\n",
      "Iteration 155: Train Loss = 0.6212; Validation Loss = 0.6230\n",
      "Iteration 156: Train Loss = 0.6208; Validation Loss = 0.6226\n",
      "Iteration 157: Train Loss = 0.6205; Validation Loss = 0.6222\n",
      "Iteration 158: Train Loss = 0.6201; Validation Loss = 0.6219\n",
      "Iteration 159: Train Loss = 0.6197; Validation Loss = 0.6215\n",
      "Iteration 160: Train Loss = 0.6193; Validation Loss = 0.6211\n",
      "Iteration 161: Train Loss = 0.6189; Validation Loss = 0.6208\n",
      "Iteration 162: Train Loss = 0.6186; Validation Loss = 0.6204\n",
      "Iteration 163: Train Loss = 0.6182; Validation Loss = 0.6200\n",
      "Iteration 164: Train Loss = 0.6178; Validation Loss = 0.6197\n",
      "Iteration 165: Train Loss = 0.6174; Validation Loss = 0.6193\n",
      "Iteration 166: Train Loss = 0.6171; Validation Loss = 0.6189\n",
      "Iteration 167: Train Loss = 0.6167; Validation Loss = 0.6186\n",
      "Iteration 168: Train Loss = 0.6163; Validation Loss = 0.6182\n",
      "Iteration 169: Train Loss = 0.6159; Validation Loss = 0.6178\n",
      "Iteration 170: Train Loss = 0.6156; Validation Loss = 0.6175\n",
      "Iteration 171: Train Loss = 0.6152; Validation Loss = 0.6171\n",
      "Iteration 172: Train Loss = 0.6148; Validation Loss = 0.6168\n",
      "Iteration 173: Train Loss = 0.6145; Validation Loss = 0.6164\n",
      "Iteration 174: Train Loss = 0.6141; Validation Loss = 0.6160\n",
      "Iteration 175: Train Loss = 0.6137; Validation Loss = 0.6157\n",
      "Iteration 176: Train Loss = 0.6134; Validation Loss = 0.6153\n",
      "Iteration 177: Train Loss = 0.6130; Validation Loss = 0.6150\n",
      "Iteration 178: Train Loss = 0.6126; Validation Loss = 0.6146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:34:19,928] Trial 19 finished with value: 0.9502354452870907 and parameters: {'max_depth': 5, 'min_samples_leaf': 4, 'n_estimators': 180, 'learning_rate': 0.001735309983772959, 'subsample': 0.8906984578175009}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 179: Train Loss = 0.6123; Validation Loss = 0.6143\n",
      "Iteration 180: Train Loss = 0.6119; Validation Loss = 0.6139\n",
      "Iteration 1: Train Loss = 0.6236; Validation Loss = 0.6259\n",
      "Iteration 2: Train Loss = 0.5725; Validation Loss = 0.5775\n",
      "Iteration 3: Train Loss = 0.5344; Validation Loss = 0.5421\n",
      "Iteration 4: Train Loss = 0.5055; Validation Loss = 0.5153\n",
      "Iteration 5: Train Loss = 0.4832; Validation Loss = 0.4945\n",
      "Iteration 6: Train Loss = 0.4659; Validation Loss = 0.4794\n",
      "Iteration 7: Train Loss = 0.4526; Validation Loss = 0.4679\n",
      "Iteration 8: Train Loss = 0.4420; Validation Loss = 0.4582\n",
      "Iteration 9: Train Loss = 0.4341; Validation Loss = 0.4510\n",
      "Iteration 10: Train Loss = 0.4269; Validation Loss = 0.4451\n",
      "Iteration 11: Train Loss = 0.4215; Validation Loss = 0.4406\n",
      "Iteration 12: Train Loss = 0.4170; Validation Loss = 0.4368\n",
      "Iteration 13: Train Loss = 0.4130; Validation Loss = 0.4335\n",
      "Iteration 14: Train Loss = 0.4102; Validation Loss = 0.4309\n",
      "Iteration 15: Train Loss = 0.4080; Validation Loss = 0.4293\n",
      "Iteration 16: Train Loss = 0.4053; Validation Loss = 0.4275\n",
      "Iteration 17: Train Loss = 0.4037; Validation Loss = 0.4262\n",
      "Iteration 18: Train Loss = 0.4019; Validation Loss = 0.4254\n",
      "Iteration 19: Train Loss = 0.4006; Validation Loss = 0.4245\n",
      "Iteration 20: Train Loss = 0.3995; Validation Loss = 0.4239\n",
      "Iteration 21: Train Loss = 0.3989; Validation Loss = 0.4236\n",
      "Iteration 22: Train Loss = 0.3981; Validation Loss = 0.4232\n",
      "Iteration 23: Train Loss = 0.3975; Validation Loss = 0.4227\n",
      "Iteration 24: Train Loss = 0.3970; Validation Loss = 0.4223\n",
      "Iteration 25: Train Loss = 0.3967; Validation Loss = 0.4221\n",
      "Iteration 26: Train Loss = 0.3962; Validation Loss = 0.4221\n",
      "Iteration 27: Train Loss = 0.3958; Validation Loss = 0.4219\n",
      "Iteration 28: Train Loss = 0.3954; Validation Loss = 0.4215\n",
      "Iteration 29: Train Loss = 0.3946; Validation Loss = 0.4213\n",
      "Iteration 30: Train Loss = 0.3939; Validation Loss = 0.4209\n",
      "Iteration 31: Train Loss = 0.3936; Validation Loss = 0.4205\n",
      "Iteration 32: Train Loss = 0.3929; Validation Loss = 0.4198\n",
      "Iteration 33: Train Loss = 0.3927; Validation Loss = 0.4202\n",
      "Iteration 34: Train Loss = 0.3922; Validation Loss = 0.4200\n",
      "Iteration 35: Train Loss = 0.3918; Validation Loss = 0.4203\n",
      "Iteration 36: Train Loss = 0.3914; Validation Loss = 0.4198\n",
      "Iteration 37: Train Loss = 0.3909; Validation Loss = 0.4195\n",
      "Iteration 38: Train Loss = 0.3904; Validation Loss = 0.4192\n",
      "Iteration 39: Train Loss = 0.3903; Validation Loss = 0.4192\n",
      "Iteration 40: Train Loss = 0.3900; Validation Loss = 0.4191\n",
      "Iteration 41: Train Loss = 0.3897; Validation Loss = 0.4191\n",
      "Iteration 42: Train Loss = 0.3894; Validation Loss = 0.4188\n",
      "Iteration 43: Train Loss = 0.3892; Validation Loss = 0.4195\n",
      "Iteration 44: Train Loss = 0.3890; Validation Loss = 0.4195\n",
      "Iteration 45: Train Loss = 0.3888; Validation Loss = 0.4196\n",
      "Iteration 46: Train Loss = 0.3884; Validation Loss = 0.4195\n",
      "Iteration 47: Train Loss = 0.3884; Validation Loss = 0.4192\n",
      "Iteration 48: Train Loss = 0.3882; Validation Loss = 0.4190\n",
      "Iteration 49: Train Loss = 0.3880; Validation Loss = 0.4189\n",
      "Iteration 50: Train Loss = 0.3876; Validation Loss = 0.4187\n",
      "Iteration 51: Train Loss = 0.3872; Validation Loss = 0.4187\n",
      "Iteration 52: Train Loss = 0.3870; Validation Loss = 0.4187\n",
      "Iteration 53: Train Loss = 0.3870; Validation Loss = 0.4187\n",
      "Iteration 54: Train Loss = 0.3867; Validation Loss = 0.4187\n",
      "Iteration 55: Train Loss = 0.3863; Validation Loss = 0.4188\n",
      "Iteration 56: Train Loss = 0.3861; Validation Loss = 0.4187\n",
      "Iteration 57: Train Loss = 0.3859; Validation Loss = 0.4189\n",
      "Iteration 58: Train Loss = 0.3856; Validation Loss = 0.4190\n",
      "Iteration 59: Train Loss = 0.3852; Validation Loss = 0.4187\n",
      "Iteration 60: Train Loss = 0.3849; Validation Loss = 0.4188\n",
      "Iteration 61: Train Loss = 0.3848; Validation Loss = 0.4187\n",
      "Iteration 62: Train Loss = 0.3847; Validation Loss = 0.4190\n",
      "Iteration 63: Train Loss = 0.3847; Validation Loss = 0.4191\n",
      "Iteration 64: Train Loss = 0.3844; Validation Loss = 0.4188\n",
      "Iteration 65: Train Loss = 0.3841; Validation Loss = 0.4186\n",
      "Iteration 66: Train Loss = 0.3838; Validation Loss = 0.4186\n",
      "Iteration 67: Train Loss = 0.3838; Validation Loss = 0.4186\n",
      "Iteration 68: Train Loss = 0.3835; Validation Loss = 0.4184\n",
      "Iteration 69: Train Loss = 0.3833; Validation Loss = 0.4182\n",
      "Iteration 70: Train Loss = 0.3831; Validation Loss = 0.4181\n",
      "Iteration 71: Train Loss = 0.3824; Validation Loss = 0.4179\n",
      "Iteration 72: Train Loss = 0.3822; Validation Loss = 0.4177\n",
      "Iteration 73: Train Loss = 0.3820; Validation Loss = 0.4175\n",
      "Iteration 74: Train Loss = 0.3817; Validation Loss = 0.4173\n",
      "Iteration 75: Train Loss = 0.3813; Validation Loss = 0.4175\n",
      "Iteration 76: Train Loss = 0.3810; Validation Loss = 0.4175\n",
      "Iteration 77: Train Loss = 0.3808; Validation Loss = 0.4173\n",
      "Iteration 78: Train Loss = 0.3806; Validation Loss = 0.4174\n",
      "Iteration 79: Train Loss = 0.3805; Validation Loss = 0.4175\n",
      "Iteration 80: Train Loss = 0.3804; Validation Loss = 0.4176\n",
      "Iteration 81: Train Loss = 0.3802; Validation Loss = 0.4175\n",
      "Iteration 82: Train Loss = 0.3802; Validation Loss = 0.4176\n",
      "Iteration 83: Train Loss = 0.3799; Validation Loss = 0.4174\n",
      "Iteration 84: Train Loss = 0.3796; Validation Loss = 0.4171\n",
      "Iteration 85: Train Loss = 0.3795; Validation Loss = 0.4171\n",
      "Iteration 86: Train Loss = 0.3792; Validation Loss = 0.4174\n",
      "Iteration 87: Train Loss = 0.3789; Validation Loss = 0.4171\n",
      "Iteration 88: Train Loss = 0.3787; Validation Loss = 0.4169\n",
      "Iteration 89: Train Loss = 0.3787; Validation Loss = 0.4172\n",
      "Iteration 90: Train Loss = 0.3785; Validation Loss = 0.4171\n",
      "Iteration 91: Train Loss = 0.3785; Validation Loss = 0.4173\n",
      "Iteration 92: Train Loss = 0.3782; Validation Loss = 0.4172\n",
      "Iteration 93: Train Loss = 0.3781; Validation Loss = 0.4174\n",
      "Iteration 94: Train Loss = 0.3780; Validation Loss = 0.4173\n",
      "Iteration 95: Train Loss = 0.3777; Validation Loss = 0.4174\n",
      "Iteration 96: Train Loss = 0.3777; Validation Loss = 0.4174\n",
      "Iteration 97: Train Loss = 0.3777; Validation Loss = 0.4173\n",
      "Iteration 98: Train Loss = 0.3775; Validation Loss = 0.4173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:34:33,976] Trial 20 finished with value: 0.9530424869122923 and parameters: {'max_depth': 11, 'min_samples_leaf': 10, 'n_estimators': 190, 'learning_rate': 0.20464699240455464, 'subsample': 0.7808814218808093}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Train Loss = 0.3774; Validation Loss = 0.4171\n",
      "Early stopping at iteration 98\n",
      "Iteration 1: Train Loss = 0.6763; Validation Loss = 0.6770\n",
      "Iteration 2: Train Loss = 0.6607; Validation Loss = 0.6620\n",
      "Iteration 3: Train Loss = 0.6462; Validation Loss = 0.6482\n",
      "Iteration 4: Train Loss = 0.6326; Validation Loss = 0.6349\n",
      "Iteration 5: Train Loss = 0.6197; Validation Loss = 0.6227\n",
      "Iteration 6: Train Loss = 0.6080; Validation Loss = 0.6114\n",
      "Iteration 7: Train Loss = 0.5970; Validation Loss = 0.6010\n",
      "Iteration 8: Train Loss = 0.5866; Validation Loss = 0.5912\n",
      "Iteration 9: Train Loss = 0.5770; Validation Loss = 0.5820\n",
      "Iteration 10: Train Loss = 0.5677; Validation Loss = 0.5732\n",
      "Iteration 11: Train Loss = 0.5591; Validation Loss = 0.5649\n",
      "Iteration 12: Train Loss = 0.5510; Validation Loss = 0.5575\n",
      "Iteration 13: Train Loss = 0.5436; Validation Loss = 0.5504\n",
      "Iteration 14: Train Loss = 0.5367; Validation Loss = 0.5438\n",
      "Iteration 15: Train Loss = 0.5299; Validation Loss = 0.5374\n",
      "Iteration 16: Train Loss = 0.5236; Validation Loss = 0.5316\n",
      "Iteration 17: Train Loss = 0.5177; Validation Loss = 0.5259\n",
      "Iteration 18: Train Loss = 0.5121; Validation Loss = 0.5209\n",
      "Iteration 19: Train Loss = 0.5069; Validation Loss = 0.5160\n",
      "Iteration 20: Train Loss = 0.5018; Validation Loss = 0.5112\n",
      "Iteration 21: Train Loss = 0.4971; Validation Loss = 0.5068\n",
      "Iteration 22: Train Loss = 0.4929; Validation Loss = 0.5029\n",
      "Iteration 23: Train Loss = 0.4886; Validation Loss = 0.4989\n",
      "Iteration 24: Train Loss = 0.4847; Validation Loss = 0.4952\n",
      "Iteration 25: Train Loss = 0.4810; Validation Loss = 0.4918\n",
      "Iteration 26: Train Loss = 0.4775; Validation Loss = 0.4885\n",
      "Iteration 27: Train Loss = 0.4742; Validation Loss = 0.4855\n",
      "Iteration 28: Train Loss = 0.4710; Validation Loss = 0.4825\n",
      "Iteration 29: Train Loss = 0.4679; Validation Loss = 0.4796\n",
      "Iteration 30: Train Loss = 0.4651; Validation Loss = 0.4768\n",
      "Iteration 31: Train Loss = 0.4624; Validation Loss = 0.4744\n",
      "Iteration 32: Train Loss = 0.4599; Validation Loss = 0.4720\n",
      "Iteration 33: Train Loss = 0.4574; Validation Loss = 0.4697\n",
      "Iteration 34: Train Loss = 0.4550; Validation Loss = 0.4675\n",
      "Iteration 35: Train Loss = 0.4529; Validation Loss = 0.4655\n",
      "Iteration 36: Train Loss = 0.4507; Validation Loss = 0.4635\n",
      "Iteration 37: Train Loss = 0.4488; Validation Loss = 0.4616\n",
      "Iteration 38: Train Loss = 0.4467; Validation Loss = 0.4598\n",
      "Iteration 39: Train Loss = 0.4449; Validation Loss = 0.4582\n",
      "Iteration 40: Train Loss = 0.4431; Validation Loss = 0.4565\n",
      "Iteration 41: Train Loss = 0.4414; Validation Loss = 0.4549\n",
      "Iteration 42: Train Loss = 0.4399; Validation Loss = 0.4534\n",
      "Iteration 43: Train Loss = 0.4384; Validation Loss = 0.4521\n",
      "Iteration 44: Train Loss = 0.4369; Validation Loss = 0.4507\n",
      "Iteration 45: Train Loss = 0.4356; Validation Loss = 0.4495\n",
      "Iteration 46: Train Loss = 0.4343; Validation Loss = 0.4485\n",
      "Iteration 47: Train Loss = 0.4330; Validation Loss = 0.4472\n",
      "Iteration 48: Train Loss = 0.4317; Validation Loss = 0.4461\n",
      "Iteration 49: Train Loss = 0.4307; Validation Loss = 0.4452\n",
      "Iteration 50: Train Loss = 0.4295; Validation Loss = 0.4441\n",
      "Iteration 51: Train Loss = 0.4285; Validation Loss = 0.4433\n",
      "Iteration 52: Train Loss = 0.4275; Validation Loss = 0.4423\n",
      "Iteration 53: Train Loss = 0.4264; Validation Loss = 0.4415\n",
      "Iteration 54: Train Loss = 0.4256; Validation Loss = 0.4408\n",
      "Iteration 55: Train Loss = 0.4247; Validation Loss = 0.4399\n",
      "Iteration 56: Train Loss = 0.4239; Validation Loss = 0.4393\n",
      "Iteration 57: Train Loss = 0.4230; Validation Loss = 0.4385\n",
      "Iteration 58: Train Loss = 0.4221; Validation Loss = 0.4378\n",
      "Iteration 59: Train Loss = 0.4213; Validation Loss = 0.4372\n",
      "Iteration 60: Train Loss = 0.4207; Validation Loss = 0.4366\n",
      "Iteration 61: Train Loss = 0.4199; Validation Loss = 0.4358\n",
      "Iteration 62: Train Loss = 0.4193; Validation Loss = 0.4354\n",
      "Iteration 63: Train Loss = 0.4187; Validation Loss = 0.4349\n",
      "Iteration 64: Train Loss = 0.4181; Validation Loss = 0.4344\n",
      "Iteration 65: Train Loss = 0.4174; Validation Loss = 0.4339\n",
      "Iteration 66: Train Loss = 0.4169; Validation Loss = 0.4334\n",
      "Iteration 67: Train Loss = 0.4162; Validation Loss = 0.4329\n",
      "Iteration 68: Train Loss = 0.4158; Validation Loss = 0.4325\n",
      "Iteration 69: Train Loss = 0.4153; Validation Loss = 0.4320\n",
      "Iteration 70: Train Loss = 0.4146; Validation Loss = 0.4315\n",
      "Iteration 71: Train Loss = 0.4140; Validation Loss = 0.4310\n",
      "Iteration 72: Train Loss = 0.4136; Validation Loss = 0.4307\n",
      "Iteration 73: Train Loss = 0.4131; Validation Loss = 0.4303\n",
      "Iteration 74: Train Loss = 0.4127; Validation Loss = 0.4299\n",
      "Iteration 75: Train Loss = 0.4123; Validation Loss = 0.4295\n",
      "Iteration 76: Train Loss = 0.4120; Validation Loss = 0.4291\n",
      "Iteration 77: Train Loss = 0.4116; Validation Loss = 0.4289\n",
      "Iteration 78: Train Loss = 0.4112; Validation Loss = 0.4285\n",
      "Iteration 79: Train Loss = 0.4110; Validation Loss = 0.4284\n",
      "Iteration 80: Train Loss = 0.4106; Validation Loss = 0.4280\n",
      "Iteration 81: Train Loss = 0.4103; Validation Loss = 0.4278\n",
      "Iteration 82: Train Loss = 0.4100; Validation Loss = 0.4276\n",
      "Iteration 83: Train Loss = 0.4097; Validation Loss = 0.4273\n",
      "Iteration 84: Train Loss = 0.4095; Validation Loss = 0.4271\n",
      "Iteration 85: Train Loss = 0.4093; Validation Loss = 0.4270\n",
      "Iteration 86: Train Loss = 0.4089; Validation Loss = 0.4267\n",
      "Iteration 87: Train Loss = 0.4087; Validation Loss = 0.4266\n",
      "Iteration 88: Train Loss = 0.4084; Validation Loss = 0.4265\n",
      "Iteration 89: Train Loss = 0.4081; Validation Loss = 0.4264\n",
      "Iteration 90: Train Loss = 0.4080; Validation Loss = 0.4263\n",
      "Iteration 91: Train Loss = 0.4078; Validation Loss = 0.4261\n",
      "Iteration 92: Train Loss = 0.4075; Validation Loss = 0.4259\n",
      "Iteration 93: Train Loss = 0.4073; Validation Loss = 0.4257\n",
      "Iteration 94: Train Loss = 0.4070; Validation Loss = 0.4255\n",
      "Iteration 95: Train Loss = 0.4068; Validation Loss = 0.4254\n",
      "Iteration 96: Train Loss = 0.4067; Validation Loss = 0.4253\n",
      "Iteration 97: Train Loss = 0.4065; Validation Loss = 0.4251\n",
      "Iteration 98: Train Loss = 0.4064; Validation Loss = 0.4251\n",
      "Iteration 99: Train Loss = 0.4062; Validation Loss = 0.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:34:44,322] Trial 21 finished with value: 0.9647918739919701 and parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 100, 'learning_rate': 0.04854972971483401, 'subsample': 0.5774424994121768}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Train Loss = 0.4060; Validation Loss = 0.4250\n",
      "Iteration 1: Train Loss = 0.6866; Validation Loss = 0.6868\n",
      "Iteration 2: Train Loss = 0.6802; Validation Loss = 0.6807\n",
      "Iteration 3: Train Loss = 0.6740; Validation Loss = 0.6748\n",
      "Iteration 4: Train Loss = 0.6680; Validation Loss = 0.6689\n",
      "Iteration 5: Train Loss = 0.6621; Validation Loss = 0.6633\n",
      "Iteration 6: Train Loss = 0.6563; Validation Loss = 0.6578\n",
      "Iteration 7: Train Loss = 0.6507; Validation Loss = 0.6525\n",
      "Iteration 8: Train Loss = 0.6453; Validation Loss = 0.6472\n",
      "Iteration 9: Train Loss = 0.6400; Validation Loss = 0.6421\n",
      "Iteration 10: Train Loss = 0.6350; Validation Loss = 0.6372\n",
      "Iteration 11: Train Loss = 0.6298; Validation Loss = 0.6323\n",
      "Iteration 12: Train Loss = 0.6249; Validation Loss = 0.6276\n",
      "Iteration 13: Train Loss = 0.6201; Validation Loss = 0.6230\n",
      "Iteration 14: Train Loss = 0.6155; Validation Loss = 0.6185\n",
      "Iteration 15: Train Loss = 0.6109; Validation Loss = 0.6142\n",
      "Iteration 16: Train Loss = 0.6064; Validation Loss = 0.6099\n",
      "Iteration 17: Train Loss = 0.6021; Validation Loss = 0.6058\n",
      "Iteration 18: Train Loss = 0.5979; Validation Loss = 0.6017\n",
      "Iteration 19: Train Loss = 0.5938; Validation Loss = 0.5977\n",
      "Iteration 20: Train Loss = 0.5899; Validation Loss = 0.5940\n",
      "Iteration 21: Train Loss = 0.5860; Validation Loss = 0.5903\n",
      "Iteration 22: Train Loss = 0.5823; Validation Loss = 0.5867\n",
      "Iteration 23: Train Loss = 0.5786; Validation Loss = 0.5832\n",
      "Iteration 24: Train Loss = 0.5749; Validation Loss = 0.5797\n",
      "Iteration 25: Train Loss = 0.5714; Validation Loss = 0.5763\n",
      "Iteration 26: Train Loss = 0.5680; Validation Loss = 0.5731\n",
      "Iteration 27: Train Loss = 0.5646; Validation Loss = 0.5699\n",
      "Iteration 28: Train Loss = 0.5614; Validation Loss = 0.5668\n",
      "Iteration 29: Train Loss = 0.5581; Validation Loss = 0.5637\n",
      "Iteration 30: Train Loss = 0.5550; Validation Loss = 0.5608\n",
      "Iteration 31: Train Loss = 0.5520; Validation Loss = 0.5579\n",
      "Iteration 32: Train Loss = 0.5490; Validation Loss = 0.5550\n",
      "Iteration 33: Train Loss = 0.5461; Validation Loss = 0.5523\n",
      "Iteration 34: Train Loss = 0.5433; Validation Loss = 0.5495\n",
      "Iteration 35: Train Loss = 0.5405; Validation Loss = 0.5469\n",
      "Iteration 36: Train Loss = 0.5378; Validation Loss = 0.5443\n",
      "Iteration 37: Train Loss = 0.5352; Validation Loss = 0.5419\n",
      "Iteration 38: Train Loss = 0.5327; Validation Loss = 0.5395\n",
      "Iteration 39: Train Loss = 0.5302; Validation Loss = 0.5372\n",
      "Iteration 40: Train Loss = 0.5277; Validation Loss = 0.5348\n",
      "Iteration 41: Train Loss = 0.5253; Validation Loss = 0.5326\n",
      "Iteration 42: Train Loss = 0.5229; Validation Loss = 0.5303\n",
      "Iteration 43: Train Loss = 0.5206; Validation Loss = 0.5282\n",
      "Iteration 44: Train Loss = 0.5184; Validation Loss = 0.5260\n",
      "Iteration 45: Train Loss = 0.5161; Validation Loss = 0.5239\n",
      "Iteration 46: Train Loss = 0.5139; Validation Loss = 0.5218\n",
      "Iteration 47: Train Loss = 0.5117; Validation Loss = 0.5198\n",
      "Iteration 48: Train Loss = 0.5097; Validation Loss = 0.5179\n",
      "Iteration 49: Train Loss = 0.5077; Validation Loss = 0.5161\n",
      "Iteration 50: Train Loss = 0.5057; Validation Loss = 0.5142\n",
      "Iteration 51: Train Loss = 0.5038; Validation Loss = 0.5124\n",
      "Iteration 52: Train Loss = 0.5019; Validation Loss = 0.5106\n",
      "Iteration 53: Train Loss = 0.5001; Validation Loss = 0.5089\n",
      "Iteration 54: Train Loss = 0.4982; Validation Loss = 0.5073\n",
      "Iteration 55: Train Loss = 0.4965; Validation Loss = 0.5057\n",
      "Iteration 56: Train Loss = 0.4947; Validation Loss = 0.5041\n",
      "Iteration 57: Train Loss = 0.4930; Validation Loss = 0.5025\n",
      "Iteration 58: Train Loss = 0.4913; Validation Loss = 0.5009\n",
      "Iteration 59: Train Loss = 0.4897; Validation Loss = 0.4994\n",
      "Iteration 60: Train Loss = 0.4881; Validation Loss = 0.4980\n",
      "Iteration 61: Train Loss = 0.4866; Validation Loss = 0.4965\n",
      "Iteration 62: Train Loss = 0.4851; Validation Loss = 0.4950\n",
      "Iteration 63: Train Loss = 0.4836; Validation Loss = 0.4937\n",
      "Iteration 64: Train Loss = 0.4822; Validation Loss = 0.4923\n",
      "Iteration 65: Train Loss = 0.4807; Validation Loss = 0.4909\n",
      "Iteration 66: Train Loss = 0.4794; Validation Loss = 0.4896\n",
      "Iteration 67: Train Loss = 0.4780; Validation Loss = 0.4884\n",
      "Iteration 68: Train Loss = 0.4767; Validation Loss = 0.4872\n",
      "Iteration 69: Train Loss = 0.4753; Validation Loss = 0.4860\n",
      "Iteration 70: Train Loss = 0.4740; Validation Loss = 0.4848\n",
      "Iteration 71: Train Loss = 0.4728; Validation Loss = 0.4837\n",
      "Iteration 72: Train Loss = 0.4715; Validation Loss = 0.4826\n",
      "Iteration 73: Train Loss = 0.4703; Validation Loss = 0.4814\n",
      "Iteration 74: Train Loss = 0.4691; Validation Loss = 0.4803\n",
      "Iteration 75: Train Loss = 0.4679; Validation Loss = 0.4792\n",
      "Iteration 76: Train Loss = 0.4667; Validation Loss = 0.4781\n",
      "Iteration 77: Train Loss = 0.4656; Validation Loss = 0.4771\n",
      "Iteration 78: Train Loss = 0.4646; Validation Loss = 0.4761\n",
      "Iteration 79: Train Loss = 0.4635; Validation Loss = 0.4751\n",
      "Iteration 80: Train Loss = 0.4625; Validation Loss = 0.4741\n",
      "Iteration 81: Train Loss = 0.4614; Validation Loss = 0.4731\n",
      "Iteration 82: Train Loss = 0.4603; Validation Loss = 0.4721\n",
      "Iteration 83: Train Loss = 0.4593; Validation Loss = 0.4712\n",
      "Iteration 84: Train Loss = 0.4583; Validation Loss = 0.4703\n",
      "Iteration 85: Train Loss = 0.4574; Validation Loss = 0.4695\n",
      "Iteration 86: Train Loss = 0.4565; Validation Loss = 0.4686\n",
      "Iteration 87: Train Loss = 0.4555; Validation Loss = 0.4677\n",
      "Iteration 88: Train Loss = 0.4545; Validation Loss = 0.4669\n",
      "Iteration 89: Train Loss = 0.4537; Validation Loss = 0.4661\n",
      "Iteration 90: Train Loss = 0.4528; Validation Loss = 0.4653\n",
      "Iteration 91: Train Loss = 0.4519; Validation Loss = 0.4645\n",
      "Iteration 92: Train Loss = 0.4511; Validation Loss = 0.4638\n",
      "Iteration 93: Train Loss = 0.4502; Validation Loss = 0.4630\n",
      "Iteration 94: Train Loss = 0.4495; Validation Loss = 0.4623\n",
      "Iteration 95: Train Loss = 0.4487; Validation Loss = 0.4615\n",
      "Iteration 96: Train Loss = 0.4479; Validation Loss = 0.4608\n",
      "Iteration 97: Train Loss = 0.4472; Validation Loss = 0.4601\n",
      "Iteration 98: Train Loss = 0.4463; Validation Loss = 0.4593\n",
      "Iteration 99: Train Loss = 0.4456; Validation Loss = 0.4586\n",
      "Iteration 100: Train Loss = 0.4448; Validation Loss = 0.4579\n",
      "Iteration 101: Train Loss = 0.4442; Validation Loss = 0.4573\n",
      "Iteration 102: Train Loss = 0.4435; Validation Loss = 0.4567\n",
      "Iteration 103: Train Loss = 0.4427; Validation Loss = 0.4561\n",
      "Iteration 104: Train Loss = 0.4421; Validation Loss = 0.4555\n",
      "Iteration 105: Train Loss = 0.4414; Validation Loss = 0.4549\n",
      "Iteration 106: Train Loss = 0.4407; Validation Loss = 0.4542\n",
      "Iteration 107: Train Loss = 0.4401; Validation Loss = 0.4537\n",
      "Iteration 108: Train Loss = 0.4395; Validation Loss = 0.4531\n",
      "Iteration 109: Train Loss = 0.4389; Validation Loss = 0.4526\n",
      "Iteration 110: Train Loss = 0.4383; Validation Loss = 0.4521\n",
      "Iteration 111: Train Loss = 0.4377; Validation Loss = 0.4516\n",
      "Iteration 112: Train Loss = 0.4371; Validation Loss = 0.4511\n",
      "Iteration 113: Train Loss = 0.4366; Validation Loss = 0.4506\n",
      "Iteration 114: Train Loss = 0.4360; Validation Loss = 0.4502\n",
      "Iteration 115: Train Loss = 0.4355; Validation Loss = 0.4497\n",
      "Iteration 116: Train Loss = 0.4349; Validation Loss = 0.4492\n",
      "Iteration 117: Train Loss = 0.4344; Validation Loss = 0.4487\n",
      "Iteration 118: Train Loss = 0.4338; Validation Loss = 0.4481\n",
      "Iteration 119: Train Loss = 0.4332; Validation Loss = 0.4476\n",
      "Iteration 120: Train Loss = 0.4327; Validation Loss = 0.4472\n",
      "Iteration 121: Train Loss = 0.4322; Validation Loss = 0.4467\n",
      "Iteration 122: Train Loss = 0.4317; Validation Loss = 0.4462\n",
      "Iteration 123: Train Loss = 0.4312; Validation Loss = 0.4458\n",
      "Iteration 124: Train Loss = 0.4307; Validation Loss = 0.4454\n",
      "Iteration 125: Train Loss = 0.4302; Validation Loss = 0.4450\n",
      "Iteration 126: Train Loss = 0.4298; Validation Loss = 0.4446\n",
      "Iteration 127: Train Loss = 0.4294; Validation Loss = 0.4443\n",
      "Iteration 128: Train Loss = 0.4289; Validation Loss = 0.4439\n",
      "Iteration 129: Train Loss = 0.4285; Validation Loss = 0.4435\n",
      "Iteration 130: Train Loss = 0.4280; Validation Loss = 0.4431\n",
      "Iteration 131: Train Loss = 0.4276; Validation Loss = 0.4427\n",
      "Iteration 132: Train Loss = 0.4273; Validation Loss = 0.4425\n",
      "Iteration 133: Train Loss = 0.4269; Validation Loss = 0.4421\n",
      "Iteration 134: Train Loss = 0.4265; Validation Loss = 0.4418\n",
      "Iteration 135: Train Loss = 0.4262; Validation Loss = 0.4414\n",
      "Iteration 136: Train Loss = 0.4257; Validation Loss = 0.4411\n",
      "Iteration 137: Train Loss = 0.4253; Validation Loss = 0.4407\n",
      "Iteration 138: Train Loss = 0.4250; Validation Loss = 0.4405\n",
      "Iteration 139: Train Loss = 0.4246; Validation Loss = 0.4401\n",
      "Iteration 140: Train Loss = 0.4243; Validation Loss = 0.4398\n",
      "Iteration 141: Train Loss = 0.4239; Validation Loss = 0.4395\n",
      "Iteration 142: Train Loss = 0.4236; Validation Loss = 0.4392\n",
      "Iteration 143: Train Loss = 0.4232; Validation Loss = 0.4388\n",
      "Iteration 144: Train Loss = 0.4228; Validation Loss = 0.4386\n",
      "Iteration 145: Train Loss = 0.4224; Validation Loss = 0.4382\n",
      "Iteration 146: Train Loss = 0.4220; Validation Loss = 0.4380\n",
      "Iteration 147: Train Loss = 0.4217; Validation Loss = 0.4377\n",
      "Iteration 148: Train Loss = 0.4214; Validation Loss = 0.4374\n",
      "Iteration 149: Train Loss = 0.4211; Validation Loss = 0.4372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:34:59,122] Trial 22 finished with value: 0.9644719734318581 and parameters: {'max_depth': 8, 'min_samples_leaf': 3, 'n_estimators': 150, 'learning_rate': 0.018823879740250574, 'subsample': 0.5069517692221872}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150: Train Loss = 0.4208; Validation Loss = 0.4369\n",
      "Iteration 1: Train Loss = 0.6559; Validation Loss = 0.6567\n",
      "Iteration 2: Train Loss = 0.6250; Validation Loss = 0.6265\n",
      "Iteration 3: Train Loss = 0.5983; Validation Loss = 0.6003\n",
      "Iteration 4: Train Loss = 0.5758; Validation Loss = 0.5782\n",
      "Iteration 5: Train Loss = 0.5567; Validation Loss = 0.5601\n",
      "Iteration 6: Train Loss = 0.5402; Validation Loss = 0.5437\n",
      "Iteration 7: Train Loss = 0.5263; Validation Loss = 0.5302\n",
      "Iteration 8: Train Loss = 0.5141; Validation Loss = 0.5183\n",
      "Iteration 9: Train Loss = 0.5038; Validation Loss = 0.5082\n",
      "Iteration 10: Train Loss = 0.4946; Validation Loss = 0.4993\n",
      "Iteration 11: Train Loss = 0.4869; Validation Loss = 0.4916\n",
      "Iteration 12: Train Loss = 0.4803; Validation Loss = 0.4852\n",
      "Iteration 13: Train Loss = 0.4743; Validation Loss = 0.4793\n",
      "Iteration 14: Train Loss = 0.4688; Validation Loss = 0.4741\n",
      "Iteration 15: Train Loss = 0.4642; Validation Loss = 0.4697\n",
      "Iteration 16: Train Loss = 0.4597; Validation Loss = 0.4656\n",
      "Iteration 17: Train Loss = 0.4559; Validation Loss = 0.4621\n",
      "Iteration 18: Train Loss = 0.4526; Validation Loss = 0.4589\n",
      "Iteration 19: Train Loss = 0.4493; Validation Loss = 0.4556\n",
      "Iteration 20: Train Loss = 0.4461; Validation Loss = 0.4527\n",
      "Iteration 21: Train Loss = 0.4439; Validation Loss = 0.4506\n",
      "Iteration 22: Train Loss = 0.4418; Validation Loss = 0.4487\n",
      "Iteration 23: Train Loss = 0.4397; Validation Loss = 0.4466\n",
      "Iteration 24: Train Loss = 0.4381; Validation Loss = 0.4452\n",
      "Iteration 25: Train Loss = 0.4363; Validation Loss = 0.4437\n",
      "Iteration 26: Train Loss = 0.4351; Validation Loss = 0.4425\n",
      "Iteration 27: Train Loss = 0.4334; Validation Loss = 0.4410\n",
      "Iteration 28: Train Loss = 0.4325; Validation Loss = 0.4402\n",
      "Iteration 29: Train Loss = 0.4313; Validation Loss = 0.4392\n",
      "Iteration 30: Train Loss = 0.4305; Validation Loss = 0.4383\n",
      "Iteration 31: Train Loss = 0.4290; Validation Loss = 0.4371\n",
      "Iteration 32: Train Loss = 0.4282; Validation Loss = 0.4362\n",
      "Iteration 33: Train Loss = 0.4274; Validation Loss = 0.4357\n",
      "Iteration 34: Train Loss = 0.4269; Validation Loss = 0.4350\n",
      "Iteration 35: Train Loss = 0.4260; Validation Loss = 0.4344\n",
      "Iteration 36: Train Loss = 0.4254; Validation Loss = 0.4337\n",
      "Iteration 37: Train Loss = 0.4250; Validation Loss = 0.4335\n",
      "Iteration 38: Train Loss = 0.4244; Validation Loss = 0.4329\n",
      "Iteration 39: Train Loss = 0.4239; Validation Loss = 0.4325\n",
      "Iteration 40: Train Loss = 0.4233; Validation Loss = 0.4319\n",
      "Iteration 41: Train Loss = 0.4228; Validation Loss = 0.4314\n",
      "Iteration 42: Train Loss = 0.4226; Validation Loss = 0.4312\n",
      "Iteration 43: Train Loss = 0.4225; Validation Loss = 0.4311\n",
      "Iteration 44: Train Loss = 0.4221; Validation Loss = 0.4308\n",
      "Iteration 45: Train Loss = 0.4216; Validation Loss = 0.4303\n",
      "Iteration 46: Train Loss = 0.4213; Validation Loss = 0.4301\n",
      "Iteration 47: Train Loss = 0.4206; Validation Loss = 0.4295\n",
      "Iteration 48: Train Loss = 0.4204; Validation Loss = 0.4295\n",
      "Iteration 49: Train Loss = 0.4201; Validation Loss = 0.4292\n",
      "Iteration 50: Train Loss = 0.4199; Validation Loss = 0.4289\n",
      "Iteration 51: Train Loss = 0.4197; Validation Loss = 0.4288\n",
      "Iteration 52: Train Loss = 0.4195; Validation Loss = 0.4286\n",
      "Iteration 53: Train Loss = 0.4190; Validation Loss = 0.4282\n",
      "Iteration 54: Train Loss = 0.4188; Validation Loss = 0.4282\n",
      "Iteration 55: Train Loss = 0.4183; Validation Loss = 0.4277\n",
      "Iteration 56: Train Loss = 0.4181; Validation Loss = 0.4277\n",
      "Iteration 57: Train Loss = 0.4181; Validation Loss = 0.4276\n",
      "Iteration 58: Train Loss = 0.4180; Validation Loss = 0.4275\n",
      "Iteration 59: Train Loss = 0.4179; Validation Loss = 0.4275\n",
      "Iteration 60: Train Loss = 0.4177; Validation Loss = 0.4274\n",
      "Iteration 61: Train Loss = 0.4176; Validation Loss = 0.4274\n",
      "Iteration 62: Train Loss = 0.4175; Validation Loss = 0.4272\n",
      "Iteration 63: Train Loss = 0.4174; Validation Loss = 0.4272\n",
      "Iteration 64: Train Loss = 0.4171; Validation Loss = 0.4270\n",
      "Iteration 65: Train Loss = 0.4169; Validation Loss = 0.4269\n",
      "Iteration 66: Train Loss = 0.4167; Validation Loss = 0.4267\n",
      "Iteration 67: Train Loss = 0.4164; Validation Loss = 0.4265\n",
      "Iteration 68: Train Loss = 0.4163; Validation Loss = 0.4265\n",
      "Iteration 69: Train Loss = 0.4159; Validation Loss = 0.4261\n",
      "Iteration 70: Train Loss = 0.4160; Validation Loss = 0.4262\n",
      "Iteration 71: Train Loss = 0.4156; Validation Loss = 0.4258\n",
      "Iteration 72: Train Loss = 0.4154; Validation Loss = 0.4257\n",
      "Iteration 73: Train Loss = 0.4152; Validation Loss = 0.4256\n",
      "Iteration 74: Train Loss = 0.4148; Validation Loss = 0.4252\n",
      "Iteration 75: Train Loss = 0.4147; Validation Loss = 0.4252\n",
      "Iteration 76: Train Loss = 0.4147; Validation Loss = 0.4253\n",
      "Iteration 77: Train Loss = 0.4145; Validation Loss = 0.4252\n",
      "Iteration 78: Train Loss = 0.4145; Validation Loss = 0.4252\n",
      "Iteration 79: Train Loss = 0.4143; Validation Loss = 0.4251\n",
      "Iteration 80: Train Loss = 0.4141; Validation Loss = 0.4251\n",
      "Iteration 81: Train Loss = 0.4141; Validation Loss = 0.4250\n",
      "Iteration 82: Train Loss = 0.4142; Validation Loss = 0.4251\n",
      "Iteration 83: Train Loss = 0.4141; Validation Loss = 0.4250\n",
      "Iteration 84: Train Loss = 0.4140; Validation Loss = 0.4250\n",
      "Iteration 85: Train Loss = 0.4141; Validation Loss = 0.4251\n",
      "Iteration 86: Train Loss = 0.4140; Validation Loss = 0.4250\n",
      "Iteration 87: Train Loss = 0.4138; Validation Loss = 0.4247\n",
      "Iteration 88: Train Loss = 0.4137; Validation Loss = 0.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:06,529] Trial 23 finished with value: 0.9645936042765252 and parameters: {'max_depth': 5, 'min_samples_leaf': 6, 'n_estimators': 90, 'learning_rate': 0.12084793682922668, 'subsample': 0.5838349745647028}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89: Train Loss = 0.4135; Validation Loss = 0.4245\n",
      "Iteration 90: Train Loss = 0.4134; Validation Loss = 0.4245\n",
      "Iteration 1: Train Loss = 0.6796; Validation Loss = 0.6800\n",
      "Iteration 2: Train Loss = 0.6665; Validation Loss = 0.6676\n",
      "Iteration 3: Train Loss = 0.6541; Validation Loss = 0.6555\n",
      "Iteration 4: Train Loss = 0.6425; Validation Loss = 0.6441\n",
      "Iteration 5: Train Loss = 0.6315; Validation Loss = 0.6334\n",
      "Iteration 6: Train Loss = 0.6213; Validation Loss = 0.6235\n",
      "Iteration 7: Train Loss = 0.6116; Validation Loss = 0.6142\n",
      "Iteration 8: Train Loss = 0.6024; Validation Loss = 0.6054\n",
      "Iteration 9: Train Loss = 0.5937; Validation Loss = 0.5968\n",
      "Iteration 10: Train Loss = 0.5855; Validation Loss = 0.5891\n",
      "Iteration 11: Train Loss = 0.5777; Validation Loss = 0.5816\n",
      "Iteration 12: Train Loss = 0.5703; Validation Loss = 0.5745\n",
      "Iteration 13: Train Loss = 0.5632; Validation Loss = 0.5678\n",
      "Iteration 14: Train Loss = 0.5565; Validation Loss = 0.5612\n",
      "Iteration 15: Train Loss = 0.5502; Validation Loss = 0.5552\n",
      "Iteration 16: Train Loss = 0.5441; Validation Loss = 0.5494\n",
      "Iteration 17: Train Loss = 0.5383; Validation Loss = 0.5439\n",
      "Iteration 18: Train Loss = 0.5329; Validation Loss = 0.5387\n",
      "Iteration 19: Train Loss = 0.5277; Validation Loss = 0.5339\n",
      "Iteration 20: Train Loss = 0.5228; Validation Loss = 0.5292\n",
      "Iteration 21: Train Loss = 0.5182; Validation Loss = 0.5247\n",
      "Iteration 22: Train Loss = 0.5137; Validation Loss = 0.5206\n",
      "Iteration 23: Train Loss = 0.5095; Validation Loss = 0.5164\n",
      "Iteration 24: Train Loss = 0.5054; Validation Loss = 0.5125\n",
      "Iteration 25: Train Loss = 0.5015; Validation Loss = 0.5088\n",
      "Iteration 26: Train Loss = 0.4977; Validation Loss = 0.5054\n",
      "Iteration 27: Train Loss = 0.4942; Validation Loss = 0.5021\n",
      "Iteration 28: Train Loss = 0.4908; Validation Loss = 0.4989\n",
      "Iteration 29: Train Loss = 0.4877; Validation Loss = 0.4959\n",
      "Iteration 30: Train Loss = 0.4846; Validation Loss = 0.4930\n",
      "Iteration 31: Train Loss = 0.4817; Validation Loss = 0.4903\n",
      "Iteration 32: Train Loss = 0.4791; Validation Loss = 0.4878\n",
      "Iteration 33: Train Loss = 0.4764; Validation Loss = 0.4853\n",
      "Iteration 34: Train Loss = 0.4739; Validation Loss = 0.4829\n",
      "Iteration 35: Train Loss = 0.4715; Validation Loss = 0.4807\n",
      "Iteration 36: Train Loss = 0.4690; Validation Loss = 0.4784\n",
      "Iteration 37: Train Loss = 0.4668; Validation Loss = 0.4764\n",
      "Iteration 38: Train Loss = 0.4646; Validation Loss = 0.4743\n",
      "Iteration 39: Train Loss = 0.4625; Validation Loss = 0.4722\n",
      "Iteration 40: Train Loss = 0.4606; Validation Loss = 0.4705\n",
      "Iteration 41: Train Loss = 0.4586; Validation Loss = 0.4687\n",
      "Iteration 42: Train Loss = 0.4567; Validation Loss = 0.4669\n",
      "Iteration 43: Train Loss = 0.4549; Validation Loss = 0.4653\n",
      "Iteration 44: Train Loss = 0.4532; Validation Loss = 0.4638\n",
      "Iteration 45: Train Loss = 0.4516; Validation Loss = 0.4623\n",
      "Iteration 46: Train Loss = 0.4500; Validation Loss = 0.4608\n",
      "Iteration 47: Train Loss = 0.4485; Validation Loss = 0.4594\n",
      "Iteration 48: Train Loss = 0.4471; Validation Loss = 0.4581\n",
      "Iteration 49: Train Loss = 0.4456; Validation Loss = 0.4567\n",
      "Iteration 50: Train Loss = 0.4444; Validation Loss = 0.4555\n",
      "Iteration 51: Train Loss = 0.4431; Validation Loss = 0.4544\n",
      "Iteration 52: Train Loss = 0.4419; Validation Loss = 0.4532\n",
      "Iteration 53: Train Loss = 0.4406; Validation Loss = 0.4521\n",
      "Iteration 54: Train Loss = 0.4393; Validation Loss = 0.4509\n",
      "Iteration 55: Train Loss = 0.4382; Validation Loss = 0.4498\n",
      "Iteration 56: Train Loss = 0.4372; Validation Loss = 0.4489\n",
      "Iteration 57: Train Loss = 0.4362; Validation Loss = 0.4481\n",
      "Iteration 58: Train Loss = 0.4353; Validation Loss = 0.4473\n",
      "Iteration 59: Train Loss = 0.4344; Validation Loss = 0.4465\n",
      "Iteration 60: Train Loss = 0.4334; Validation Loss = 0.4455\n",
      "Iteration 61: Train Loss = 0.4324; Validation Loss = 0.4446\n",
      "Iteration 62: Train Loss = 0.4316; Validation Loss = 0.4440\n",
      "Iteration 63: Train Loss = 0.4308; Validation Loss = 0.4433\n",
      "Iteration 64: Train Loss = 0.4299; Validation Loss = 0.4425\n",
      "Iteration 65: Train Loss = 0.4291; Validation Loss = 0.4418\n",
      "Iteration 66: Train Loss = 0.4283; Validation Loss = 0.4411\n",
      "Iteration 67: Train Loss = 0.4275; Validation Loss = 0.4404\n",
      "Iteration 68: Train Loss = 0.4267; Validation Loss = 0.4397\n",
      "Iteration 69: Train Loss = 0.4261; Validation Loss = 0.4392\n",
      "Iteration 70: Train Loss = 0.4255; Validation Loss = 0.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:15,002] Trial 24 finished with value: 0.9643049693253873 and parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'n_estimators': 70, 'learning_rate': 0.04065921315604672, 'subsample': 0.6936094749987759}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6019; Validation Loss = 0.6050\n",
      "Iteration 2: Train Loss = 0.5436; Validation Loss = 0.5508\n",
      "Iteration 3: Train Loss = 0.5042; Validation Loss = 0.5133\n",
      "Iteration 4: Train Loss = 0.4762; Validation Loss = 0.4886\n",
      "Iteration 5: Train Loss = 0.4573; Validation Loss = 0.4706\n",
      "Iteration 6: Train Loss = 0.4445; Validation Loss = 0.4582\n",
      "Iteration 7: Train Loss = 0.4345; Validation Loss = 0.4498\n",
      "Iteration 8: Train Loss = 0.4266; Validation Loss = 0.4429\n",
      "Iteration 9: Train Loss = 0.4214; Validation Loss = 0.4382\n",
      "Iteration 10: Train Loss = 0.4166; Validation Loss = 0.4340\n",
      "Iteration 11: Train Loss = 0.4134; Validation Loss = 0.4319\n",
      "Iteration 12: Train Loss = 0.4105; Validation Loss = 0.4299\n",
      "Iteration 13: Train Loss = 0.4087; Validation Loss = 0.4285\n",
      "Iteration 14: Train Loss = 0.4068; Validation Loss = 0.4274\n",
      "Iteration 15: Train Loss = 0.4058; Validation Loss = 0.4266\n",
      "Iteration 16: Train Loss = 0.4048; Validation Loss = 0.4254\n",
      "Iteration 17: Train Loss = 0.4042; Validation Loss = 0.4254\n",
      "Iteration 18: Train Loss = 0.4038; Validation Loss = 0.4252\n",
      "Iteration 19: Train Loss = 0.4030; Validation Loss = 0.4239\n",
      "Iteration 20: Train Loss = 0.4028; Validation Loss = 0.4237\n",
      "Iteration 21: Train Loss = 0.4025; Validation Loss = 0.4234\n",
      "Iteration 22: Train Loss = 0.4009; Validation Loss = 0.4228\n",
      "Iteration 23: Train Loss = 0.4009; Validation Loss = 0.4233\n",
      "Iteration 24: Train Loss = 0.4006; Validation Loss = 0.4232\n",
      "Iteration 25: Train Loss = 0.4002; Validation Loss = 0.4226\n",
      "Iteration 26: Train Loss = 0.3993; Validation Loss = 0.4223\n",
      "Iteration 27: Train Loss = 0.3989; Validation Loss = 0.4224\n",
      "Iteration 28: Train Loss = 0.3984; Validation Loss = 0.4221\n",
      "Iteration 29: Train Loss = 0.3979; Validation Loss = 0.4216\n",
      "Iteration 30: Train Loss = 0.3975; Validation Loss = 0.4221\n",
      "Iteration 31: Train Loss = 0.3975; Validation Loss = 0.4223\n",
      "Iteration 32: Train Loss = 0.3967; Validation Loss = 0.4217\n",
      "Iteration 33: Train Loss = 0.3958; Validation Loss = 0.4211\n",
      "Iteration 34: Train Loss = 0.3954; Validation Loss = 0.4208\n",
      "Iteration 35: Train Loss = 0.3957; Validation Loss = 0.4214\n",
      "Iteration 36: Train Loss = 0.3946; Validation Loss = 0.4208\n",
      "Iteration 37: Train Loss = 0.3938; Validation Loss = 0.4208\n",
      "Iteration 38: Train Loss = 0.3938; Validation Loss = 0.4209\n",
      "Iteration 39: Train Loss = 0.3936; Validation Loss = 0.4207\n",
      "Iteration 40: Train Loss = 0.3934; Validation Loss = 0.4209\n",
      "Iteration 41: Train Loss = 0.3930; Validation Loss = 0.4209\n",
      "Iteration 42: Train Loss = 0.3929; Validation Loss = 0.4208\n",
      "Iteration 43: Train Loss = 0.3927; Validation Loss = 0.4206\n",
      "Iteration 44: Train Loss = 0.3922; Validation Loss = 0.4204\n",
      "Iteration 45: Train Loss = 0.3918; Validation Loss = 0.4207\n",
      "Iteration 46: Train Loss = 0.3916; Validation Loss = 0.4209\n",
      "Iteration 47: Train Loss = 0.3913; Validation Loss = 0.4206\n",
      "Iteration 48: Train Loss = 0.3911; Validation Loss = 0.4209\n",
      "Iteration 49: Train Loss = 0.3909; Validation Loss = 0.4209\n",
      "Iteration 50: Train Loss = 0.3907; Validation Loss = 0.4209\n",
      "Iteration 51: Train Loss = 0.3905; Validation Loss = 0.4205\n",
      "Iteration 52: Train Loss = 0.3899; Validation Loss = 0.4203\n",
      "Iteration 53: Train Loss = 0.3896; Validation Loss = 0.4202\n",
      "Iteration 54: Train Loss = 0.3893; Validation Loss = 0.4203\n",
      "Iteration 55: Train Loss = 0.3894; Validation Loss = 0.4207\n",
      "Iteration 56: Train Loss = 0.3889; Validation Loss = 0.4203\n",
      "Iteration 57: Train Loss = 0.3889; Validation Loss = 0.4205\n",
      "Iteration 58: Train Loss = 0.3889; Validation Loss = 0.4206\n",
      "Iteration 59: Train Loss = 0.3892; Validation Loss = 0.4210\n",
      "Iteration 60: Train Loss = 0.3888; Validation Loss = 0.4205\n",
      "Iteration 61: Train Loss = 0.3887; Validation Loss = 0.4205\n",
      "Iteration 62: Train Loss = 0.3885; Validation Loss = 0.4209\n",
      "Iteration 63: Train Loss = 0.3885; Validation Loss = 0.4207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:21,491] Trial 25 finished with value: 0.9517583092168729 and parameters: {'max_depth': 9, 'min_samples_leaf': 7, 'n_estimators': 120, 'learning_rate': 0.27815931193964394, 'subsample': 0.6092717674784307}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64: Train Loss = 0.3881; Validation Loss = 0.4210\n",
      "Early stopping at iteration 63\n",
      "Iteration 1: Train Loss = 0.4343; Validation Loss = 0.4478\n",
      "Iteration 2: Train Loss = 0.4150; Validation Loss = 0.4334\n",
      "Iteration 3: Train Loss = 0.4137; Validation Loss = 0.4337\n",
      "Iteration 4: Train Loss = 0.4111; Validation Loss = 0.4319\n",
      "Iteration 5: Train Loss = 0.4120; Validation Loss = 0.4343\n",
      "Iteration 6: Train Loss = 0.4092; Validation Loss = 0.4333\n",
      "Iteration 7: Train Loss = 0.4062; Validation Loss = 0.4303\n",
      "Iteration 8: Train Loss = 0.4054; Validation Loss = 0.4310\n",
      "Iteration 9: Train Loss = 0.4068; Validation Loss = 0.4337\n",
      "Iteration 10: Train Loss = 0.4028; Validation Loss = 0.4331\n",
      "Iteration 11: Train Loss = 0.4024; Validation Loss = 0.4319\n",
      "Iteration 12: Train Loss = 0.4024; Validation Loss = 0.4325\n",
      "Iteration 13: Train Loss = 0.4031; Validation Loss = 0.4324\n",
      "Iteration 14: Train Loss = 0.4009; Validation Loss = 0.4307\n",
      "Iteration 15: Train Loss = 0.4008; Validation Loss = 0.4311\n",
      "Iteration 16: Train Loss = 0.4025; Validation Loss = 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:23,223] Trial 26 finished with value: 0.9226233371080836 and parameters: {'max_depth': 10, 'min_samples_leaf': 5, 'n_estimators': 130, 'learning_rate': 0.9503126171846494, 'subsample': 0.5425052258840186}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17: Train Loss = 0.3970; Validation Loss = 0.4309\n",
      "Iteration 18: Train Loss = 0.3975; Validation Loss = 0.4328\n",
      "Early stopping at iteration 17\n",
      "Iteration 1: Train Loss = 0.6598; Validation Loss = 0.6604\n",
      "Iteration 2: Train Loss = 0.6310; Validation Loss = 0.6323\n",
      "Iteration 3: Train Loss = 0.6063; Validation Loss = 0.6083\n",
      "Iteration 4: Train Loss = 0.5848; Validation Loss = 0.5878\n",
      "Iteration 5: Train Loss = 0.5662; Validation Loss = 0.5700\n",
      "Iteration 6: Train Loss = 0.5502; Validation Loss = 0.5548\n",
      "Iteration 7: Train Loss = 0.5360; Validation Loss = 0.5410\n",
      "Iteration 8: Train Loss = 0.5234; Validation Loss = 0.5283\n",
      "Iteration 9: Train Loss = 0.5124; Validation Loss = 0.5181\n",
      "Iteration 10: Train Loss = 0.5030; Validation Loss = 0.5090\n",
      "Iteration 11: Train Loss = 0.4945; Validation Loss = 0.5006\n",
      "Iteration 12: Train Loss = 0.4872; Validation Loss = 0.4938\n",
      "Iteration 13: Train Loss = 0.4807; Validation Loss = 0.4875\n",
      "Iteration 14: Train Loss = 0.4747; Validation Loss = 0.4817\n",
      "Iteration 15: Train Loss = 0.4692; Validation Loss = 0.4765\n",
      "Iteration 16: Train Loss = 0.4642; Validation Loss = 0.4717\n",
      "Iteration 17: Train Loss = 0.4598; Validation Loss = 0.4673\n",
      "Iteration 18: Train Loss = 0.4556; Validation Loss = 0.4635\n",
      "Iteration 19: Train Loss = 0.4517; Validation Loss = 0.4600\n",
      "Iteration 20: Train Loss = 0.4485; Validation Loss = 0.4570\n",
      "Iteration 21: Train Loss = 0.4455; Validation Loss = 0.4542\n",
      "Iteration 22: Train Loss = 0.4428; Validation Loss = 0.4516\n",
      "Iteration 23: Train Loss = 0.4404; Validation Loss = 0.4493\n",
      "Iteration 24: Train Loss = 0.4381; Validation Loss = 0.4472\n",
      "Iteration 25: Train Loss = 0.4359; Validation Loss = 0.4453\n",
      "Iteration 26: Train Loss = 0.4343; Validation Loss = 0.4437\n",
      "Iteration 27: Train Loss = 0.4323; Validation Loss = 0.4419\n",
      "Iteration 28: Train Loss = 0.4309; Validation Loss = 0.4406\n",
      "Iteration 29: Train Loss = 0.4298; Validation Loss = 0.4394\n",
      "Iteration 30: Train Loss = 0.4284; Validation Loss = 0.4382\n",
      "Iteration 31: Train Loss = 0.4275; Validation Loss = 0.4374\n",
      "Iteration 32: Train Loss = 0.4264; Validation Loss = 0.4367\n",
      "Iteration 33: Train Loss = 0.4254; Validation Loss = 0.4359\n",
      "Iteration 34: Train Loss = 0.4245; Validation Loss = 0.4351\n",
      "Iteration 35: Train Loss = 0.4234; Validation Loss = 0.4340\n",
      "Iteration 36: Train Loss = 0.4227; Validation Loss = 0.4334\n",
      "Iteration 37: Train Loss = 0.4218; Validation Loss = 0.4326\n",
      "Iteration 38: Train Loss = 0.4211; Validation Loss = 0.4320\n",
      "Iteration 39: Train Loss = 0.4206; Validation Loss = 0.4316\n",
      "Iteration 40: Train Loss = 0.4199; Validation Loss = 0.4310\n",
      "Iteration 41: Train Loss = 0.4192; Validation Loss = 0.4303\n",
      "Iteration 42: Train Loss = 0.4188; Validation Loss = 0.4298\n",
      "Iteration 43: Train Loss = 0.4182; Validation Loss = 0.4294\n",
      "Iteration 44: Train Loss = 0.4178; Validation Loss = 0.4291\n",
      "Iteration 45: Train Loss = 0.4173; Validation Loss = 0.4288\n",
      "Iteration 46: Train Loss = 0.4168; Validation Loss = 0.4283\n",
      "Iteration 47: Train Loss = 0.4162; Validation Loss = 0.4279\n",
      "Iteration 48: Train Loss = 0.4158; Validation Loss = 0.4277\n",
      "Iteration 49: Train Loss = 0.4155; Validation Loss = 0.4274\n",
      "Iteration 50: Train Loss = 0.4151; Validation Loss = 0.4270\n",
      "Iteration 51: Train Loss = 0.4149; Validation Loss = 0.4270\n",
      "Iteration 52: Train Loss = 0.4145; Validation Loss = 0.4268\n",
      "Iteration 53: Train Loss = 0.4142; Validation Loss = 0.4266\n",
      "Iteration 54: Train Loss = 0.4140; Validation Loss = 0.4266\n",
      "Iteration 55: Train Loss = 0.4138; Validation Loss = 0.4265\n",
      "Iteration 56: Train Loss = 0.4135; Validation Loss = 0.4263\n",
      "Iteration 57: Train Loss = 0.4133; Validation Loss = 0.4261\n",
      "Iteration 58: Train Loss = 0.4131; Validation Loss = 0.4259\n",
      "Iteration 59: Train Loss = 0.4128; Validation Loss = 0.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:28,946] Trial 27 finished with value: 0.9646401214020718 and parameters: {'max_depth': 6, 'min_samples_leaf': 3, 'n_estimators': 60, 'learning_rate': 0.1029385694337041, 'subsample': 0.6555685755644419}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60: Train Loss = 0.4124; Validation Loss = 0.4255\n",
      "Iteration 1: Train Loss = 0.6840; Validation Loss = 0.6841\n",
      "Iteration 2: Train Loss = 0.6752; Validation Loss = 0.6755\n",
      "Iteration 3: Train Loss = 0.6668; Validation Loss = 0.6672\n",
      "Iteration 4: Train Loss = 0.6588; Validation Loss = 0.6594\n",
      "Iteration 5: Train Loss = 0.6510; Validation Loss = 0.6518\n",
      "Iteration 6: Train Loss = 0.6436; Validation Loss = 0.6447\n",
      "Iteration 7: Train Loss = 0.6364; Validation Loss = 0.6377\n",
      "Iteration 8: Train Loss = 0.6295; Validation Loss = 0.6309\n",
      "Iteration 9: Train Loss = 0.6229; Validation Loss = 0.6244\n",
      "Iteration 10: Train Loss = 0.6165; Validation Loss = 0.6182\n",
      "Iteration 11: Train Loss = 0.6105; Validation Loss = 0.6123\n",
      "Iteration 12: Train Loss = 0.6045; Validation Loss = 0.6065\n",
      "Iteration 13: Train Loss = 0.5989; Validation Loss = 0.6010\n",
      "Iteration 14: Train Loss = 0.5935; Validation Loss = 0.5958\n",
      "Iteration 15: Train Loss = 0.5882; Validation Loss = 0.5907\n",
      "Iteration 16: Train Loss = 0.5832; Validation Loss = 0.5859\n",
      "Iteration 17: Train Loss = 0.5783; Validation Loss = 0.5811\n",
      "Iteration 18: Train Loss = 0.5737; Validation Loss = 0.5766\n",
      "Iteration 19: Train Loss = 0.5692; Validation Loss = 0.5721\n",
      "Iteration 20: Train Loss = 0.5648; Validation Loss = 0.5680\n",
      "Iteration 21: Train Loss = 0.5606; Validation Loss = 0.5639\n",
      "Iteration 22: Train Loss = 0.5566; Validation Loss = 0.5601\n",
      "Iteration 23: Train Loss = 0.5527; Validation Loss = 0.5563\n",
      "Iteration 24: Train Loss = 0.5489; Validation Loss = 0.5526\n",
      "Iteration 25: Train Loss = 0.5453; Validation Loss = 0.5490\n",
      "Iteration 26: Train Loss = 0.5417; Validation Loss = 0.5456\n",
      "Iteration 27: Train Loss = 0.5383; Validation Loss = 0.5422\n",
      "Iteration 28: Train Loss = 0.5351; Validation Loss = 0.5390\n",
      "Iteration 29: Train Loss = 0.5319; Validation Loss = 0.5359\n",
      "Iteration 30: Train Loss = 0.5288; Validation Loss = 0.5329\n",
      "Iteration 31: Train Loss = 0.5258; Validation Loss = 0.5300\n",
      "Iteration 32: Train Loss = 0.5229; Validation Loss = 0.5271\n",
      "Iteration 33: Train Loss = 0.5201; Validation Loss = 0.5244\n",
      "Iteration 34: Train Loss = 0.5174; Validation Loss = 0.5218\n",
      "Iteration 35: Train Loss = 0.5148; Validation Loss = 0.5193\n",
      "Iteration 36: Train Loss = 0.5123; Validation Loss = 0.5168\n",
      "Iteration 37: Train Loss = 0.5098; Validation Loss = 0.5144\n",
      "Iteration 38: Train Loss = 0.5074; Validation Loss = 0.5121\n",
      "Iteration 39: Train Loss = 0.5052; Validation Loss = 0.5099\n",
      "Iteration 40: Train Loss = 0.5030; Validation Loss = 0.5078\n",
      "Iteration 41: Train Loss = 0.5008; Validation Loss = 0.5057\n",
      "Iteration 42: Train Loss = 0.4987; Validation Loss = 0.5037\n",
      "Iteration 43: Train Loss = 0.4967; Validation Loss = 0.5017\n",
      "Iteration 44: Train Loss = 0.4947; Validation Loss = 0.4998\n",
      "Iteration 45: Train Loss = 0.4928; Validation Loss = 0.4981\n",
      "Iteration 46: Train Loss = 0.4911; Validation Loss = 0.4964\n",
      "Iteration 47: Train Loss = 0.4893; Validation Loss = 0.4946\n",
      "Iteration 48: Train Loss = 0.4876; Validation Loss = 0.4930\n",
      "Iteration 49: Train Loss = 0.4859; Validation Loss = 0.4914\n",
      "Iteration 50: Train Loss = 0.4842; Validation Loss = 0.4898\n",
      "Iteration 51: Train Loss = 0.4826; Validation Loss = 0.4882\n",
      "Iteration 52: Train Loss = 0.4811; Validation Loss = 0.4868\n",
      "Iteration 53: Train Loss = 0.4796; Validation Loss = 0.4854\n",
      "Iteration 54: Train Loss = 0.4781; Validation Loss = 0.4840\n",
      "Iteration 55: Train Loss = 0.4768; Validation Loss = 0.4827\n",
      "Iteration 56: Train Loss = 0.4754; Validation Loss = 0.4814\n",
      "Iteration 57: Train Loss = 0.4741; Validation Loss = 0.4802\n",
      "Iteration 58: Train Loss = 0.4728; Validation Loss = 0.4790\n",
      "Iteration 59: Train Loss = 0.4715; Validation Loss = 0.4777\n",
      "Iteration 60: Train Loss = 0.4703; Validation Loss = 0.4764\n",
      "Iteration 61: Train Loss = 0.4691; Validation Loss = 0.4753\n",
      "Iteration 62: Train Loss = 0.4679; Validation Loss = 0.4742\n",
      "Iteration 63: Train Loss = 0.4667; Validation Loss = 0.4730\n",
      "Iteration 64: Train Loss = 0.4657; Validation Loss = 0.4720\n",
      "Iteration 65: Train Loss = 0.4646; Validation Loss = 0.4709\n",
      "Iteration 66: Train Loss = 0.4636; Validation Loss = 0.4700\n",
      "Iteration 67: Train Loss = 0.4625; Validation Loss = 0.4689\n",
      "Iteration 68: Train Loss = 0.4615; Validation Loss = 0.4679\n",
      "Iteration 69: Train Loss = 0.4605; Validation Loss = 0.4670\n",
      "Iteration 70: Train Loss = 0.4595; Validation Loss = 0.4660\n",
      "Iteration 71: Train Loss = 0.4585; Validation Loss = 0.4651\n",
      "Iteration 72: Train Loss = 0.4575; Validation Loss = 0.4641\n",
      "Iteration 73: Train Loss = 0.4566; Validation Loss = 0.4633\n",
      "Iteration 74: Train Loss = 0.4558; Validation Loss = 0.4624\n",
      "Iteration 75: Train Loss = 0.4550; Validation Loss = 0.4616\n",
      "Iteration 76: Train Loss = 0.4541; Validation Loss = 0.4608\n",
      "Iteration 77: Train Loss = 0.4533; Validation Loss = 0.4600\n",
      "Iteration 78: Train Loss = 0.4526; Validation Loss = 0.4594\n",
      "Iteration 79: Train Loss = 0.4518; Validation Loss = 0.4587\n",
      "Iteration 80: Train Loss = 0.4511; Validation Loss = 0.4580\n",
      "Iteration 81: Train Loss = 0.4504; Validation Loss = 0.4573\n",
      "Iteration 82: Train Loss = 0.4497; Validation Loss = 0.4567\n",
      "Iteration 83: Train Loss = 0.4490; Validation Loss = 0.4560\n",
      "Iteration 84: Train Loss = 0.4484; Validation Loss = 0.4554\n",
      "Iteration 85: Train Loss = 0.4477; Validation Loss = 0.4548\n",
      "Iteration 86: Train Loss = 0.4471; Validation Loss = 0.4542\n",
      "Iteration 87: Train Loss = 0.4464; Validation Loss = 0.4536\n",
      "Iteration 88: Train Loss = 0.4459; Validation Loss = 0.4530\n",
      "Iteration 89: Train Loss = 0.4453; Validation Loss = 0.4525\n",
      "Iteration 90: Train Loss = 0.4447; Validation Loss = 0.4520\n",
      "Iteration 91: Train Loss = 0.4442; Validation Loss = 0.4515\n",
      "Iteration 92: Train Loss = 0.4436; Validation Loss = 0.4510\n",
      "Iteration 93: Train Loss = 0.4430; Validation Loss = 0.4504\n",
      "Iteration 94: Train Loss = 0.4425; Validation Loss = 0.4499\n",
      "Iteration 95: Train Loss = 0.4420; Validation Loss = 0.4494\n",
      "Iteration 96: Train Loss = 0.4414; Validation Loss = 0.4489\n",
      "Iteration 97: Train Loss = 0.4410; Validation Loss = 0.4485\n",
      "Iteration 98: Train Loss = 0.4405; Validation Loss = 0.4481\n",
      "Iteration 99: Train Loss = 0.4401; Validation Loss = 0.4477\n",
      "Iteration 100: Train Loss = 0.4397; Validation Loss = 0.4473\n",
      "Iteration 101: Train Loss = 0.4393; Validation Loss = 0.4469\n",
      "Iteration 102: Train Loss = 0.4388; Validation Loss = 0.4464\n",
      "Iteration 103: Train Loss = 0.4384; Validation Loss = 0.4460\n",
      "Iteration 104: Train Loss = 0.4380; Validation Loss = 0.4457\n",
      "Iteration 105: Train Loss = 0.4375; Validation Loss = 0.4452\n",
      "Iteration 106: Train Loss = 0.4371; Validation Loss = 0.4448\n",
      "Iteration 107: Train Loss = 0.4368; Validation Loss = 0.4444\n",
      "Iteration 108: Train Loss = 0.4364; Validation Loss = 0.4441\n",
      "Iteration 109: Train Loss = 0.4360; Validation Loss = 0.4437\n",
      "Iteration 110: Train Loss = 0.4357; Validation Loss = 0.4434\n",
      "Iteration 111: Train Loss = 0.4353; Validation Loss = 0.4430\n",
      "Iteration 112: Train Loss = 0.4350; Validation Loss = 0.4427\n",
      "Iteration 113: Train Loss = 0.4347; Validation Loss = 0.4425\n",
      "Iteration 114: Train Loss = 0.4343; Validation Loss = 0.4421\n",
      "Iteration 115: Train Loss = 0.4340; Validation Loss = 0.4418\n",
      "Iteration 116: Train Loss = 0.4336; Validation Loss = 0.4415\n",
      "Iteration 117: Train Loss = 0.4333; Validation Loss = 0.4412\n",
      "Iteration 118: Train Loss = 0.4330; Validation Loss = 0.4410\n",
      "Iteration 119: Train Loss = 0.4327; Validation Loss = 0.4407\n",
      "Iteration 120: Train Loss = 0.4325; Validation Loss = 0.4405\n",
      "Iteration 121: Train Loss = 0.4322; Validation Loss = 0.4403\n",
      "Iteration 122: Train Loss = 0.4320; Validation Loss = 0.4401\n",
      "Iteration 123: Train Loss = 0.4318; Validation Loss = 0.4398\n",
      "Iteration 124: Train Loss = 0.4315; Validation Loss = 0.4396\n",
      "Iteration 125: Train Loss = 0.4313; Validation Loss = 0.4394\n",
      "Iteration 126: Train Loss = 0.4310; Validation Loss = 0.4391\n",
      "Iteration 127: Train Loss = 0.4308; Validation Loss = 0.4389\n",
      "Iteration 128: Train Loss = 0.4305; Validation Loss = 0.4386\n",
      "Iteration 129: Train Loss = 0.4302; Validation Loss = 0.4383\n",
      "Iteration 130: Train Loss = 0.4300; Validation Loss = 0.4382\n",
      "Iteration 131: Train Loss = 0.4297; Validation Loss = 0.4378\n",
      "Iteration 132: Train Loss = 0.4295; Validation Loss = 0.4377\n",
      "Iteration 133: Train Loss = 0.4293; Validation Loss = 0.4375\n",
      "Iteration 134: Train Loss = 0.4290; Validation Loss = 0.4372\n",
      "Iteration 135: Train Loss = 0.4288; Validation Loss = 0.4371\n",
      "Iteration 136: Train Loss = 0.4286; Validation Loss = 0.4369\n",
      "Iteration 137: Train Loss = 0.4284; Validation Loss = 0.4367\n",
      "Iteration 138: Train Loss = 0.4281; Validation Loss = 0.4365\n",
      "Iteration 139: Train Loss = 0.4279; Validation Loss = 0.4363\n",
      "Iteration 140: Train Loss = 0.4277; Validation Loss = 0.4362\n",
      "Iteration 141: Train Loss = 0.4276; Validation Loss = 0.4361\n",
      "Iteration 142: Train Loss = 0.4274; Validation Loss = 0.4359\n",
      "Iteration 143: Train Loss = 0.4273; Validation Loss = 0.4358\n",
      "Iteration 144: Train Loss = 0.4271; Validation Loss = 0.4356\n",
      "Iteration 145: Train Loss = 0.4268; Validation Loss = 0.4354\n",
      "Iteration 146: Train Loss = 0.4267; Validation Loss = 0.4353\n",
      "Iteration 147: Train Loss = 0.4265; Validation Loss = 0.4351\n",
      "Iteration 148: Train Loss = 0.4263; Validation Loss = 0.4350\n",
      "Iteration 149: Train Loss = 0.4263; Validation Loss = 0.4349\n",
      "Iteration 150: Train Loss = 0.4261; Validation Loss = 0.4348\n",
      "Iteration 151: Train Loss = 0.4261; Validation Loss = 0.4347\n",
      "Iteration 152: Train Loss = 0.4259; Validation Loss = 0.4346\n",
      "Iteration 153: Train Loss = 0.4257; Validation Loss = 0.4345\n",
      "Iteration 154: Train Loss = 0.4256; Validation Loss = 0.4344\n",
      "Iteration 155: Train Loss = 0.4255; Validation Loss = 0.4343\n",
      "Iteration 156: Train Loss = 0.4253; Validation Loss = 0.4341\n",
      "Iteration 157: Train Loss = 0.4250; Validation Loss = 0.4339\n",
      "Iteration 158: Train Loss = 0.4249; Validation Loss = 0.4338\n",
      "Iteration 159: Train Loss = 0.4248; Validation Loss = 0.4337\n",
      "Iteration 160: Train Loss = 0.4247; Validation Loss = 0.4335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:45,046] Trial 28 finished with value: 0.9640277730116789 and parameters: {'max_depth': 5, 'min_samples_leaf': 2, 'n_estimators': 160, 'learning_rate': 0.02862430257674577, 'subsample': 0.7340607922885538}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6767; Validation Loss = 0.6771\n",
      "Iteration 2: Train Loss = 0.6615; Validation Loss = 0.6621\n",
      "Iteration 3: Train Loss = 0.6477; Validation Loss = 0.6485\n",
      "Iteration 4: Train Loss = 0.6347; Validation Loss = 0.6356\n",
      "Iteration 5: Train Loss = 0.6225; Validation Loss = 0.6237\n",
      "Iteration 6: Train Loss = 0.6114; Validation Loss = 0.6129\n",
      "Iteration 7: Train Loss = 0.6010; Validation Loss = 0.6027\n",
      "Iteration 8: Train Loss = 0.5913; Validation Loss = 0.5932\n",
      "Iteration 9: Train Loss = 0.5823; Validation Loss = 0.5844\n",
      "Iteration 10: Train Loss = 0.5737; Validation Loss = 0.5761\n",
      "Iteration 11: Train Loss = 0.5659; Validation Loss = 0.5685\n",
      "Iteration 12: Train Loss = 0.5586; Validation Loss = 0.5614\n",
      "Iteration 13: Train Loss = 0.5517; Validation Loss = 0.5546\n",
      "Iteration 14: Train Loss = 0.5452; Validation Loss = 0.5481\n",
      "Iteration 15: Train Loss = 0.5392; Validation Loss = 0.5422\n",
      "Iteration 16: Train Loss = 0.5337; Validation Loss = 0.5369\n",
      "Iteration 17: Train Loss = 0.5284; Validation Loss = 0.5318\n",
      "Iteration 18: Train Loss = 0.5234; Validation Loss = 0.5269\n",
      "Iteration 19: Train Loss = 0.5188; Validation Loss = 0.5225\n",
      "Iteration 20: Train Loss = 0.5145; Validation Loss = 0.5182\n",
      "Iteration 21: Train Loss = 0.5102; Validation Loss = 0.5140\n",
      "Iteration 22: Train Loss = 0.5062; Validation Loss = 0.5101\n",
      "Iteration 23: Train Loss = 0.5025; Validation Loss = 0.5064\n",
      "Iteration 24: Train Loss = 0.4991; Validation Loss = 0.5028\n",
      "Iteration 25: Train Loss = 0.4957; Validation Loss = 0.4996\n",
      "Iteration 26: Train Loss = 0.4927; Validation Loss = 0.4966\n",
      "Iteration 27: Train Loss = 0.4899; Validation Loss = 0.4939\n",
      "Iteration 28: Train Loss = 0.4870; Validation Loss = 0.4910\n",
      "Iteration 29: Train Loss = 0.4843; Validation Loss = 0.4882\n",
      "Iteration 30: Train Loss = 0.4819; Validation Loss = 0.4858\n",
      "Iteration 31: Train Loss = 0.4794; Validation Loss = 0.4835\n",
      "Iteration 32: Train Loss = 0.4772; Validation Loss = 0.4812\n",
      "Iteration 33: Train Loss = 0.4750; Validation Loss = 0.4790\n",
      "Iteration 34: Train Loss = 0.4731; Validation Loss = 0.4772\n",
      "Iteration 35: Train Loss = 0.4711; Validation Loss = 0.4753\n",
      "Iteration 36: Train Loss = 0.4693; Validation Loss = 0.4736\n",
      "Iteration 37: Train Loss = 0.4675; Validation Loss = 0.4718\n",
      "Iteration 38: Train Loss = 0.4657; Validation Loss = 0.4702\n",
      "Iteration 39: Train Loss = 0.4641; Validation Loss = 0.4685\n",
      "Iteration 40: Train Loss = 0.4625; Validation Loss = 0.4670\n",
      "Iteration 41: Train Loss = 0.4611; Validation Loss = 0.4656\n",
      "Iteration 42: Train Loss = 0.4597; Validation Loss = 0.4643\n",
      "Iteration 43: Train Loss = 0.4584; Validation Loss = 0.4631\n",
      "Iteration 44: Train Loss = 0.4571; Validation Loss = 0.4618\n",
      "Iteration 45: Train Loss = 0.4560; Validation Loss = 0.4607\n",
      "Iteration 46: Train Loss = 0.4549; Validation Loss = 0.4597\n",
      "Iteration 47: Train Loss = 0.4537; Validation Loss = 0.4584\n",
      "Iteration 48: Train Loss = 0.4525; Validation Loss = 0.4573\n",
      "Iteration 49: Train Loss = 0.4515; Validation Loss = 0.4564\n",
      "Iteration 50: Train Loss = 0.4504; Validation Loss = 0.4554\n",
      "Iteration 51: Train Loss = 0.4493; Validation Loss = 0.4543\n",
      "Iteration 52: Train Loss = 0.4484; Validation Loss = 0.4534\n",
      "Iteration 53: Train Loss = 0.4475; Validation Loss = 0.4526\n",
      "Iteration 54: Train Loss = 0.4470; Validation Loss = 0.4520\n",
      "Iteration 55: Train Loss = 0.4461; Validation Loss = 0.4511\n",
      "Iteration 56: Train Loss = 0.4453; Validation Loss = 0.4503\n",
      "Iteration 57: Train Loss = 0.4446; Validation Loss = 0.4495\n",
      "Iteration 58: Train Loss = 0.4442; Validation Loss = 0.4493\n",
      "Iteration 59: Train Loss = 0.4435; Validation Loss = 0.4485\n",
      "Iteration 60: Train Loss = 0.4430; Validation Loss = 0.4481\n",
      "Iteration 61: Train Loss = 0.4426; Validation Loss = 0.4477\n",
      "Iteration 62: Train Loss = 0.4422; Validation Loss = 0.4473\n",
      "Iteration 63: Train Loss = 0.4416; Validation Loss = 0.4467\n",
      "Iteration 64: Train Loss = 0.4410; Validation Loss = 0.4462\n",
      "Iteration 65: Train Loss = 0.4407; Validation Loss = 0.4459\n",
      "Iteration 66: Train Loss = 0.4401; Validation Loss = 0.4454\n",
      "Iteration 67: Train Loss = 0.4396; Validation Loss = 0.4450\n",
      "Iteration 68: Train Loss = 0.4392; Validation Loss = 0.4446\n",
      "Iteration 69: Train Loss = 0.4387; Validation Loss = 0.4442\n",
      "Iteration 70: Train Loss = 0.4383; Validation Loss = 0.4437\n",
      "Iteration 71: Train Loss = 0.4377; Validation Loss = 0.4432\n",
      "Iteration 72: Train Loss = 0.4373; Validation Loss = 0.4427\n",
      "Iteration 73: Train Loss = 0.4370; Validation Loss = 0.4425\n",
      "Iteration 74: Train Loss = 0.4366; Validation Loss = 0.4421\n",
      "Iteration 75: Train Loss = 0.4361; Validation Loss = 0.4417\n",
      "Iteration 76: Train Loss = 0.4357; Validation Loss = 0.4413\n",
      "Iteration 77: Train Loss = 0.4355; Validation Loss = 0.4411\n",
      "Iteration 78: Train Loss = 0.4351; Validation Loss = 0.4407\n",
      "Iteration 79: Train Loss = 0.4348; Validation Loss = 0.4404\n",
      "Iteration 80: Train Loss = 0.4344; Validation Loss = 0.4401\n",
      "Iteration 81: Train Loss = 0.4342; Validation Loss = 0.4398\n",
      "Iteration 82: Train Loss = 0.4339; Validation Loss = 0.4396\n",
      "Iteration 83: Train Loss = 0.4337; Validation Loss = 0.4394\n",
      "Iteration 84: Train Loss = 0.4334; Validation Loss = 0.4391\n",
      "Iteration 85: Train Loss = 0.4331; Validation Loss = 0.4388\n",
      "Iteration 86: Train Loss = 0.4330; Validation Loss = 0.4386\n",
      "Iteration 87: Train Loss = 0.4326; Validation Loss = 0.4383\n",
      "Iteration 88: Train Loss = 0.4325; Validation Loss = 0.4382\n",
      "Iteration 89: Train Loss = 0.4324; Validation Loss = 0.4381\n",
      "Iteration 90: Train Loss = 0.4321; Validation Loss = 0.4378\n",
      "Iteration 91: Train Loss = 0.4320; Validation Loss = 0.4378\n",
      "Iteration 92: Train Loss = 0.4318; Validation Loss = 0.4376\n",
      "Iteration 93: Train Loss = 0.4317; Validation Loss = 0.4374\n",
      "Iteration 94: Train Loss = 0.4314; Validation Loss = 0.4373\n",
      "Iteration 95: Train Loss = 0.4312; Validation Loss = 0.4370\n",
      "Iteration 96: Train Loss = 0.4310; Validation Loss = 0.4368\n",
      "Iteration 97: Train Loss = 0.4309; Validation Loss = 0.4368\n",
      "Iteration 98: Train Loss = 0.4306; Validation Loss = 0.4365\n",
      "Iteration 99: Train Loss = 0.4304; Validation Loss = 0.4363\n",
      "Iteration 100: Train Loss = 0.4302; Validation Loss = 0.4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:35:55,895] Trial 29 finished with value: 0.9618258353064983 and parameters: {'max_depth': 4, 'min_samples_leaf': 8, 'n_estimators': 100, 'learning_rate': 0.05448698004974494, 'subsample': 0.9090536910450805}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6891; Validation Loss = 0.6893\n",
      "Iteration 2: Train Loss = 0.6851; Validation Loss = 0.6855\n",
      "Iteration 3: Train Loss = 0.6812; Validation Loss = 0.6818\n",
      "Iteration 4: Train Loss = 0.6773; Validation Loss = 0.6781\n",
      "Iteration 5: Train Loss = 0.6736; Validation Loss = 0.6745\n",
      "Iteration 6: Train Loss = 0.6698; Validation Loss = 0.6710\n",
      "Iteration 7: Train Loss = 0.6662; Validation Loss = 0.6675\n",
      "Iteration 8: Train Loss = 0.6626; Validation Loss = 0.6641\n",
      "Iteration 9: Train Loss = 0.6590; Validation Loss = 0.6608\n",
      "Iteration 10: Train Loss = 0.6555; Validation Loss = 0.6575\n",
      "Iteration 11: Train Loss = 0.6520; Validation Loss = 0.6542\n",
      "Iteration 12: Train Loss = 0.6486; Validation Loss = 0.6510\n",
      "Iteration 13: Train Loss = 0.6453; Validation Loss = 0.6478\n",
      "Iteration 14: Train Loss = 0.6420; Validation Loss = 0.6446\n",
      "Iteration 15: Train Loss = 0.6388; Validation Loss = 0.6416\n",
      "Iteration 16: Train Loss = 0.6356; Validation Loss = 0.6385\n",
      "Iteration 17: Train Loss = 0.6325; Validation Loss = 0.6356\n",
      "Iteration 18: Train Loss = 0.6294; Validation Loss = 0.6328\n",
      "Iteration 19: Train Loss = 0.6263; Validation Loss = 0.6299\n",
      "Iteration 20: Train Loss = 0.6233; Validation Loss = 0.6271\n",
      "Iteration 21: Train Loss = 0.6204; Validation Loss = 0.6243\n",
      "Iteration 22: Train Loss = 0.6175; Validation Loss = 0.6215\n",
      "Iteration 23: Train Loss = 0.6146; Validation Loss = 0.6188\n",
      "Iteration 24: Train Loss = 0.6118; Validation Loss = 0.6161\n",
      "Iteration 25: Train Loss = 0.6090; Validation Loss = 0.6134\n",
      "Iteration 26: Train Loss = 0.6062; Validation Loss = 0.6109\n",
      "Iteration 27: Train Loss = 0.6035; Validation Loss = 0.6083\n",
      "Iteration 28: Train Loss = 0.6009; Validation Loss = 0.6058\n",
      "Iteration 29: Train Loss = 0.5983; Validation Loss = 0.6034\n",
      "Iteration 30: Train Loss = 0.5957; Validation Loss = 0.6009\n",
      "Iteration 31: Train Loss = 0.5931; Validation Loss = 0.5985\n",
      "Iteration 32: Train Loss = 0.5906; Validation Loss = 0.5962\n",
      "Iteration 33: Train Loss = 0.5881; Validation Loss = 0.5939\n",
      "Iteration 34: Train Loss = 0.5857; Validation Loss = 0.5916\n",
      "Iteration 35: Train Loss = 0.5833; Validation Loss = 0.5893\n",
      "Iteration 36: Train Loss = 0.5810; Validation Loss = 0.5871\n",
      "Iteration 37: Train Loss = 0.5787; Validation Loss = 0.5850\n",
      "Iteration 38: Train Loss = 0.5764; Validation Loss = 0.5828\n",
      "Iteration 39: Train Loss = 0.5741; Validation Loss = 0.5806\n",
      "Iteration 40: Train Loss = 0.5719; Validation Loss = 0.5786\n",
      "Iteration 41: Train Loss = 0.5697; Validation Loss = 0.5765\n",
      "Iteration 42: Train Loss = 0.5675; Validation Loss = 0.5744\n",
      "Iteration 43: Train Loss = 0.5654; Validation Loss = 0.5724\n",
      "Iteration 44: Train Loss = 0.5633; Validation Loss = 0.5705\n",
      "Iteration 45: Train Loss = 0.5613; Validation Loss = 0.5686\n",
      "Iteration 46: Train Loss = 0.5592; Validation Loss = 0.5667\n",
      "Iteration 47: Train Loss = 0.5572; Validation Loss = 0.5649\n",
      "Iteration 48: Train Loss = 0.5552; Validation Loss = 0.5630\n",
      "Iteration 49: Train Loss = 0.5533; Validation Loss = 0.5612\n",
      "Iteration 50: Train Loss = 0.5513; Validation Loss = 0.5594\n",
      "Iteration 51: Train Loss = 0.5494; Validation Loss = 0.5576\n",
      "Iteration 52: Train Loss = 0.5476; Validation Loss = 0.5558\n",
      "Iteration 53: Train Loss = 0.5457; Validation Loss = 0.5541\n",
      "Iteration 54: Train Loss = 0.5439; Validation Loss = 0.5524\n",
      "Iteration 55: Train Loss = 0.5421; Validation Loss = 0.5507\n",
      "Iteration 56: Train Loss = 0.5403; Validation Loss = 0.5491\n",
      "Iteration 57: Train Loss = 0.5386; Validation Loss = 0.5475\n",
      "Iteration 58: Train Loss = 0.5369; Validation Loss = 0.5459\n",
      "Iteration 59: Train Loss = 0.5352; Validation Loss = 0.5444\n",
      "Iteration 60: Train Loss = 0.5336; Validation Loss = 0.5429\n",
      "Iteration 61: Train Loss = 0.5319; Validation Loss = 0.5413\n",
      "Iteration 62: Train Loss = 0.5303; Validation Loss = 0.5398\n",
      "Iteration 63: Train Loss = 0.5287; Validation Loss = 0.5383\n",
      "Iteration 64: Train Loss = 0.5272; Validation Loss = 0.5368\n",
      "Iteration 65: Train Loss = 0.5256; Validation Loss = 0.5354\n",
      "Iteration 66: Train Loss = 0.5241; Validation Loss = 0.5339\n",
      "Iteration 67: Train Loss = 0.5225; Validation Loss = 0.5325\n",
      "Iteration 68: Train Loss = 0.5211; Validation Loss = 0.5311\n",
      "Iteration 69: Train Loss = 0.5196; Validation Loss = 0.5298\n",
      "Iteration 70: Train Loss = 0.5181; Validation Loss = 0.5284\n",
      "Iteration 71: Train Loss = 0.5167; Validation Loss = 0.5271\n",
      "Iteration 72: Train Loss = 0.5153; Validation Loss = 0.5258\n",
      "Iteration 73: Train Loss = 0.5139; Validation Loss = 0.5245\n",
      "Iteration 74: Train Loss = 0.5125; Validation Loss = 0.5232\n",
      "Iteration 75: Train Loss = 0.5111; Validation Loss = 0.5220\n",
      "Iteration 76: Train Loss = 0.5098; Validation Loss = 0.5207\n",
      "Iteration 77: Train Loss = 0.5085; Validation Loss = 0.5195\n",
      "Iteration 78: Train Loss = 0.5072; Validation Loss = 0.5183\n",
      "Iteration 79: Train Loss = 0.5058; Validation Loss = 0.5171\n",
      "Iteration 80: Train Loss = 0.5046; Validation Loss = 0.5159\n",
      "Iteration 81: Train Loss = 0.5033; Validation Loss = 0.5148\n",
      "Iteration 82: Train Loss = 0.5021; Validation Loss = 0.5136\n",
      "Iteration 83: Train Loss = 0.5008; Validation Loss = 0.5125\n",
      "Iteration 84: Train Loss = 0.4996; Validation Loss = 0.5114\n",
      "Iteration 85: Train Loss = 0.4984; Validation Loss = 0.5103\n",
      "Iteration 86: Train Loss = 0.4973; Validation Loss = 0.5092\n",
      "Iteration 87: Train Loss = 0.4961; Validation Loss = 0.5082\n",
      "Iteration 88: Train Loss = 0.4950; Validation Loss = 0.5071\n",
      "Iteration 89: Train Loss = 0.4938; Validation Loss = 0.5061\n",
      "Iteration 90: Train Loss = 0.4927; Validation Loss = 0.5050\n",
      "Iteration 91: Train Loss = 0.4916; Validation Loss = 0.5040\n",
      "Iteration 92: Train Loss = 0.4905; Validation Loss = 0.5031\n",
      "Iteration 93: Train Loss = 0.4895; Validation Loss = 0.5021\n",
      "Iteration 94: Train Loss = 0.4884; Validation Loss = 0.5011\n",
      "Iteration 95: Train Loss = 0.4874; Validation Loss = 0.5002\n",
      "Iteration 96: Train Loss = 0.4863; Validation Loss = 0.4992\n",
      "Iteration 97: Train Loss = 0.4852; Validation Loss = 0.4983\n",
      "Iteration 98: Train Loss = 0.4842; Validation Loss = 0.4974\n",
      "Iteration 99: Train Loss = 0.4832; Validation Loss = 0.4965\n",
      "Iteration 100: Train Loss = 0.4822; Validation Loss = 0.4956\n",
      "Iteration 101: Train Loss = 0.4813; Validation Loss = 0.4947\n",
      "Iteration 102: Train Loss = 0.4803; Validation Loss = 0.4938\n",
      "Iteration 103: Train Loss = 0.4794; Validation Loss = 0.4930\n",
      "Iteration 104: Train Loss = 0.4784; Validation Loss = 0.4922\n",
      "Iteration 105: Train Loss = 0.4775; Validation Loss = 0.4914\n",
      "Iteration 106: Train Loss = 0.4766; Validation Loss = 0.4905\n",
      "Iteration 107: Train Loss = 0.4757; Validation Loss = 0.4897\n",
      "Iteration 108: Train Loss = 0.4748; Validation Loss = 0.4889\n",
      "Iteration 109: Train Loss = 0.4739; Validation Loss = 0.4881\n",
      "Iteration 110: Train Loss = 0.4730; Validation Loss = 0.4873\n",
      "Iteration 111: Train Loss = 0.4721; Validation Loss = 0.4865\n",
      "Iteration 112: Train Loss = 0.4712; Validation Loss = 0.4858\n",
      "Iteration 113: Train Loss = 0.4704; Validation Loss = 0.4850\n",
      "Iteration 114: Train Loss = 0.4695; Validation Loss = 0.4843\n",
      "Iteration 115: Train Loss = 0.4687; Validation Loss = 0.4835\n",
      "Iteration 116: Train Loss = 0.4679; Validation Loss = 0.4828\n",
      "Iteration 117: Train Loss = 0.4671; Validation Loss = 0.4820\n",
      "Iteration 118: Train Loss = 0.4663; Validation Loss = 0.4813\n",
      "Iteration 119: Train Loss = 0.4655; Validation Loss = 0.4806\n",
      "Iteration 120: Train Loss = 0.4647; Validation Loss = 0.4799\n",
      "Iteration 121: Train Loss = 0.4640; Validation Loss = 0.4792\n",
      "Iteration 122: Train Loss = 0.4632; Validation Loss = 0.4786\n",
      "Iteration 123: Train Loss = 0.4624; Validation Loss = 0.4779\n",
      "Iteration 124: Train Loss = 0.4617; Validation Loss = 0.4773\n",
      "Iteration 125: Train Loss = 0.4609; Validation Loss = 0.4766\n",
      "Iteration 126: Train Loss = 0.4602; Validation Loss = 0.4759\n",
      "Iteration 127: Train Loss = 0.4595; Validation Loss = 0.4753\n",
      "Iteration 128: Train Loss = 0.4587; Validation Loss = 0.4747\n",
      "Iteration 129: Train Loss = 0.4581; Validation Loss = 0.4741\n",
      "Iteration 130: Train Loss = 0.4574; Validation Loss = 0.4735\n",
      "Iteration 131: Train Loss = 0.4568; Validation Loss = 0.4729\n",
      "Iteration 132: Train Loss = 0.4561; Validation Loss = 0.4723\n",
      "Iteration 133: Train Loss = 0.4554; Validation Loss = 0.4717\n",
      "Iteration 134: Train Loss = 0.4548; Validation Loss = 0.4711\n",
      "Iteration 135: Train Loss = 0.4541; Validation Loss = 0.4705\n",
      "Iteration 136: Train Loss = 0.4535; Validation Loss = 0.4700\n",
      "Iteration 137: Train Loss = 0.4529; Validation Loss = 0.4694\n",
      "Iteration 138: Train Loss = 0.4522; Validation Loss = 0.4688\n",
      "Iteration 139: Train Loss = 0.4516; Validation Loss = 0.4683\n",
      "Iteration 140: Train Loss = 0.4510; Validation Loss = 0.4677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:36:20,736] Trial 30 finished with value: 0.9635896731981286 and parameters: {'max_depth': 10, 'min_samples_leaf': 6, 'n_estimators': 140, 'learning_rate': 0.011179101208116448, 'subsample': 0.8177412378114054}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6771; Validation Loss = 0.6776\n",
      "Iteration 2: Train Loss = 0.6622; Validation Loss = 0.6633\n",
      "Iteration 3: Train Loss = 0.6480; Validation Loss = 0.6497\n",
      "Iteration 4: Train Loss = 0.6348; Validation Loss = 0.6371\n",
      "Iteration 5: Train Loss = 0.6225; Validation Loss = 0.6254\n",
      "Iteration 6: Train Loss = 0.6110; Validation Loss = 0.6143\n",
      "Iteration 7: Train Loss = 0.6004; Validation Loss = 0.6039\n",
      "Iteration 8: Train Loss = 0.5902; Validation Loss = 0.5942\n",
      "Iteration 9: Train Loss = 0.5807; Validation Loss = 0.5852\n",
      "Iteration 10: Train Loss = 0.5717; Validation Loss = 0.5767\n",
      "Iteration 11: Train Loss = 0.5633; Validation Loss = 0.5688\n",
      "Iteration 12: Train Loss = 0.5553; Validation Loss = 0.5614\n",
      "Iteration 13: Train Loss = 0.5478; Validation Loss = 0.5542\n",
      "Iteration 14: Train Loss = 0.5407; Validation Loss = 0.5476\n",
      "Iteration 15: Train Loss = 0.5340; Validation Loss = 0.5413\n",
      "Iteration 16: Train Loss = 0.5279; Validation Loss = 0.5355\n",
      "Iteration 17: Train Loss = 0.5220; Validation Loss = 0.5299\n",
      "Iteration 18: Train Loss = 0.5165; Validation Loss = 0.5247\n",
      "Iteration 19: Train Loss = 0.5112; Validation Loss = 0.5198\n",
      "Iteration 20: Train Loss = 0.5062; Validation Loss = 0.5152\n",
      "Iteration 21: Train Loss = 0.5015; Validation Loss = 0.5108\n",
      "Iteration 22: Train Loss = 0.4970; Validation Loss = 0.5066\n",
      "Iteration 23: Train Loss = 0.4928; Validation Loss = 0.5028\n",
      "Iteration 24: Train Loss = 0.4888; Validation Loss = 0.4991\n",
      "Iteration 25: Train Loss = 0.4850; Validation Loss = 0.4955\n",
      "Iteration 26: Train Loss = 0.4813; Validation Loss = 0.4921\n",
      "Iteration 27: Train Loss = 0.4777; Validation Loss = 0.4886\n",
      "Iteration 28: Train Loss = 0.4742; Validation Loss = 0.4854\n",
      "Iteration 29: Train Loss = 0.4710; Validation Loss = 0.4824\n",
      "Iteration 30: Train Loss = 0.4681; Validation Loss = 0.4798\n",
      "Iteration 31: Train Loss = 0.4653; Validation Loss = 0.4773\n",
      "Iteration 32: Train Loss = 0.4627; Validation Loss = 0.4749\n",
      "Iteration 33: Train Loss = 0.4602; Validation Loss = 0.4724\n",
      "Iteration 34: Train Loss = 0.4577; Validation Loss = 0.4702\n",
      "Iteration 35: Train Loss = 0.4554; Validation Loss = 0.4682\n",
      "Iteration 36: Train Loss = 0.4531; Validation Loss = 0.4661\n",
      "Iteration 37: Train Loss = 0.4509; Validation Loss = 0.4641\n",
      "Iteration 38: Train Loss = 0.4488; Validation Loss = 0.4622\n",
      "Iteration 39: Train Loss = 0.4467; Validation Loss = 0.4603\n",
      "Iteration 40: Train Loss = 0.4449; Validation Loss = 0.4585\n",
      "Iteration 41: Train Loss = 0.4431; Validation Loss = 0.4569\n",
      "Iteration 42: Train Loss = 0.4415; Validation Loss = 0.4555\n",
      "Iteration 43: Train Loss = 0.4398; Validation Loss = 0.4543\n",
      "Iteration 44: Train Loss = 0.4383; Validation Loss = 0.4529\n",
      "Iteration 45: Train Loss = 0.4369; Validation Loss = 0.4516\n",
      "Iteration 46: Train Loss = 0.4356; Validation Loss = 0.4505\n",
      "Iteration 47: Train Loss = 0.4342; Validation Loss = 0.4493\n",
      "Iteration 48: Train Loss = 0.4329; Validation Loss = 0.4481\n",
      "Iteration 49: Train Loss = 0.4317; Validation Loss = 0.4470\n",
      "Iteration 50: Train Loss = 0.4306; Validation Loss = 0.4459\n",
      "Iteration 51: Train Loss = 0.4295; Validation Loss = 0.4449\n",
      "Iteration 52: Train Loss = 0.4285; Validation Loss = 0.4440\n",
      "Iteration 53: Train Loss = 0.4274; Validation Loss = 0.4431\n",
      "Iteration 54: Train Loss = 0.4265; Validation Loss = 0.4424\n",
      "Iteration 55: Train Loss = 0.4255; Validation Loss = 0.4415\n",
      "Iteration 56: Train Loss = 0.4246; Validation Loss = 0.4407\n",
      "Iteration 57: Train Loss = 0.4237; Validation Loss = 0.4399\n",
      "Iteration 58: Train Loss = 0.4228; Validation Loss = 0.4393\n",
      "Iteration 59: Train Loss = 0.4220; Validation Loss = 0.4385\n",
      "Iteration 60: Train Loss = 0.4213; Validation Loss = 0.4379\n",
      "Iteration 61: Train Loss = 0.4206; Validation Loss = 0.4373\n",
      "Iteration 62: Train Loss = 0.4199; Validation Loss = 0.4366\n",
      "Iteration 63: Train Loss = 0.4192; Validation Loss = 0.4358\n",
      "Iteration 64: Train Loss = 0.4186; Validation Loss = 0.4354\n",
      "Iteration 65: Train Loss = 0.4179; Validation Loss = 0.4347\n",
      "Iteration 66: Train Loss = 0.4173; Validation Loss = 0.4343\n",
      "Iteration 67: Train Loss = 0.4168; Validation Loss = 0.4338\n",
      "Iteration 68: Train Loss = 0.4162; Validation Loss = 0.4333\n",
      "Iteration 69: Train Loss = 0.4157; Validation Loss = 0.4329\n",
      "Iteration 70: Train Loss = 0.4152; Validation Loss = 0.4326\n",
      "Iteration 71: Train Loss = 0.4148; Validation Loss = 0.4322\n",
      "Iteration 72: Train Loss = 0.4143; Validation Loss = 0.4318\n",
      "Iteration 73: Train Loss = 0.4138; Validation Loss = 0.4315\n",
      "Iteration 74: Train Loss = 0.4133; Validation Loss = 0.4311\n",
      "Iteration 75: Train Loss = 0.4128; Validation Loss = 0.4306\n",
      "Iteration 76: Train Loss = 0.4124; Validation Loss = 0.4303\n",
      "Iteration 77: Train Loss = 0.4120; Validation Loss = 0.4300\n",
      "Iteration 78: Train Loss = 0.4115; Validation Loss = 0.4296\n",
      "Iteration 79: Train Loss = 0.4112; Validation Loss = 0.4293\n",
      "Iteration 80: Train Loss = 0.4108; Validation Loss = 0.4290\n",
      "Iteration 81: Train Loss = 0.4104; Validation Loss = 0.4287\n",
      "Iteration 82: Train Loss = 0.4101; Validation Loss = 0.4283\n",
      "Iteration 83: Train Loss = 0.4097; Validation Loss = 0.4280\n",
      "Iteration 84: Train Loss = 0.4094; Validation Loss = 0.4277\n",
      "Iteration 85: Train Loss = 0.4091; Validation Loss = 0.4275\n",
      "Iteration 86: Train Loss = 0.4088; Validation Loss = 0.4273\n",
      "Iteration 87: Train Loss = 0.4086; Validation Loss = 0.4271\n",
      "Iteration 88: Train Loss = 0.4083; Validation Loss = 0.4268\n",
      "Iteration 89: Train Loss = 0.4079; Validation Loss = 0.4266\n",
      "Iteration 90: Train Loss = 0.4076; Validation Loss = 0.4263\n",
      "Iteration 91: Train Loss = 0.4075; Validation Loss = 0.4262\n",
      "Iteration 92: Train Loss = 0.4072; Validation Loss = 0.4260\n",
      "Iteration 93: Train Loss = 0.4068; Validation Loss = 0.4257\n",
      "Iteration 94: Train Loss = 0.4066; Validation Loss = 0.4256\n",
      "Iteration 95: Train Loss = 0.4063; Validation Loss = 0.4253\n",
      "Iteration 96: Train Loss = 0.4060; Validation Loss = 0.4251\n",
      "Iteration 97: Train Loss = 0.4059; Validation Loss = 0.4250\n",
      "Iteration 98: Train Loss = 0.4056; Validation Loss = 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:36:30,432] Trial 31 finished with value: 0.964740018835623 and parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 100, 'learning_rate': 0.046443273901898306, 'subsample': 0.6014339194784715}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Train Loss = 0.4055; Validation Loss = 0.4247\n",
      "Iteration 100: Train Loss = 0.4053; Validation Loss = 0.4246\n",
      "Iteration 1: Train Loss = 0.6857; Validation Loss = 0.6859\n",
      "Iteration 2: Train Loss = 0.6786; Validation Loss = 0.6790\n",
      "Iteration 3: Train Loss = 0.6715; Validation Loss = 0.6723\n",
      "Iteration 4: Train Loss = 0.6648; Validation Loss = 0.6658\n",
      "Iteration 5: Train Loss = 0.6581; Validation Loss = 0.6593\n",
      "Iteration 6: Train Loss = 0.6517; Validation Loss = 0.6531\n",
      "Iteration 7: Train Loss = 0.6455; Validation Loss = 0.6472\n",
      "Iteration 8: Train Loss = 0.6395; Validation Loss = 0.6415\n",
      "Iteration 9: Train Loss = 0.6337; Validation Loss = 0.6359\n",
      "Iteration 10: Train Loss = 0.6280; Validation Loss = 0.6304\n",
      "Iteration 11: Train Loss = 0.6225; Validation Loss = 0.6252\n",
      "Iteration 12: Train Loss = 0.6171; Validation Loss = 0.6200\n",
      "Iteration 13: Train Loss = 0.6120; Validation Loss = 0.6151\n",
      "Iteration 14: Train Loss = 0.6069; Validation Loss = 0.6102\n",
      "Iteration 15: Train Loss = 0.6020; Validation Loss = 0.6055\n",
      "Iteration 16: Train Loss = 0.5972; Validation Loss = 0.6009\n",
      "Iteration 17: Train Loss = 0.5927; Validation Loss = 0.5966\n",
      "Iteration 18: Train Loss = 0.5882; Validation Loss = 0.5923\n",
      "Iteration 19: Train Loss = 0.5839; Validation Loss = 0.5882\n",
      "Iteration 20: Train Loss = 0.5797; Validation Loss = 0.5841\n",
      "Iteration 21: Train Loss = 0.5756; Validation Loss = 0.5802\n",
      "Iteration 22: Train Loss = 0.5715; Validation Loss = 0.5764\n",
      "Iteration 23: Train Loss = 0.5675; Validation Loss = 0.5726\n",
      "Iteration 24: Train Loss = 0.5638; Validation Loss = 0.5690\n",
      "Iteration 25: Train Loss = 0.5601; Validation Loss = 0.5654\n",
      "Iteration 26: Train Loss = 0.5566; Validation Loss = 0.5620\n",
      "Iteration 27: Train Loss = 0.5530; Validation Loss = 0.5587\n",
      "Iteration 28: Train Loss = 0.5497; Validation Loss = 0.5554\n",
      "Iteration 29: Train Loss = 0.5463; Validation Loss = 0.5523\n",
      "Iteration 30: Train Loss = 0.5431; Validation Loss = 0.5492\n",
      "Iteration 31: Train Loss = 0.5399; Validation Loss = 0.5463\n",
      "Iteration 32: Train Loss = 0.5369; Validation Loss = 0.5434\n",
      "Iteration 33: Train Loss = 0.5339; Validation Loss = 0.5406\n",
      "Iteration 34: Train Loss = 0.5310; Validation Loss = 0.5378\n",
      "Iteration 35: Train Loss = 0.5282; Validation Loss = 0.5351\n",
      "Iteration 36: Train Loss = 0.5254; Validation Loss = 0.5325\n",
      "Iteration 37: Train Loss = 0.5228; Validation Loss = 0.5299\n",
      "Iteration 38: Train Loss = 0.5202; Validation Loss = 0.5275\n",
      "Iteration 39: Train Loss = 0.5176; Validation Loss = 0.5251\n",
      "Iteration 40: Train Loss = 0.5151; Validation Loss = 0.5227\n",
      "Iteration 41: Train Loss = 0.5127; Validation Loss = 0.5205\n",
      "Iteration 42: Train Loss = 0.5104; Validation Loss = 0.5184\n",
      "Iteration 43: Train Loss = 0.5081; Validation Loss = 0.5162\n",
      "Iteration 44: Train Loss = 0.5058; Validation Loss = 0.5141\n",
      "Iteration 45: Train Loss = 0.5036; Validation Loss = 0.5120\n",
      "Iteration 46: Train Loss = 0.5015; Validation Loss = 0.5102\n",
      "Iteration 47: Train Loss = 0.4994; Validation Loss = 0.5081\n",
      "Iteration 48: Train Loss = 0.4974; Validation Loss = 0.5063\n",
      "Iteration 49: Train Loss = 0.4954; Validation Loss = 0.5045\n",
      "Iteration 50: Train Loss = 0.4936; Validation Loss = 0.5027\n",
      "Iteration 51: Train Loss = 0.4917; Validation Loss = 0.5009\n",
      "Iteration 52: Train Loss = 0.4898; Validation Loss = 0.4991\n",
      "Iteration 53: Train Loss = 0.4880; Validation Loss = 0.4975\n",
      "Iteration 54: Train Loss = 0.4863; Validation Loss = 0.4959\n",
      "Iteration 55: Train Loss = 0.4846; Validation Loss = 0.4943\n",
      "Iteration 56: Train Loss = 0.4830; Validation Loss = 0.4927\n",
      "Iteration 57: Train Loss = 0.4813; Validation Loss = 0.4912\n",
      "Iteration 58: Train Loss = 0.4797; Validation Loss = 0.4897\n",
      "Iteration 59: Train Loss = 0.4782; Validation Loss = 0.4883\n",
      "Iteration 60: Train Loss = 0.4768; Validation Loss = 0.4870\n",
      "Iteration 61: Train Loss = 0.4754; Validation Loss = 0.4857\n",
      "Iteration 62: Train Loss = 0.4739; Validation Loss = 0.4843\n",
      "Iteration 63: Train Loss = 0.4725; Validation Loss = 0.4831\n",
      "Iteration 64: Train Loss = 0.4711; Validation Loss = 0.4818\n",
      "Iteration 65: Train Loss = 0.4697; Validation Loss = 0.4805\n",
      "Iteration 66: Train Loss = 0.4684; Validation Loss = 0.4792\n",
      "Iteration 67: Train Loss = 0.4671; Validation Loss = 0.4781\n",
      "Iteration 68: Train Loss = 0.4658; Validation Loss = 0.4770\n",
      "Iteration 69: Train Loss = 0.4647; Validation Loss = 0.4759\n",
      "Iteration 70: Train Loss = 0.4634; Validation Loss = 0.4748\n",
      "Iteration 71: Train Loss = 0.4622; Validation Loss = 0.4737\n",
      "Iteration 72: Train Loss = 0.4611; Validation Loss = 0.4726\n",
      "Iteration 73: Train Loss = 0.4599; Validation Loss = 0.4715\n",
      "Iteration 74: Train Loss = 0.4588; Validation Loss = 0.4705\n",
      "Iteration 75: Train Loss = 0.4577; Validation Loss = 0.4695\n",
      "Iteration 76: Train Loss = 0.4566; Validation Loss = 0.4685\n",
      "Iteration 77: Train Loss = 0.4556; Validation Loss = 0.4675\n",
      "Iteration 78: Train Loss = 0.4546; Validation Loss = 0.4666\n",
      "Iteration 79: Train Loss = 0.4536; Validation Loss = 0.4657\n",
      "Iteration 80: Train Loss = 0.4527; Validation Loss = 0.4649\n",
      "Iteration 81: Train Loss = 0.4518; Validation Loss = 0.4641\n",
      "Iteration 82: Train Loss = 0.4509; Validation Loss = 0.4632\n",
      "Iteration 83: Train Loss = 0.4499; Validation Loss = 0.4623\n",
      "Iteration 84: Train Loss = 0.4491; Validation Loss = 0.4615\n",
      "Iteration 85: Train Loss = 0.4482; Validation Loss = 0.4607\n",
      "Iteration 86: Train Loss = 0.4473; Validation Loss = 0.4600\n",
      "Iteration 87: Train Loss = 0.4466; Validation Loss = 0.4593\n",
      "Iteration 88: Train Loss = 0.4458; Validation Loss = 0.4585\n",
      "Iteration 89: Train Loss = 0.4450; Validation Loss = 0.4578\n",
      "Iteration 90: Train Loss = 0.4442; Validation Loss = 0.4571\n",
      "Iteration 91: Train Loss = 0.4435; Validation Loss = 0.4563\n",
      "Iteration 92: Train Loss = 0.4428; Validation Loss = 0.4557\n",
      "Iteration 93: Train Loss = 0.4420; Validation Loss = 0.4551\n",
      "Iteration 94: Train Loss = 0.4413; Validation Loss = 0.4545\n",
      "Iteration 95: Train Loss = 0.4406; Validation Loss = 0.4538\n",
      "Iteration 96: Train Loss = 0.4400; Validation Loss = 0.4532\n",
      "Iteration 97: Train Loss = 0.4392; Validation Loss = 0.4525\n",
      "Iteration 98: Train Loss = 0.4386; Validation Loss = 0.4520\n",
      "Iteration 99: Train Loss = 0.4379; Validation Loss = 0.4513\n",
      "Iteration 100: Train Loss = 0.4372; Validation Loss = 0.4507\n",
      "Iteration 101: Train Loss = 0.4366; Validation Loss = 0.4502\n",
      "Iteration 102: Train Loss = 0.4360; Validation Loss = 0.4497\n",
      "Iteration 103: Train Loss = 0.4355; Validation Loss = 0.4492\n",
      "Iteration 104: Train Loss = 0.4349; Validation Loss = 0.4487\n",
      "Iteration 105: Train Loss = 0.4343; Validation Loss = 0.4481\n",
      "Iteration 106: Train Loss = 0.4337; Validation Loss = 0.4476\n",
      "Iteration 107: Train Loss = 0.4332; Validation Loss = 0.4471\n",
      "Iteration 108: Train Loss = 0.4327; Validation Loss = 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:36:41,152] Trial 32 finished with value: 0.9632876931699896 and parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 110, 'learning_rate': 0.021532848488878285, 'subsample': 0.534221922389372}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109: Train Loss = 0.4321; Validation Loss = 0.4460\n",
      "Iteration 110: Train Loss = 0.4315; Validation Loss = 0.4455\n",
      "Iteration 1: Train Loss = 0.6506; Validation Loss = 0.6520\n",
      "Iteration 2: Train Loss = 0.6157; Validation Loss = 0.6180\n",
      "Iteration 3: Train Loss = 0.5866; Validation Loss = 0.5900\n",
      "Iteration 4: Train Loss = 0.5621; Validation Loss = 0.5668\n",
      "Iteration 5: Train Loss = 0.5419; Validation Loss = 0.5470\n",
      "Iteration 6: Train Loss = 0.5244; Validation Loss = 0.5304\n",
      "Iteration 7: Train Loss = 0.5100; Validation Loss = 0.5166\n",
      "Iteration 8: Train Loss = 0.4971; Validation Loss = 0.5040\n",
      "Iteration 9: Train Loss = 0.4862; Validation Loss = 0.4936\n",
      "Iteration 10: Train Loss = 0.4773; Validation Loss = 0.4850\n",
      "Iteration 11: Train Loss = 0.4692; Validation Loss = 0.4772\n",
      "Iteration 12: Train Loss = 0.4622; Validation Loss = 0.4709\n",
      "Iteration 13: Train Loss = 0.4556; Validation Loss = 0.4649\n",
      "Iteration 14: Train Loss = 0.4504; Validation Loss = 0.4601\n",
      "Iteration 15: Train Loss = 0.4459; Validation Loss = 0.4560\n",
      "Iteration 16: Train Loss = 0.4419; Validation Loss = 0.4524\n",
      "Iteration 17: Train Loss = 0.4382; Validation Loss = 0.4491\n",
      "Iteration 18: Train Loss = 0.4352; Validation Loss = 0.4463\n",
      "Iteration 19: Train Loss = 0.4324; Validation Loss = 0.4441\n",
      "Iteration 20: Train Loss = 0.4296; Validation Loss = 0.4414\n",
      "Iteration 21: Train Loss = 0.4273; Validation Loss = 0.4393\n",
      "Iteration 22: Train Loss = 0.4252; Validation Loss = 0.4377\n",
      "Iteration 23: Train Loss = 0.4233; Validation Loss = 0.4360\n",
      "Iteration 24: Train Loss = 0.4217; Validation Loss = 0.4345\n",
      "Iteration 25: Train Loss = 0.4202; Validation Loss = 0.4332\n",
      "Iteration 26: Train Loss = 0.4184; Validation Loss = 0.4318\n",
      "Iteration 27: Train Loss = 0.4175; Validation Loss = 0.4312\n",
      "Iteration 28: Train Loss = 0.4165; Validation Loss = 0.4304\n",
      "Iteration 29: Train Loss = 0.4156; Validation Loss = 0.4297\n",
      "Iteration 30: Train Loss = 0.4148; Validation Loss = 0.4290\n",
      "Iteration 31: Train Loss = 0.4140; Validation Loss = 0.4287\n",
      "Iteration 32: Train Loss = 0.4131; Validation Loss = 0.4280\n",
      "Iteration 33: Train Loss = 0.4123; Validation Loss = 0.4273\n",
      "Iteration 34: Train Loss = 0.4119; Validation Loss = 0.4270\n",
      "Iteration 35: Train Loss = 0.4111; Validation Loss = 0.4263\n",
      "Iteration 36: Train Loss = 0.4107; Validation Loss = 0.4258\n",
      "Iteration 37: Train Loss = 0.4100; Validation Loss = 0.4256\n",
      "Iteration 38: Train Loss = 0.4095; Validation Loss = 0.4251\n",
      "Iteration 39: Train Loss = 0.4089; Validation Loss = 0.4248\n",
      "Iteration 40: Train Loss = 0.4087; Validation Loss = 0.4247\n",
      "Iteration 41: Train Loss = 0.4085; Validation Loss = 0.4246\n",
      "Iteration 42: Train Loss = 0.4081; Validation Loss = 0.4243\n",
      "Iteration 43: Train Loss = 0.4079; Validation Loss = 0.4242\n",
      "Iteration 44: Train Loss = 0.4076; Validation Loss = 0.4241\n",
      "Iteration 45: Train Loss = 0.4073; Validation Loss = 0.4238\n",
      "Iteration 46: Train Loss = 0.4068; Validation Loss = 0.4236\n",
      "Iteration 47: Train Loss = 0.4065; Validation Loss = 0.4237\n",
      "Iteration 48: Train Loss = 0.4060; Validation Loss = 0.4233\n",
      "Iteration 49: Train Loss = 0.4059; Validation Loss = 0.4234\n",
      "Iteration 50: Train Loss = 0.4058; Validation Loss = 0.4233\n",
      "Iteration 51: Train Loss = 0.4056; Validation Loss = 0.4232\n",
      "Iteration 52: Train Loss = 0.4054; Validation Loss = 0.4230\n",
      "Iteration 53: Train Loss = 0.4052; Validation Loss = 0.4229\n",
      "Iteration 54: Train Loss = 0.4050; Validation Loss = 0.4228\n",
      "Iteration 55: Train Loss = 0.4049; Validation Loss = 0.4227\n",
      "Iteration 56: Train Loss = 0.4047; Validation Loss = 0.4224\n",
      "Iteration 57: Train Loss = 0.4044; Validation Loss = 0.4222\n",
      "Iteration 58: Train Loss = 0.4041; Validation Loss = 0.4220\n",
      "Iteration 59: Train Loss = 0.4037; Validation Loss = 0.4219\n",
      "Iteration 60: Train Loss = 0.4037; Validation Loss = 0.4220\n",
      "Iteration 61: Train Loss = 0.4035; Validation Loss = 0.4219\n",
      "Iteration 62: Train Loss = 0.4033; Validation Loss = 0.4219\n",
      "Iteration 63: Train Loss = 0.4032; Validation Loss = 0.4220\n",
      "Iteration 64: Train Loss = 0.4031; Validation Loss = 0.4219\n",
      "Iteration 65: Train Loss = 0.4028; Validation Loss = 0.4217\n",
      "Iteration 66: Train Loss = 0.4027; Validation Loss = 0.4216\n",
      "Iteration 67: Train Loss = 0.4024; Validation Loss = 0.4213\n",
      "Iteration 68: Train Loss = 0.4023; Validation Loss = 0.4215\n",
      "Iteration 69: Train Loss = 0.4021; Validation Loss = 0.4215\n",
      "Iteration 70: Train Loss = 0.4018; Validation Loss = 0.4216\n",
      "Iteration 71: Train Loss = 0.4017; Validation Loss = 0.4215\n",
      "Iteration 72: Train Loss = 0.4015; Validation Loss = 0.4215\n",
      "Iteration 73: Train Loss = 0.4016; Validation Loss = 0.4215\n",
      "Iteration 74: Train Loss = 0.4014; Validation Loss = 0.4217\n",
      "Iteration 75: Train Loss = 0.4012; Validation Loss = 0.4215\n",
      "Iteration 76: Train Loss = 0.4010; Validation Loss = 0.4214\n",
      "Iteration 77: Train Loss = 0.4007; Validation Loss = 0.4210\n",
      "Iteration 78: Train Loss = 0.4006; Validation Loss = 0.4209\n",
      "Iteration 79: Train Loss = 0.4006; Validation Loss = 0.4209\n",
      "Iteration 80: Train Loss = 0.4004; Validation Loss = 0.4209\n",
      "Iteration 81: Train Loss = 0.4002; Validation Loss = 0.4208\n",
      "Iteration 82: Train Loss = 0.4002; Validation Loss = 0.4209\n",
      "Iteration 83: Train Loss = 0.4000; Validation Loss = 0.4209\n",
      "Iteration 84: Train Loss = 0.3997; Validation Loss = 0.4206\n",
      "Iteration 85: Train Loss = 0.3993; Validation Loss = 0.4202\n",
      "Iteration 86: Train Loss = 0.3991; Validation Loss = 0.4201\n",
      "Iteration 87: Train Loss = 0.3992; Validation Loss = 0.4202\n",
      "Iteration 88: Train Loss = 0.3991; Validation Loss = 0.4200\n",
      "Iteration 89: Train Loss = 0.3991; Validation Loss = 0.4202\n",
      "Iteration 90: Train Loss = 0.3990; Validation Loss = 0.4200\n",
      "Iteration 91: Train Loss = 0.3987; Validation Loss = 0.4200\n",
      "Iteration 92: Train Loss = 0.3986; Validation Loss = 0.4201\n",
      "Iteration 93: Train Loss = 0.3986; Validation Loss = 0.4201\n",
      "Iteration 94: Train Loss = 0.3986; Validation Loss = 0.4201\n",
      "Iteration 95: Train Loss = 0.3985; Validation Loss = 0.4201\n",
      "Iteration 96: Train Loss = 0.3985; Validation Loss = 0.4201\n",
      "Iteration 97: Train Loss = 0.3980; Validation Loss = 0.4198\n",
      "Iteration 98: Train Loss = 0.3979; Validation Loss = 0.4197\n",
      "Iteration 99: Train Loss = 0.3978; Validation Loss = 0.4198\n",
      "Iteration 100: Train Loss = 0.3975; Validation Loss = 0.4195\n",
      "Iteration 101: Train Loss = 0.3975; Validation Loss = 0.4196\n",
      "Iteration 102: Train Loss = 0.3974; Validation Loss = 0.4198\n",
      "Iteration 103: Train Loss = 0.3973; Validation Loss = 0.4197\n",
      "Iteration 104: Train Loss = 0.3973; Validation Loss = 0.4197\n",
      "Iteration 105: Train Loss = 0.3973; Validation Loss = 0.4198\n",
      "Iteration 106: Train Loss = 0.3971; Validation Loss = 0.4197\n",
      "Iteration 107: Train Loss = 0.3970; Validation Loss = 0.4196\n",
      "Iteration 108: Train Loss = 0.3968; Validation Loss = 0.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:36:50,020] Trial 33 finished with value: 0.964121569838601 and parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'n_estimators': 110, 'learning_rate': 0.12785186458198602, 'subsample': 0.5802834730416102}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109: Train Loss = 0.3966; Validation Loss = 0.4197\n",
      "Iteration 110: Train Loss = 0.3966; Validation Loss = 0.4196\n",
      "Iteration 1: Train Loss = 0.6730; Validation Loss = 0.6739\n",
      "Iteration 2: Train Loss = 0.6544; Validation Loss = 0.6563\n",
      "Iteration 3: Train Loss = 0.6373; Validation Loss = 0.6399\n",
      "Iteration 4: Train Loss = 0.6215; Validation Loss = 0.6247\n",
      "Iteration 5: Train Loss = 0.6069; Validation Loss = 0.6110\n",
      "Iteration 6: Train Loss = 0.5936; Validation Loss = 0.5986\n",
      "Iteration 7: Train Loss = 0.5813; Validation Loss = 0.5868\n",
      "Iteration 8: Train Loss = 0.5699; Validation Loss = 0.5759\n",
      "Iteration 9: Train Loss = 0.5592; Validation Loss = 0.5660\n",
      "Iteration 10: Train Loss = 0.5496; Validation Loss = 0.5570\n",
      "Iteration 11: Train Loss = 0.5405; Validation Loss = 0.5484\n",
      "Iteration 12: Train Loss = 0.5319; Validation Loss = 0.5405\n",
      "Iteration 13: Train Loss = 0.5240; Validation Loss = 0.5329\n",
      "Iteration 14: Train Loss = 0.5165; Validation Loss = 0.5259\n",
      "Iteration 15: Train Loss = 0.5098; Validation Loss = 0.5198\n",
      "Iteration 16: Train Loss = 0.5035; Validation Loss = 0.5139\n",
      "Iteration 17: Train Loss = 0.4975; Validation Loss = 0.5081\n",
      "Iteration 18: Train Loss = 0.4917; Validation Loss = 0.5029\n",
      "Iteration 19: Train Loss = 0.4865; Validation Loss = 0.4980\n",
      "Iteration 20: Train Loss = 0.4813; Validation Loss = 0.4936\n",
      "Iteration 21: Train Loss = 0.4765; Validation Loss = 0.4890\n",
      "Iteration 22: Train Loss = 0.4723; Validation Loss = 0.4851\n",
      "Iteration 23: Train Loss = 0.4684; Validation Loss = 0.4814\n",
      "Iteration 24: Train Loss = 0.4646; Validation Loss = 0.4783\n",
      "Iteration 25: Train Loss = 0.4612; Validation Loss = 0.4752\n",
      "Iteration 26: Train Loss = 0.4578; Validation Loss = 0.4722\n",
      "Iteration 27: Train Loss = 0.4547; Validation Loss = 0.4695\n",
      "Iteration 28: Train Loss = 0.4517; Validation Loss = 0.4667\n",
      "Iteration 29: Train Loss = 0.4488; Validation Loss = 0.4642\n",
      "Iteration 30: Train Loss = 0.4462; Validation Loss = 0.4618\n",
      "Iteration 31: Train Loss = 0.4436; Validation Loss = 0.4594\n",
      "Iteration 32: Train Loss = 0.4413; Validation Loss = 0.4572\n",
      "Iteration 33: Train Loss = 0.4390; Validation Loss = 0.4552\n",
      "Iteration 34: Train Loss = 0.4371; Validation Loss = 0.4535\n",
      "Iteration 35: Train Loss = 0.4350; Validation Loss = 0.4516\n",
      "Iteration 36: Train Loss = 0.4330; Validation Loss = 0.4499\n",
      "Iteration 37: Train Loss = 0.4312; Validation Loss = 0.4484\n",
      "Iteration 38: Train Loss = 0.4296; Validation Loss = 0.4470\n",
      "Iteration 39: Train Loss = 0.4281; Validation Loss = 0.4457\n",
      "Iteration 40: Train Loss = 0.4265; Validation Loss = 0.4444\n",
      "Iteration 41: Train Loss = 0.4251; Validation Loss = 0.4432\n",
      "Iteration 42: Train Loss = 0.4238; Validation Loss = 0.4420\n",
      "Iteration 43: Train Loss = 0.4225; Validation Loss = 0.4409\n",
      "Iteration 44: Train Loss = 0.4214; Validation Loss = 0.4399\n",
      "Iteration 45: Train Loss = 0.4201; Validation Loss = 0.4387\n",
      "Iteration 46: Train Loss = 0.4189; Validation Loss = 0.4378\n",
      "Iteration 47: Train Loss = 0.4177; Validation Loss = 0.4370\n",
      "Iteration 48: Train Loss = 0.4167; Validation Loss = 0.4362\n",
      "Iteration 49: Train Loss = 0.4158; Validation Loss = 0.4355\n",
      "Iteration 50: Train Loss = 0.4147; Validation Loss = 0.4348\n",
      "Iteration 51: Train Loss = 0.4140; Validation Loss = 0.4342\n",
      "Iteration 52: Train Loss = 0.4133; Validation Loss = 0.4337\n",
      "Iteration 53: Train Loss = 0.4125; Validation Loss = 0.4329\n",
      "Iteration 54: Train Loss = 0.4118; Validation Loss = 0.4322\n",
      "Iteration 55: Train Loss = 0.4110; Validation Loss = 0.4315\n",
      "Iteration 56: Train Loss = 0.4102; Validation Loss = 0.4310\n",
      "Iteration 57: Train Loss = 0.4098; Validation Loss = 0.4306\n",
      "Iteration 58: Train Loss = 0.4090; Validation Loss = 0.4299\n",
      "Iteration 59: Train Loss = 0.4085; Validation Loss = 0.4296\n",
      "Iteration 60: Train Loss = 0.4079; Validation Loss = 0.4292\n",
      "Iteration 61: Train Loss = 0.4073; Validation Loss = 0.4288\n",
      "Iteration 62: Train Loss = 0.4069; Validation Loss = 0.4285\n",
      "Iteration 63: Train Loss = 0.4065; Validation Loss = 0.4280\n",
      "Iteration 64: Train Loss = 0.4060; Validation Loss = 0.4276\n",
      "Iteration 65: Train Loss = 0.4056; Validation Loss = 0.4272\n",
      "Iteration 66: Train Loss = 0.4052; Validation Loss = 0.4269\n",
      "Iteration 67: Train Loss = 0.4047; Validation Loss = 0.4265\n",
      "Iteration 68: Train Loss = 0.4042; Validation Loss = 0.4260\n",
      "Iteration 69: Train Loss = 0.4039; Validation Loss = 0.4258\n",
      "Iteration 70: Train Loss = 0.4036; Validation Loss = 0.4255\n",
      "Iteration 71: Train Loss = 0.4032; Validation Loss = 0.4251\n",
      "Iteration 72: Train Loss = 0.4027; Validation Loss = 0.4247\n",
      "Iteration 73: Train Loss = 0.4024; Validation Loss = 0.4246\n",
      "Iteration 74: Train Loss = 0.4021; Validation Loss = 0.4243\n",
      "Iteration 75: Train Loss = 0.4019; Validation Loss = 0.4241\n",
      "Iteration 76: Train Loss = 0.4017; Validation Loss = 0.4240\n",
      "Iteration 77: Train Loss = 0.4013; Validation Loss = 0.4239\n",
      "Iteration 78: Train Loss = 0.4011; Validation Loss = 0.4238\n",
      "Iteration 79: Train Loss = 0.4009; Validation Loss = 0.4237\n",
      "Iteration 80: Train Loss = 0.4005; Validation Loss = 0.4235\n",
      "Iteration 81: Train Loss = 0.4004; Validation Loss = 0.4233\n",
      "Iteration 82: Train Loss = 0.4001; Validation Loss = 0.4231\n",
      "Iteration 83: Train Loss = 0.3999; Validation Loss = 0.4230\n",
      "Iteration 84: Train Loss = 0.3997; Validation Loss = 0.4229\n",
      "Iteration 85: Train Loss = 0.3995; Validation Loss = 0.4229\n",
      "Iteration 86: Train Loss = 0.3992; Validation Loss = 0.4227\n",
      "Iteration 87: Train Loss = 0.3990; Validation Loss = 0.4226\n",
      "Iteration 88: Train Loss = 0.3989; Validation Loss = 0.4224\n",
      "Iteration 89: Train Loss = 0.3987; Validation Loss = 0.4224\n",
      "Iteration 90: Train Loss = 0.3985; Validation Loss = 0.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:00,615] Trial 34 finished with value: 0.963308664005277 and parameters: {'max_depth': 9, 'min_samples_leaf': 5, 'n_estimators': 90, 'learning_rate': 0.057254080670232924, 'subsample': 0.6859159273223691}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6881; Validation Loss = 0.6882\n",
      "Iteration 2: Train Loss = 0.6831; Validation Loss = 0.6834\n",
      "Iteration 3: Train Loss = 0.6783; Validation Loss = 0.6787\n",
      "Iteration 4: Train Loss = 0.6735; Validation Loss = 0.6741\n",
      "Iteration 5: Train Loss = 0.6689; Validation Loss = 0.6696\n",
      "Iteration 6: Train Loss = 0.6644; Validation Loss = 0.6652\n",
      "Iteration 7: Train Loss = 0.6600; Validation Loss = 0.6609\n",
      "Iteration 8: Train Loss = 0.6556; Validation Loss = 0.6567\n",
      "Iteration 9: Train Loss = 0.6514; Validation Loss = 0.6526\n",
      "Iteration 10: Train Loss = 0.6473; Validation Loss = 0.6486\n",
      "Iteration 11: Train Loss = 0.6432; Validation Loss = 0.6446\n",
      "Iteration 12: Train Loss = 0.6392; Validation Loss = 0.6407\n",
      "Iteration 13: Train Loss = 0.6353; Validation Loss = 0.6370\n",
      "Iteration 14: Train Loss = 0.6315; Validation Loss = 0.6333\n",
      "Iteration 15: Train Loss = 0.6278; Validation Loss = 0.6296\n",
      "Iteration 16: Train Loss = 0.6242; Validation Loss = 0.6261\n",
      "Iteration 17: Train Loss = 0.6205; Validation Loss = 0.6226\n",
      "Iteration 18: Train Loss = 0.6170; Validation Loss = 0.6192\n",
      "Iteration 19: Train Loss = 0.6136; Validation Loss = 0.6159\n",
      "Iteration 20: Train Loss = 0.6102; Validation Loss = 0.6126\n",
      "Iteration 21: Train Loss = 0.6069; Validation Loss = 0.6094\n",
      "Iteration 22: Train Loss = 0.6037; Validation Loss = 0.6063\n",
      "Iteration 23: Train Loss = 0.6006; Validation Loss = 0.6033\n",
      "Iteration 24: Train Loss = 0.5975; Validation Loss = 0.6003\n",
      "Iteration 25: Train Loss = 0.5945; Validation Loss = 0.5974\n",
      "Iteration 26: Train Loss = 0.5915; Validation Loss = 0.5945\n",
      "Iteration 27: Train Loss = 0.5886; Validation Loss = 0.5916\n",
      "Iteration 28: Train Loss = 0.5857; Validation Loss = 0.5889\n",
      "Iteration 29: Train Loss = 0.5829; Validation Loss = 0.5862\n",
      "Iteration 30: Train Loss = 0.5802; Validation Loss = 0.5835\n",
      "Iteration 31: Train Loss = 0.5775; Validation Loss = 0.5809\n",
      "Iteration 32: Train Loss = 0.5748; Validation Loss = 0.5784\n",
      "Iteration 33: Train Loss = 0.5723; Validation Loss = 0.5759\n",
      "Iteration 34: Train Loss = 0.5697; Validation Loss = 0.5734\n",
      "Iteration 35: Train Loss = 0.5673; Validation Loss = 0.5711\n",
      "Iteration 36: Train Loss = 0.5648; Validation Loss = 0.5687\n",
      "Iteration 37: Train Loss = 0.5625; Validation Loss = 0.5664\n",
      "Iteration 38: Train Loss = 0.5601; Validation Loss = 0.5641\n",
      "Iteration 39: Train Loss = 0.5578; Validation Loss = 0.5619\n",
      "Iteration 40: Train Loss = 0.5555; Validation Loss = 0.5597\n",
      "Iteration 41: Train Loss = 0.5533; Validation Loss = 0.5576\n",
      "Iteration 42: Train Loss = 0.5511; Validation Loss = 0.5555\n",
      "Iteration 43: Train Loss = 0.5490; Validation Loss = 0.5535\n",
      "Iteration 44: Train Loss = 0.5469; Validation Loss = 0.5514\n",
      "Iteration 45: Train Loss = 0.5448; Validation Loss = 0.5494\n",
      "Iteration 46: Train Loss = 0.5428; Validation Loss = 0.5476\n",
      "Iteration 47: Train Loss = 0.5408; Validation Loss = 0.5457\n",
      "Iteration 48: Train Loss = 0.5389; Validation Loss = 0.5437\n",
      "Iteration 49: Train Loss = 0.5370; Validation Loss = 0.5419\n",
      "Iteration 50: Train Loss = 0.5351; Validation Loss = 0.5401\n",
      "Iteration 51: Train Loss = 0.5333; Validation Loss = 0.5383\n",
      "Iteration 52: Train Loss = 0.5314; Validation Loss = 0.5365\n",
      "Iteration 53: Train Loss = 0.5297; Validation Loss = 0.5348\n",
      "Iteration 54: Train Loss = 0.5280; Validation Loss = 0.5332\n",
      "Iteration 55: Train Loss = 0.5262; Validation Loss = 0.5315\n",
      "Iteration 56: Train Loss = 0.5246; Validation Loss = 0.5299\n",
      "Iteration 57: Train Loss = 0.5229; Validation Loss = 0.5283\n",
      "Iteration 58: Train Loss = 0.5212; Validation Loss = 0.5267\n",
      "Iteration 59: Train Loss = 0.5196; Validation Loss = 0.5252\n",
      "Iteration 60: Train Loss = 0.5181; Validation Loss = 0.5237\n",
      "Iteration 61: Train Loss = 0.5165; Validation Loss = 0.5222\n",
      "Iteration 62: Train Loss = 0.5150; Validation Loss = 0.5207\n",
      "Iteration 63: Train Loss = 0.5135; Validation Loss = 0.5193\n",
      "Iteration 64: Train Loss = 0.5121; Validation Loss = 0.5179\n",
      "Iteration 65: Train Loss = 0.5106; Validation Loss = 0.5165\n",
      "Iteration 66: Train Loss = 0.5092; Validation Loss = 0.5151\n",
      "Iteration 67: Train Loss = 0.5078; Validation Loss = 0.5138\n",
      "Iteration 68: Train Loss = 0.5065; Validation Loss = 0.5125\n",
      "Iteration 69: Train Loss = 0.5051; Validation Loss = 0.5112\n",
      "Iteration 70: Train Loss = 0.5038; Validation Loss = 0.5099\n",
      "Iteration 71: Train Loss = 0.5025; Validation Loss = 0.5088\n",
      "Iteration 72: Train Loss = 0.5012; Validation Loss = 0.5075\n",
      "Iteration 73: Train Loss = 0.5000; Validation Loss = 0.5063\n",
      "Iteration 74: Train Loss = 0.4987; Validation Loss = 0.5051\n",
      "Iteration 75: Train Loss = 0.4975; Validation Loss = 0.5040\n",
      "Iteration 76: Train Loss = 0.4963; Validation Loss = 0.5029\n",
      "Iteration 77: Train Loss = 0.4951; Validation Loss = 0.5017\n",
      "Iteration 78: Train Loss = 0.4940; Validation Loss = 0.5006\n",
      "Iteration 79: Train Loss = 0.4928; Validation Loss = 0.4995\n",
      "Iteration 80: Train Loss = 0.4917; Validation Loss = 0.4984\n",
      "Iteration 81: Train Loss = 0.4907; Validation Loss = 0.4974\n",
      "Iteration 82: Train Loss = 0.4896; Validation Loss = 0.4964\n",
      "Iteration 83: Train Loss = 0.4885; Validation Loss = 0.4953\n",
      "Iteration 84: Train Loss = 0.4875; Validation Loss = 0.4943\n",
      "Iteration 85: Train Loss = 0.4865; Validation Loss = 0.4934\n",
      "Iteration 86: Train Loss = 0.4854; Validation Loss = 0.4924\n",
      "Iteration 87: Train Loss = 0.4844; Validation Loss = 0.4915\n",
      "Iteration 88: Train Loss = 0.4835; Validation Loss = 0.4905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:13,031] Trial 35 finished with value: 0.958487659616653 and parameters: {'max_depth': 6, 'min_samples_leaf': 7, 'n_estimators': 90, 'learning_rate': 0.015189102051740622, 'subsample': 0.9545759502941098}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89: Train Loss = 0.4825; Validation Loss = 0.4896\n",
      "Iteration 90: Train Loss = 0.4815; Validation Loss = 0.4887\n",
      "Iteration 1: Train Loss = 0.6837; Validation Loss = 0.6841\n",
      "Iteration 2: Train Loss = 0.6744; Validation Loss = 0.6751\n",
      "Iteration 3: Train Loss = 0.6655; Validation Loss = 0.6664\n",
      "Iteration 4: Train Loss = 0.6571; Validation Loss = 0.6583\n",
      "Iteration 5: Train Loss = 0.6489; Validation Loss = 0.6504\n",
      "Iteration 6: Train Loss = 0.6410; Validation Loss = 0.6428\n",
      "Iteration 7: Train Loss = 0.6334; Validation Loss = 0.6355\n",
      "Iteration 8: Train Loss = 0.6262; Validation Loss = 0.6286\n",
      "Iteration 9: Train Loss = 0.6192; Validation Loss = 0.6219\n",
      "Iteration 10: Train Loss = 0.6124; Validation Loss = 0.6155\n",
      "Iteration 11: Train Loss = 0.6060; Validation Loss = 0.6094\n",
      "Iteration 12: Train Loss = 0.5998; Validation Loss = 0.6034\n",
      "Iteration 13: Train Loss = 0.5938; Validation Loss = 0.5977\n",
      "Iteration 14: Train Loss = 0.5880; Validation Loss = 0.5922\n",
      "Iteration 15: Train Loss = 0.5825; Validation Loss = 0.5869\n",
      "Iteration 16: Train Loss = 0.5771; Validation Loss = 0.5818\n",
      "Iteration 17: Train Loss = 0.5719; Validation Loss = 0.5768\n",
      "Iteration 18: Train Loss = 0.5669; Validation Loss = 0.5721\n",
      "Iteration 19: Train Loss = 0.5621; Validation Loss = 0.5675\n",
      "Iteration 20: Train Loss = 0.5574; Validation Loss = 0.5630\n",
      "Iteration 21: Train Loss = 0.5530; Validation Loss = 0.5588\n",
      "Iteration 22: Train Loss = 0.5486; Validation Loss = 0.5546\n",
      "Iteration 23: Train Loss = 0.5444; Validation Loss = 0.5506\n",
      "Iteration 24: Train Loss = 0.5404; Validation Loss = 0.5467\n",
      "Iteration 25: Train Loss = 0.5365; Validation Loss = 0.5431\n",
      "Iteration 26: Train Loss = 0.5327; Validation Loss = 0.5394\n",
      "Iteration 27: Train Loss = 0.5290; Validation Loss = 0.5360\n",
      "Iteration 28: Train Loss = 0.5254; Validation Loss = 0.5327\n",
      "Iteration 29: Train Loss = 0.5220; Validation Loss = 0.5293\n",
      "Iteration 30: Train Loss = 0.5186; Validation Loss = 0.5262\n",
      "Iteration 31: Train Loss = 0.5154; Validation Loss = 0.5232\n",
      "Iteration 32: Train Loss = 0.5123; Validation Loss = 0.5202\n",
      "Iteration 33: Train Loss = 0.5093; Validation Loss = 0.5175\n",
      "Iteration 34: Train Loss = 0.5064; Validation Loss = 0.5147\n",
      "Iteration 35: Train Loss = 0.5037; Validation Loss = 0.5121\n",
      "Iteration 36: Train Loss = 0.5010; Validation Loss = 0.5096\n",
      "Iteration 37: Train Loss = 0.4984; Validation Loss = 0.5072\n",
      "Iteration 38: Train Loss = 0.4958; Validation Loss = 0.5048\n",
      "Iteration 39: Train Loss = 0.4933; Validation Loss = 0.5026\n",
      "Iteration 40: Train Loss = 0.4909; Validation Loss = 0.5003\n",
      "Iteration 41: Train Loss = 0.4886; Validation Loss = 0.4981\n",
      "Iteration 42: Train Loss = 0.4862; Validation Loss = 0.4959\n",
      "Iteration 43: Train Loss = 0.4840; Validation Loss = 0.4938\n",
      "Iteration 44: Train Loss = 0.4818; Validation Loss = 0.4917\n",
      "Iteration 45: Train Loss = 0.4798; Validation Loss = 0.4898\n",
      "Iteration 46: Train Loss = 0.4777; Validation Loss = 0.4879\n",
      "Iteration 47: Train Loss = 0.4759; Validation Loss = 0.4861\n",
      "Iteration 48: Train Loss = 0.4740; Validation Loss = 0.4845\n",
      "Iteration 49: Train Loss = 0.4721; Validation Loss = 0.4827\n",
      "Iteration 50: Train Loss = 0.4704; Validation Loss = 0.4811\n",
      "Iteration 51: Train Loss = 0.4687; Validation Loss = 0.4796\n",
      "Iteration 52: Train Loss = 0.4669; Validation Loss = 0.4781\n",
      "Iteration 53: Train Loss = 0.4653; Validation Loss = 0.4765\n",
      "Iteration 54: Train Loss = 0.4637; Validation Loss = 0.4751\n",
      "Iteration 55: Train Loss = 0.4622; Validation Loss = 0.4736\n",
      "Iteration 56: Train Loss = 0.4607; Validation Loss = 0.4723\n",
      "Iteration 57: Train Loss = 0.4592; Validation Loss = 0.4709\n",
      "Iteration 58: Train Loss = 0.4579; Validation Loss = 0.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:20,231] Trial 36 finished with value: 0.9625033839302396 and parameters: {'max_depth': 8, 'min_samples_leaf': 6, 'n_estimators': 60, 'learning_rate': 0.02746821602942719, 'subsample': 0.656266576294254}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59: Train Loss = 0.4565; Validation Loss = 0.4685\n",
      "Iteration 60: Train Loss = 0.4552; Validation Loss = 0.4673\n",
      "Iteration 1: Train Loss = 0.6907; Validation Loss = 0.6908\n",
      "Iteration 2: Train Loss = 0.6883; Validation Loss = 0.6884\n",
      "Iteration 3: Train Loss = 0.6859; Validation Loss = 0.6861\n",
      "Iteration 4: Train Loss = 0.6835; Validation Loss = 0.6838\n",
      "Iteration 5: Train Loss = 0.6812; Validation Loss = 0.6816\n",
      "Iteration 6: Train Loss = 0.6789; Validation Loss = 0.6793\n",
      "Iteration 7: Train Loss = 0.6766; Validation Loss = 0.6771\n",
      "Iteration 8: Train Loss = 0.6744; Validation Loss = 0.6749\n",
      "Iteration 9: Train Loss = 0.6721; Validation Loss = 0.6728\n",
      "Iteration 10: Train Loss = 0.6699; Validation Loss = 0.6706\n",
      "Iteration 11: Train Loss = 0.6677; Validation Loss = 0.6685\n",
      "Iteration 12: Train Loss = 0.6656; Validation Loss = 0.6664\n",
      "Iteration 13: Train Loss = 0.6634; Validation Loss = 0.6643\n",
      "Iteration 14: Train Loss = 0.6613; Validation Loss = 0.6622\n",
      "Iteration 15: Train Loss = 0.6592; Validation Loss = 0.6602\n",
      "Iteration 16: Train Loss = 0.6571; Validation Loss = 0.6582\n",
      "Iteration 17: Train Loss = 0.6551; Validation Loss = 0.6561\n",
      "Iteration 18: Train Loss = 0.6530; Validation Loss = 0.6541\n",
      "Iteration 19: Train Loss = 0.6510; Validation Loss = 0.6522\n",
      "Iteration 20: Train Loss = 0.6490; Validation Loss = 0.6503\n",
      "Iteration 21: Train Loss = 0.6470; Validation Loss = 0.6483\n",
      "Iteration 22: Train Loss = 0.6450; Validation Loss = 0.6464\n",
      "Iteration 23: Train Loss = 0.6431; Validation Loss = 0.6446\n",
      "Iteration 24: Train Loss = 0.6412; Validation Loss = 0.6427\n",
      "Iteration 25: Train Loss = 0.6393; Validation Loss = 0.6408\n",
      "Iteration 26: Train Loss = 0.6374; Validation Loss = 0.6390\n",
      "Iteration 27: Train Loss = 0.6355; Validation Loss = 0.6372\n",
      "Iteration 28: Train Loss = 0.6336; Validation Loss = 0.6354\n",
      "Iteration 29: Train Loss = 0.6318; Validation Loss = 0.6336\n",
      "Iteration 30: Train Loss = 0.6300; Validation Loss = 0.6318\n",
      "Iteration 31: Train Loss = 0.6282; Validation Loss = 0.6301\n",
      "Iteration 32: Train Loss = 0.6264; Validation Loss = 0.6284\n",
      "Iteration 33: Train Loss = 0.6246; Validation Loss = 0.6267\n",
      "Iteration 34: Train Loss = 0.6229; Validation Loss = 0.6250\n",
      "Iteration 35: Train Loss = 0.6211; Validation Loss = 0.6233\n",
      "Iteration 36: Train Loss = 0.6194; Validation Loss = 0.6216\n",
      "Iteration 37: Train Loss = 0.6177; Validation Loss = 0.6200\n",
      "Iteration 38: Train Loss = 0.6160; Validation Loss = 0.6184\n",
      "Iteration 39: Train Loss = 0.6144; Validation Loss = 0.6168\n",
      "Iteration 40: Train Loss = 0.6127; Validation Loss = 0.6152\n",
      "Iteration 41: Train Loss = 0.6111; Validation Loss = 0.6136\n",
      "Iteration 42: Train Loss = 0.6095; Validation Loss = 0.6121\n",
      "Iteration 43: Train Loss = 0.6079; Validation Loss = 0.6105\n",
      "Iteration 44: Train Loss = 0.6063; Validation Loss = 0.6090\n",
      "Iteration 45: Train Loss = 0.6048; Validation Loss = 0.6075\n",
      "Iteration 46: Train Loss = 0.6032; Validation Loss = 0.6060\n",
      "Iteration 47: Train Loss = 0.6016; Validation Loss = 0.6045\n",
      "Iteration 48: Train Loss = 0.6001; Validation Loss = 0.6030\n",
      "Iteration 49: Train Loss = 0.5986; Validation Loss = 0.6016\n",
      "Iteration 50: Train Loss = 0.5971; Validation Loss = 0.6001\n",
      "Iteration 51: Train Loss = 0.5956; Validation Loss = 0.5987\n",
      "Iteration 52: Train Loss = 0.5942; Validation Loss = 0.5973\n",
      "Iteration 53: Train Loss = 0.5927; Validation Loss = 0.5959\n",
      "Iteration 54: Train Loss = 0.5913; Validation Loss = 0.5945\n",
      "Iteration 55: Train Loss = 0.5899; Validation Loss = 0.5932\n",
      "Iteration 56: Train Loss = 0.5885; Validation Loss = 0.5918\n",
      "Iteration 57: Train Loss = 0.5871; Validation Loss = 0.5904\n",
      "Iteration 58: Train Loss = 0.5857; Validation Loss = 0.5891\n",
      "Iteration 59: Train Loss = 0.5843; Validation Loss = 0.5878\n",
      "Iteration 60: Train Loss = 0.5829; Validation Loss = 0.5864\n",
      "Iteration 61: Train Loss = 0.5816; Validation Loss = 0.5852\n",
      "Iteration 62: Train Loss = 0.5803; Validation Loss = 0.5839\n",
      "Iteration 63: Train Loss = 0.5790; Validation Loss = 0.5827\n",
      "Iteration 64: Train Loss = 0.5776; Validation Loss = 0.5814\n",
      "Iteration 65: Train Loss = 0.5764; Validation Loss = 0.5801\n",
      "Iteration 66: Train Loss = 0.5751; Validation Loss = 0.5789\n",
      "Iteration 67: Train Loss = 0.5738; Validation Loss = 0.5776\n",
      "Iteration 68: Train Loss = 0.5725; Validation Loss = 0.5764\n",
      "Iteration 69: Train Loss = 0.5713; Validation Loss = 0.5752\n",
      "Iteration 70: Train Loss = 0.5700; Validation Loss = 0.5740\n",
      "Iteration 71: Train Loss = 0.5688; Validation Loss = 0.5729\n",
      "Iteration 72: Train Loss = 0.5676; Validation Loss = 0.5717\n",
      "Iteration 73: Train Loss = 0.5664; Validation Loss = 0.5705\n",
      "Iteration 74: Train Loss = 0.5652; Validation Loss = 0.5694\n",
      "Iteration 75: Train Loss = 0.5640; Validation Loss = 0.5682\n",
      "Iteration 76: Train Loss = 0.5628; Validation Loss = 0.5671\n",
      "Iteration 77: Train Loss = 0.5616; Validation Loss = 0.5660\n",
      "Iteration 78: Train Loss = 0.5605; Validation Loss = 0.5649\n",
      "Iteration 79: Train Loss = 0.5594; Validation Loss = 0.5638\n",
      "Iteration 80: Train Loss = 0.5582; Validation Loss = 0.5626\n",
      "Iteration 81: Train Loss = 0.5571; Validation Loss = 0.5616\n",
      "Iteration 82: Train Loss = 0.5560; Validation Loss = 0.5605\n",
      "Iteration 83: Train Loss = 0.5549; Validation Loss = 0.5595\n",
      "Iteration 84: Train Loss = 0.5538; Validation Loss = 0.5584\n",
      "Iteration 85: Train Loss = 0.5527; Validation Loss = 0.5574\n",
      "Iteration 86: Train Loss = 0.5516; Validation Loss = 0.5564\n",
      "Iteration 87: Train Loss = 0.5505; Validation Loss = 0.5553\n",
      "Iteration 88: Train Loss = 0.5495; Validation Loss = 0.5543\n",
      "Iteration 89: Train Loss = 0.5484; Validation Loss = 0.5533\n",
      "Iteration 90: Train Loss = 0.5474; Validation Loss = 0.5523\n",
      "Iteration 91: Train Loss = 0.5464; Validation Loss = 0.5513\n",
      "Iteration 92: Train Loss = 0.5453; Validation Loss = 0.5504\n",
      "Iteration 93: Train Loss = 0.5443; Validation Loss = 0.5494\n",
      "Iteration 94: Train Loss = 0.5433; Validation Loss = 0.5484\n",
      "Iteration 95: Train Loss = 0.5423; Validation Loss = 0.5475\n",
      "Iteration 96: Train Loss = 0.5413; Validation Loss = 0.5465\n",
      "Iteration 97: Train Loss = 0.5403; Validation Loss = 0.5456\n",
      "Iteration 98: Train Loss = 0.5394; Validation Loss = 0.5447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:29,418] Trial 37 finished with value: 0.9592635805222881 and parameters: {'max_depth': 7, 'min_samples_leaf': 5, 'n_estimators': 100, 'learning_rate': 0.00710851443900103, 'subsample': 0.5286712466370151}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Train Loss = 0.5384; Validation Loss = 0.5438\n",
      "Iteration 100: Train Loss = 0.5375; Validation Loss = 0.5429\n",
      "Iteration 1: Train Loss = 0.6620; Validation Loss = 0.6640\n",
      "Iteration 2: Train Loss = 0.6349; Validation Loss = 0.6383\n",
      "Iteration 3: Train Loss = 0.6108; Validation Loss = 0.6158\n",
      "Iteration 4: Train Loss = 0.5895; Validation Loss = 0.5958\n",
      "Iteration 5: Train Loss = 0.5705; Validation Loss = 0.5784\n",
      "Iteration 6: Train Loss = 0.5536; Validation Loss = 0.5630\n",
      "Iteration 7: Train Loss = 0.5384; Validation Loss = 0.5490\n",
      "Iteration 8: Train Loss = 0.5251; Validation Loss = 0.5366\n",
      "Iteration 9: Train Loss = 0.5132; Validation Loss = 0.5255\n",
      "Iteration 10: Train Loss = 0.5026; Validation Loss = 0.5161\n",
      "Iteration 11: Train Loss = 0.4928; Validation Loss = 0.5072\n",
      "Iteration 12: Train Loss = 0.4841; Validation Loss = 0.4991\n",
      "Iteration 13: Train Loss = 0.4761; Validation Loss = 0.4921\n",
      "Iteration 14: Train Loss = 0.4688; Validation Loss = 0.4856\n",
      "Iteration 15: Train Loss = 0.4623; Validation Loss = 0.4798\n",
      "Iteration 16: Train Loss = 0.4567; Validation Loss = 0.4751\n",
      "Iteration 17: Train Loss = 0.4513; Validation Loss = 0.4704\n",
      "Iteration 18: Train Loss = 0.4464; Validation Loss = 0.4663\n",
      "Iteration 19: Train Loss = 0.4420; Validation Loss = 0.4624\n",
      "Iteration 20: Train Loss = 0.4377; Validation Loss = 0.4588\n",
      "Iteration 21: Train Loss = 0.4343; Validation Loss = 0.4555\n",
      "Iteration 22: Train Loss = 0.4310; Validation Loss = 0.4527\n",
      "Iteration 23: Train Loss = 0.4277; Validation Loss = 0.4498\n",
      "Iteration 24: Train Loss = 0.4247; Validation Loss = 0.4474\n",
      "Iteration 25: Train Loss = 0.4218; Validation Loss = 0.4450\n",
      "Iteration 26: Train Loss = 0.4194; Validation Loss = 0.4428\n",
      "Iteration 27: Train Loss = 0.4172; Validation Loss = 0.4407\n",
      "Iteration 28: Train Loss = 0.4152; Validation Loss = 0.4392\n",
      "Iteration 29: Train Loss = 0.4133; Validation Loss = 0.4375\n",
      "Iteration 30: Train Loss = 0.4114; Validation Loss = 0.4359\n",
      "Iteration 31: Train Loss = 0.4101; Validation Loss = 0.4348\n",
      "Iteration 32: Train Loss = 0.4083; Validation Loss = 0.4335\n",
      "Iteration 33: Train Loss = 0.4068; Validation Loss = 0.4325\n",
      "Iteration 34: Train Loss = 0.4055; Validation Loss = 0.4314\n",
      "Iteration 35: Train Loss = 0.4042; Validation Loss = 0.4303\n",
      "Iteration 36: Train Loss = 0.4032; Validation Loss = 0.4294\n",
      "Iteration 37: Train Loss = 0.4022; Validation Loss = 0.4285\n",
      "Iteration 38: Train Loss = 0.4013; Validation Loss = 0.4278\n",
      "Iteration 39: Train Loss = 0.4004; Validation Loss = 0.4271\n",
      "Iteration 40: Train Loss = 0.3996; Validation Loss = 0.4268\n",
      "Iteration 41: Train Loss = 0.3990; Validation Loss = 0.4264\n",
      "Iteration 42: Train Loss = 0.3982; Validation Loss = 0.4257\n",
      "Iteration 43: Train Loss = 0.3974; Validation Loss = 0.4253\n",
      "Iteration 44: Train Loss = 0.3966; Validation Loss = 0.4249\n",
      "Iteration 45: Train Loss = 0.3960; Validation Loss = 0.4247\n",
      "Iteration 46: Train Loss = 0.3954; Validation Loss = 0.4243\n",
      "Iteration 47: Train Loss = 0.3948; Validation Loss = 0.4242\n",
      "Iteration 48: Train Loss = 0.3944; Validation Loss = 0.4239\n",
      "Iteration 49: Train Loss = 0.3938; Validation Loss = 0.4236\n",
      "Iteration 50: Train Loss = 0.3932; Validation Loss = 0.4232\n",
      "Iteration 51: Train Loss = 0.3929; Validation Loss = 0.4230\n",
      "Iteration 52: Train Loss = 0.3927; Validation Loss = 0.4229\n",
      "Iteration 53: Train Loss = 0.3921; Validation Loss = 0.4226\n",
      "Iteration 54: Train Loss = 0.3916; Validation Loss = 0.4226\n",
      "Iteration 55: Train Loss = 0.3913; Validation Loss = 0.4225\n",
      "Iteration 56: Train Loss = 0.3910; Validation Loss = 0.4224\n",
      "Iteration 57: Train Loss = 0.3907; Validation Loss = 0.4222\n",
      "Iteration 58: Train Loss = 0.3905; Validation Loss = 0.4219\n",
      "Iteration 59: Train Loss = 0.3899; Validation Loss = 0.4218\n",
      "Iteration 60: Train Loss = 0.3895; Validation Loss = 0.4217\n",
      "Iteration 61: Train Loss = 0.3895; Validation Loss = 0.4216\n",
      "Iteration 62: Train Loss = 0.3894; Validation Loss = 0.4217\n",
      "Iteration 63: Train Loss = 0.3892; Validation Loss = 0.4216\n",
      "Iteration 64: Train Loss = 0.3888; Validation Loss = 0.4216\n",
      "Iteration 65: Train Loss = 0.3886; Validation Loss = 0.4215\n",
      "Iteration 66: Train Loss = 0.3884; Validation Loss = 0.4214\n",
      "Iteration 67: Train Loss = 0.3880; Validation Loss = 0.4213\n",
      "Iteration 68: Train Loss = 0.3877; Validation Loss = 0.4212\n",
      "Iteration 69: Train Loss = 0.3876; Validation Loss = 0.4211\n",
      "Iteration 70: Train Loss = 0.3873; Validation Loss = 0.4209\n",
      "Iteration 71: Train Loss = 0.3871; Validation Loss = 0.4208\n",
      "Iteration 72: Train Loss = 0.3869; Validation Loss = 0.4208\n",
      "Iteration 73: Train Loss = 0.3868; Validation Loss = 0.4207\n",
      "Iteration 74: Train Loss = 0.3866; Validation Loss = 0.4208\n",
      "Iteration 75: Train Loss = 0.3864; Validation Loss = 0.4206\n",
      "Iteration 76: Train Loss = 0.3862; Validation Loss = 0.4205\n",
      "Iteration 77: Train Loss = 0.3862; Validation Loss = 0.4207\n",
      "Iteration 78: Train Loss = 0.3860; Validation Loss = 0.4206\n",
      "Iteration 79: Train Loss = 0.3858; Validation Loss = 0.4203\n",
      "Iteration 80: Train Loss = 0.3858; Validation Loss = 0.4202\n",
      "Iteration 81: Train Loss = 0.3855; Validation Loss = 0.4201\n",
      "Iteration 82: Train Loss = 0.3853; Validation Loss = 0.4200\n",
      "Iteration 83: Train Loss = 0.3853; Validation Loss = 0.4200\n",
      "Iteration 84: Train Loss = 0.3850; Validation Loss = 0.4199\n",
      "Iteration 85: Train Loss = 0.3849; Validation Loss = 0.4199\n",
      "Iteration 86: Train Loss = 0.3848; Validation Loss = 0.4199\n",
      "Iteration 87: Train Loss = 0.3845; Validation Loss = 0.4195\n",
      "Iteration 88: Train Loss = 0.3844; Validation Loss = 0.4195\n",
      "Iteration 89: Train Loss = 0.3844; Validation Loss = 0.4196\n",
      "Iteration 90: Train Loss = 0.3843; Validation Loss = 0.4195\n",
      "Iteration 91: Train Loss = 0.3841; Validation Loss = 0.4196\n",
      "Iteration 92: Train Loss = 0.3841; Validation Loss = 0.4197\n",
      "Iteration 93: Train Loss = 0.3840; Validation Loss = 0.4197\n",
      "Iteration 94: Train Loss = 0.3839; Validation Loss = 0.4199\n",
      "Iteration 95: Train Loss = 0.3837; Validation Loss = 0.4199\n",
      "Iteration 96: Train Loss = 0.3835; Validation Loss = 0.4197\n",
      "Iteration 97: Train Loss = 0.3834; Validation Loss = 0.4196\n",
      "Iteration 98: Train Loss = 0.3833; Validation Loss = 0.4197\n",
      "Early stopping at iteration 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:40,098] Trial 38 finished with value: 0.9615147043684156 and parameters: {'max_depth': 11, 'min_samples_leaf': 4, 'n_estimators': 110, 'learning_rate': 0.08627709822448278, 'subsample': 0.5748768778228004}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6921; Validation Loss = 0.6922\n",
      "Iteration 2: Train Loss = 0.6911; Validation Loss = 0.6912\n",
      "Iteration 3: Train Loss = 0.6901; Validation Loss = 0.6902\n",
      "Iteration 4: Train Loss = 0.6891; Validation Loss = 0.6892\n",
      "Iteration 5: Train Loss = 0.6881; Validation Loss = 0.6882\n",
      "Iteration 6: Train Loss = 0.6871; Validation Loss = 0.6873\n",
      "Iteration 7: Train Loss = 0.6861; Validation Loss = 0.6863\n",
      "Iteration 8: Train Loss = 0.6851; Validation Loss = 0.6853\n",
      "Iteration 9: Train Loss = 0.6842; Validation Loss = 0.6844\n",
      "Iteration 10: Train Loss = 0.6832; Validation Loss = 0.6834\n",
      "Iteration 11: Train Loss = 0.6822; Validation Loss = 0.6825\n",
      "Iteration 12: Train Loss = 0.6812; Validation Loss = 0.6815\n",
      "Iteration 13: Train Loss = 0.6803; Validation Loss = 0.6806\n",
      "Iteration 14: Train Loss = 0.6793; Validation Loss = 0.6796\n",
      "Iteration 15: Train Loss = 0.6783; Validation Loss = 0.6787\n",
      "Iteration 16: Train Loss = 0.6774; Validation Loss = 0.6778\n",
      "Iteration 17: Train Loss = 0.6764; Validation Loss = 0.6768\n",
      "Iteration 18: Train Loss = 0.6755; Validation Loss = 0.6759\n",
      "Iteration 19: Train Loss = 0.6746; Validation Loss = 0.6750\n",
      "Iteration 20: Train Loss = 0.6736; Validation Loss = 0.6741\n",
      "Iteration 21: Train Loss = 0.6727; Validation Loss = 0.6732\n",
      "Iteration 22: Train Loss = 0.6718; Validation Loss = 0.6722\n",
      "Iteration 23: Train Loss = 0.6709; Validation Loss = 0.6713\n",
      "Iteration 24: Train Loss = 0.6699; Validation Loss = 0.6704\n",
      "Iteration 25: Train Loss = 0.6690; Validation Loss = 0.6696\n",
      "Iteration 26: Train Loss = 0.6681; Validation Loss = 0.6687\n",
      "Iteration 27: Train Loss = 0.6672; Validation Loss = 0.6678\n",
      "Iteration 28: Train Loss = 0.6663; Validation Loss = 0.6669\n",
      "Iteration 29: Train Loss = 0.6654; Validation Loss = 0.6660\n",
      "Iteration 30: Train Loss = 0.6645; Validation Loss = 0.6652\n",
      "Iteration 31: Train Loss = 0.6637; Validation Loss = 0.6643\n",
      "Iteration 32: Train Loss = 0.6628; Validation Loss = 0.6634\n",
      "Iteration 33: Train Loss = 0.6619; Validation Loss = 0.6626\n",
      "Iteration 34: Train Loss = 0.6611; Validation Loss = 0.6618\n",
      "Iteration 35: Train Loss = 0.6602; Validation Loss = 0.6609\n",
      "Iteration 36: Train Loss = 0.6593; Validation Loss = 0.6601\n",
      "Iteration 37: Train Loss = 0.6585; Validation Loss = 0.6592\n",
      "Iteration 38: Train Loss = 0.6576; Validation Loss = 0.6584\n",
      "Iteration 39: Train Loss = 0.6568; Validation Loss = 0.6576\n",
      "Iteration 40: Train Loss = 0.6559; Validation Loss = 0.6567\n",
      "Iteration 41: Train Loss = 0.6551; Validation Loss = 0.6559\n",
      "Iteration 42: Train Loss = 0.6542; Validation Loss = 0.6551\n",
      "Iteration 43: Train Loss = 0.6534; Validation Loss = 0.6542\n",
      "Iteration 44: Train Loss = 0.6526; Validation Loss = 0.6534\n",
      "Iteration 45: Train Loss = 0.6517; Validation Loss = 0.6526\n",
      "Iteration 46: Train Loss = 0.6509; Validation Loss = 0.6518\n",
      "Iteration 47: Train Loss = 0.6501; Validation Loss = 0.6510\n",
      "Iteration 48: Train Loss = 0.6493; Validation Loss = 0.6502\n",
      "Iteration 49: Train Loss = 0.6484; Validation Loss = 0.6494\n",
      "Iteration 50: Train Loss = 0.6476; Validation Loss = 0.6487\n",
      "Iteration 51: Train Loss = 0.6468; Validation Loss = 0.6479\n",
      "Iteration 52: Train Loss = 0.6460; Validation Loss = 0.6471\n",
      "Iteration 53: Train Loss = 0.6452; Validation Loss = 0.6463\n",
      "Iteration 54: Train Loss = 0.6444; Validation Loss = 0.6455\n",
      "Iteration 55: Train Loss = 0.6436; Validation Loss = 0.6448\n",
      "Iteration 56: Train Loss = 0.6428; Validation Loss = 0.6440\n",
      "Iteration 57: Train Loss = 0.6421; Validation Loss = 0.6432\n",
      "Iteration 58: Train Loss = 0.6413; Validation Loss = 0.6425\n",
      "Iteration 59: Train Loss = 0.6405; Validation Loss = 0.6417\n",
      "Iteration 60: Train Loss = 0.6397; Validation Loss = 0.6410\n",
      "Iteration 61: Train Loss = 0.6390; Validation Loss = 0.6402\n",
      "Iteration 62: Train Loss = 0.6382; Validation Loss = 0.6395\n",
      "Iteration 63: Train Loss = 0.6374; Validation Loss = 0.6387\n",
      "Iteration 64: Train Loss = 0.6367; Validation Loss = 0.6380\n",
      "Iteration 65: Train Loss = 0.6359; Validation Loss = 0.6372\n",
      "Iteration 66: Train Loss = 0.6352; Validation Loss = 0.6365\n",
      "Iteration 67: Train Loss = 0.6344; Validation Loss = 0.6358\n",
      "Iteration 68: Train Loss = 0.6337; Validation Loss = 0.6350\n",
      "Iteration 69: Train Loss = 0.6329; Validation Loss = 0.6343\n",
      "Iteration 70: Train Loss = 0.6322; Validation Loss = 0.6336\n",
      "Iteration 71: Train Loss = 0.6315; Validation Loss = 0.6329\n",
      "Iteration 72: Train Loss = 0.6307; Validation Loss = 0.6321\n",
      "Iteration 73: Train Loss = 0.6300; Validation Loss = 0.6314\n",
      "Iteration 74: Train Loss = 0.6293; Validation Loss = 0.6307\n",
      "Iteration 75: Train Loss = 0.6286; Validation Loss = 0.6300\n",
      "Iteration 76: Train Loss = 0.6278; Validation Loss = 0.6293\n",
      "Iteration 77: Train Loss = 0.6271; Validation Loss = 0.6286\n",
      "Iteration 78: Train Loss = 0.6264; Validation Loss = 0.6279\n",
      "Iteration 79: Train Loss = 0.6257; Validation Loss = 0.6272\n",
      "Iteration 80: Train Loss = 0.6250; Validation Loss = 0.6265\n",
      "Iteration 81: Train Loss = 0.6243; Validation Loss = 0.6258\n",
      "Iteration 82: Train Loss = 0.6236; Validation Loss = 0.6252\n",
      "Iteration 83: Train Loss = 0.6229; Validation Loss = 0.6245\n",
      "Iteration 84: Train Loss = 0.6222; Validation Loss = 0.6238\n",
      "Iteration 85: Train Loss = 0.6215; Validation Loss = 0.6231\n",
      "Iteration 86: Train Loss = 0.6208; Validation Loss = 0.6225\n",
      "Iteration 87: Train Loss = 0.6201; Validation Loss = 0.6218\n",
      "Iteration 88: Train Loss = 0.6195; Validation Loss = 0.6211\n",
      "Iteration 89: Train Loss = 0.6188; Validation Loss = 0.6205\n",
      "Iteration 90: Train Loss = 0.6181; Validation Loss = 0.6198\n",
      "Iteration 91: Train Loss = 0.6174; Validation Loss = 0.6192\n",
      "Iteration 92: Train Loss = 0.6168; Validation Loss = 0.6185\n",
      "Iteration 93: Train Loss = 0.6161; Validation Loss = 0.6179\n",
      "Iteration 94: Train Loss = 0.6154; Validation Loss = 0.6172\n",
      "Iteration 95: Train Loss = 0.6148; Validation Loss = 0.6166\n",
      "Iteration 96: Train Loss = 0.6141; Validation Loss = 0.6159\n",
      "Iteration 97: Train Loss = 0.6135; Validation Loss = 0.6153\n",
      "Iteration 98: Train Loss = 0.6128; Validation Loss = 0.6146\n",
      "Iteration 99: Train Loss = 0.6122; Validation Loss = 0.6140\n",
      "Iteration 100: Train Loss = 0.6115; Validation Loss = 0.6134\n",
      "Iteration 101: Train Loss = 0.6109; Validation Loss = 0.6127\n",
      "Iteration 102: Train Loss = 0.6102; Validation Loss = 0.6121\n",
      "Iteration 103: Train Loss = 0.6096; Validation Loss = 0.6115\n",
      "Iteration 104: Train Loss = 0.6089; Validation Loss = 0.6108\n",
      "Iteration 105: Train Loss = 0.6083; Validation Loss = 0.6102\n",
      "Iteration 106: Train Loss = 0.6077; Validation Loss = 0.6096\n",
      "Iteration 107: Train Loss = 0.6070; Validation Loss = 0.6090\n",
      "Iteration 108: Train Loss = 0.6064; Validation Loss = 0.6084\n",
      "Iteration 109: Train Loss = 0.6058; Validation Loss = 0.6078\n",
      "Iteration 110: Train Loss = 0.6052; Validation Loss = 0.6072\n",
      "Iteration 111: Train Loss = 0.6046; Validation Loss = 0.6066\n",
      "Iteration 112: Train Loss = 0.6039; Validation Loss = 0.6060\n",
      "Iteration 113: Train Loss = 0.6033; Validation Loss = 0.6054\n",
      "Iteration 114: Train Loss = 0.6027; Validation Loss = 0.6048\n",
      "Iteration 115: Train Loss = 0.6021; Validation Loss = 0.6042\n",
      "Iteration 116: Train Loss = 0.6015; Validation Loss = 0.6036\n",
      "Iteration 117: Train Loss = 0.6009; Validation Loss = 0.6030\n",
      "Iteration 118: Train Loss = 0.6003; Validation Loss = 0.6024\n",
      "Iteration 119: Train Loss = 0.5997; Validation Loss = 0.6018\n",
      "Iteration 120: Train Loss = 0.5991; Validation Loss = 0.6013\n",
      "Iteration 121: Train Loss = 0.5985; Validation Loss = 0.6007\n",
      "Iteration 122: Train Loss = 0.5979; Validation Loss = 0.6001\n",
      "Iteration 123: Train Loss = 0.5974; Validation Loss = 0.5995\n",
      "Iteration 124: Train Loss = 0.5968; Validation Loss = 0.5990\n",
      "Iteration 125: Train Loss = 0.5962; Validation Loss = 0.5984\n",
      "Iteration 126: Train Loss = 0.5956; Validation Loss = 0.5978\n",
      "Iteration 127: Train Loss = 0.5950; Validation Loss = 0.5973\n",
      "Iteration 128: Train Loss = 0.5945; Validation Loss = 0.5967\n",
      "Iteration 129: Train Loss = 0.5939; Validation Loss = 0.5961\n",
      "Iteration 130: Train Loss = 0.5933; Validation Loss = 0.5956\n",
      "Iteration 131: Train Loss = 0.5928; Validation Loss = 0.5950\n",
      "Iteration 132: Train Loss = 0.5922; Validation Loss = 0.5945\n",
      "Iteration 133: Train Loss = 0.5916; Validation Loss = 0.5939\n",
      "Iteration 134: Train Loss = 0.5911; Validation Loss = 0.5934\n",
      "Iteration 135: Train Loss = 0.5905; Validation Loss = 0.5928\n",
      "Iteration 136: Train Loss = 0.5900; Validation Loss = 0.5923\n",
      "Iteration 137: Train Loss = 0.5894; Validation Loss = 0.5918\n",
      "Iteration 138: Train Loss = 0.5889; Validation Loss = 0.5912\n",
      "Iteration 139: Train Loss = 0.5883; Validation Loss = 0.5907\n",
      "Iteration 140: Train Loss = 0.5878; Validation Loss = 0.5902\n",
      "Iteration 141: Train Loss = 0.5873; Validation Loss = 0.5896\n",
      "Iteration 142: Train Loss = 0.5867; Validation Loss = 0.5891\n",
      "Iteration 143: Train Loss = 0.5862; Validation Loss = 0.5886\n",
      "Iteration 144: Train Loss = 0.5856; Validation Loss = 0.5880\n",
      "Iteration 145: Train Loss = 0.5851; Validation Loss = 0.5875\n",
      "Iteration 146: Train Loss = 0.5846; Validation Loss = 0.5870\n",
      "Iteration 147: Train Loss = 0.5841; Validation Loss = 0.5865\n",
      "Iteration 148: Train Loss = 0.5835; Validation Loss = 0.5860\n",
      "Iteration 149: Train Loss = 0.5830; Validation Loss = 0.5855\n",
      "Iteration 150: Train Loss = 0.5825; Validation Loss = 0.5850\n",
      "Iteration 151: Train Loss = 0.5820; Validation Loss = 0.5845\n",
      "Iteration 152: Train Loss = 0.5814; Validation Loss = 0.5839\n",
      "Iteration 153: Train Loss = 0.5809; Validation Loss = 0.5834\n",
      "Iteration 154: Train Loss = 0.5804; Validation Loss = 0.5829\n",
      "Iteration 155: Train Loss = 0.5799; Validation Loss = 0.5824\n",
      "Iteration 156: Train Loss = 0.5794; Validation Loss = 0.5820\n",
      "Iteration 157: Train Loss = 0.5789; Validation Loss = 0.5815\n",
      "Iteration 158: Train Loss = 0.5784; Validation Loss = 0.5810\n",
      "Iteration 159: Train Loss = 0.5779; Validation Loss = 0.5805\n",
      "Iteration 160: Train Loss = 0.5774; Validation Loss = 0.5800\n",
      "Iteration 161: Train Loss = 0.5769; Validation Loss = 0.5795\n",
      "Iteration 162: Train Loss = 0.5764; Validation Loss = 0.5790\n",
      "Iteration 163: Train Loss = 0.5759; Validation Loss = 0.5785\n",
      "Iteration 164: Train Loss = 0.5754; Validation Loss = 0.5780\n",
      "Iteration 165: Train Loss = 0.5749; Validation Loss = 0.5776\n",
      "Iteration 166: Train Loss = 0.5744; Validation Loss = 0.5771\n",
      "Iteration 167: Train Loss = 0.5739; Validation Loss = 0.5766\n",
      "Iteration 168: Train Loss = 0.5734; Validation Loss = 0.5761\n",
      "Iteration 169: Train Loss = 0.5730; Validation Loss = 0.5756\n",
      "Iteration 170: Train Loss = 0.5725; Validation Loss = 0.5752\n",
      "Iteration 171: Train Loss = 0.5720; Validation Loss = 0.5747\n",
      "Iteration 172: Train Loss = 0.5715; Validation Loss = 0.5742\n",
      "Iteration 173: Train Loss = 0.5710; Validation Loss = 0.5738\n",
      "Iteration 174: Train Loss = 0.5706; Validation Loss = 0.5733\n",
      "Iteration 175: Train Loss = 0.5701; Validation Loss = 0.5729\n",
      "Iteration 176: Train Loss = 0.5696; Validation Loss = 0.5724\n",
      "Iteration 177: Train Loss = 0.5692; Validation Loss = 0.5719\n",
      "Iteration 178: Train Loss = 0.5687; Validation Loss = 0.5715\n",
      "Iteration 179: Train Loss = 0.5682; Validation Loss = 0.5710\n",
      "Iteration 180: Train Loss = 0.5678; Validation Loss = 0.5706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:37:54,888] Trial 39 finished with value: 0.9465682181271899 and parameters: {'max_depth': 4, 'min_samples_leaf': 8, 'n_estimators': 180, 'learning_rate': 0.0033278112187615752, 'subsample': 0.6178658613957752}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.5983; Validation Loss = 0.6052\n",
      "Iteration 2: Train Loss = 0.5362; Validation Loss = 0.5487\n",
      "Iteration 3: Train Loss = 0.4944; Validation Loss = 0.5117\n",
      "Iteration 4: Train Loss = 0.4646; Validation Loss = 0.4851\n",
      "Iteration 5: Train Loss = 0.4434; Validation Loss = 0.4669\n",
      "Iteration 6: Train Loss = 0.4285; Validation Loss = 0.4544\n",
      "Iteration 7: Train Loss = 0.4174; Validation Loss = 0.4450\n",
      "Iteration 8: Train Loss = 0.4093; Validation Loss = 0.4391\n",
      "Iteration 9: Train Loss = 0.4020; Validation Loss = 0.4340\n",
      "Iteration 10: Train Loss = 0.3976; Validation Loss = 0.4304\n",
      "Iteration 11: Train Loss = 0.3931; Validation Loss = 0.4275\n",
      "Iteration 12: Train Loss = 0.3904; Validation Loss = 0.4255\n",
      "Iteration 13: Train Loss = 0.3878; Validation Loss = 0.4236\n",
      "Iteration 14: Train Loss = 0.3863; Validation Loss = 0.4233\n",
      "Iteration 15: Train Loss = 0.3849; Validation Loss = 0.4225\n",
      "Iteration 16: Train Loss = 0.3830; Validation Loss = 0.4209\n",
      "Iteration 17: Train Loss = 0.3815; Validation Loss = 0.4206\n",
      "Iteration 18: Train Loss = 0.3808; Validation Loss = 0.4206\n",
      "Iteration 19: Train Loss = 0.3797; Validation Loss = 0.4200\n",
      "Iteration 20: Train Loss = 0.3791; Validation Loss = 0.4197\n",
      "Iteration 21: Train Loss = 0.3787; Validation Loss = 0.4203\n",
      "Iteration 22: Train Loss = 0.3783; Validation Loss = 0.4204\n",
      "Iteration 23: Train Loss = 0.3778; Validation Loss = 0.4201\n",
      "Iteration 24: Train Loss = 0.3769; Validation Loss = 0.4198\n",
      "Iteration 25: Train Loss = 0.3763; Validation Loss = 0.4196\n",
      "Iteration 26: Train Loss = 0.3757; Validation Loss = 0.4194\n",
      "Iteration 27: Train Loss = 0.3757; Validation Loss = 0.4194\n",
      "Iteration 28: Train Loss = 0.3749; Validation Loss = 0.4192\n",
      "Iteration 29: Train Loss = 0.3740; Validation Loss = 0.4190\n",
      "Iteration 30: Train Loss = 0.3736; Validation Loss = 0.4187\n",
      "Iteration 31: Train Loss = 0.3732; Validation Loss = 0.4184\n",
      "Iteration 32: Train Loss = 0.3726; Validation Loss = 0.4182\n",
      "Iteration 33: Train Loss = 0.3721; Validation Loss = 0.4184\n",
      "Iteration 34: Train Loss = 0.3716; Validation Loss = 0.4180\n",
      "Iteration 35: Train Loss = 0.3709; Validation Loss = 0.4177\n",
      "Iteration 36: Train Loss = 0.3703; Validation Loss = 0.4174\n",
      "Iteration 37: Train Loss = 0.3699; Validation Loss = 0.4177\n",
      "Iteration 38: Train Loss = 0.3693; Validation Loss = 0.4177\n",
      "Iteration 39: Train Loss = 0.3690; Validation Loss = 0.4174\n",
      "Iteration 40: Train Loss = 0.3688; Validation Loss = 0.4181\n",
      "Iteration 41: Train Loss = 0.3683; Validation Loss = 0.4180\n",
      "Iteration 42: Train Loss = 0.3675; Validation Loss = 0.4182\n",
      "Iteration 43: Train Loss = 0.3672; Validation Loss = 0.4184\n",
      "Iteration 44: Train Loss = 0.3666; Validation Loss = 0.4184\n",
      "Iteration 45: Train Loss = 0.3664; Validation Loss = 0.4182\n",
      "Iteration 46: Train Loss = 0.3659; Validation Loss = 0.4183\n",
      "Iteration 47: Train Loss = 0.3652; Validation Loss = 0.4186\n",
      "Iteration 48: Train Loss = 0.3650; Validation Loss = 0.4193\n",
      "Iteration 49: Train Loss = 0.3649; Validation Loss = 0.4198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:02,613] Trial 40 finished with value: 0.9472511047817317 and parameters: {'max_depth': 12, 'min_samples_leaf': 3, 'n_estimators': 70, 'learning_rate': 0.2675237834715763, 'subsample': 0.83650935141095}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: Train Loss = 0.3645; Validation Loss = 0.4204\n",
      "Early stopping at iteration 49\n",
      "Iteration 1: Train Loss = 0.6762; Validation Loss = 0.6767\n",
      "Iteration 2: Train Loss = 0.6604; Validation Loss = 0.6618\n",
      "Iteration 3: Train Loss = 0.6457; Validation Loss = 0.6479\n",
      "Iteration 4: Train Loss = 0.6322; Validation Loss = 0.6349\n",
      "Iteration 5: Train Loss = 0.6194; Validation Loss = 0.6226\n",
      "Iteration 6: Train Loss = 0.6076; Validation Loss = 0.6112\n",
      "Iteration 7: Train Loss = 0.5964; Validation Loss = 0.6006\n",
      "Iteration 8: Train Loss = 0.5860; Validation Loss = 0.5905\n",
      "Iteration 9: Train Loss = 0.5763; Validation Loss = 0.5813\n",
      "Iteration 10: Train Loss = 0.5673; Validation Loss = 0.5727\n",
      "Iteration 11: Train Loss = 0.5588; Validation Loss = 0.5645\n",
      "Iteration 12: Train Loss = 0.5508; Validation Loss = 0.5569\n",
      "Iteration 13: Train Loss = 0.5432; Validation Loss = 0.5496\n",
      "Iteration 14: Train Loss = 0.5362; Validation Loss = 0.5430\n",
      "Iteration 15: Train Loss = 0.5296; Validation Loss = 0.5369\n",
      "Iteration 16: Train Loss = 0.5234; Validation Loss = 0.5311\n",
      "Iteration 17: Train Loss = 0.5174; Validation Loss = 0.5255\n",
      "Iteration 18: Train Loss = 0.5118; Validation Loss = 0.5202\n",
      "Iteration 19: Train Loss = 0.5066; Validation Loss = 0.5153\n",
      "Iteration 20: Train Loss = 0.5016; Validation Loss = 0.5105\n",
      "Iteration 21: Train Loss = 0.4970; Validation Loss = 0.5062\n",
      "Iteration 22: Train Loss = 0.4925; Validation Loss = 0.5020\n",
      "Iteration 23: Train Loss = 0.4884; Validation Loss = 0.4982\n",
      "Iteration 24: Train Loss = 0.4845; Validation Loss = 0.4946\n",
      "Iteration 25: Train Loss = 0.4807; Validation Loss = 0.4911\n",
      "Iteration 26: Train Loss = 0.4772; Validation Loss = 0.4878\n",
      "Iteration 27: Train Loss = 0.4738; Validation Loss = 0.4845\n",
      "Iteration 28: Train Loss = 0.4707; Validation Loss = 0.4817\n",
      "Iteration 29: Train Loss = 0.4679; Validation Loss = 0.4791\n",
      "Iteration 30: Train Loss = 0.4650; Validation Loss = 0.4763\n",
      "Iteration 31: Train Loss = 0.4622; Validation Loss = 0.4738\n",
      "Iteration 32: Train Loss = 0.4597; Validation Loss = 0.4713\n",
      "Iteration 33: Train Loss = 0.4571; Validation Loss = 0.4690\n",
      "Iteration 34: Train Loss = 0.4547; Validation Loss = 0.4667\n",
      "Iteration 35: Train Loss = 0.4526; Validation Loss = 0.4647\n",
      "Iteration 36: Train Loss = 0.4504; Validation Loss = 0.4628\n",
      "Iteration 37: Train Loss = 0.4483; Validation Loss = 0.4608\n",
      "Iteration 38: Train Loss = 0.4465; Validation Loss = 0.4591\n",
      "Iteration 39: Train Loss = 0.4445; Validation Loss = 0.4572\n",
      "Iteration 40: Train Loss = 0.4426; Validation Loss = 0.4555\n",
      "Iteration 41: Train Loss = 0.4409; Validation Loss = 0.4538\n",
      "Iteration 42: Train Loss = 0.4394; Validation Loss = 0.4524\n",
      "Iteration 43: Train Loss = 0.4378; Validation Loss = 0.4511\n",
      "Iteration 44: Train Loss = 0.4364; Validation Loss = 0.4497\n",
      "Iteration 45: Train Loss = 0.4351; Validation Loss = 0.4486\n",
      "Iteration 46: Train Loss = 0.4338; Validation Loss = 0.4474\n",
      "Iteration 47: Train Loss = 0.4325; Validation Loss = 0.4462\n",
      "Iteration 48: Train Loss = 0.4314; Validation Loss = 0.4452\n",
      "Iteration 49: Train Loss = 0.4303; Validation Loss = 0.4442\n",
      "Iteration 50: Train Loss = 0.4290; Validation Loss = 0.4431\n",
      "Iteration 51: Train Loss = 0.4279; Validation Loss = 0.4423\n",
      "Iteration 52: Train Loss = 0.4270; Validation Loss = 0.4416\n",
      "Iteration 53: Train Loss = 0.4260; Validation Loss = 0.4406\n",
      "Iteration 54: Train Loss = 0.4251; Validation Loss = 0.4397\n",
      "Iteration 55: Train Loss = 0.4242; Validation Loss = 0.4388\n",
      "Iteration 56: Train Loss = 0.4234; Validation Loss = 0.4382\n",
      "Iteration 57: Train Loss = 0.4226; Validation Loss = 0.4375\n",
      "Iteration 58: Train Loss = 0.4218; Validation Loss = 0.4368\n",
      "Iteration 59: Train Loss = 0.4210; Validation Loss = 0.4361\n",
      "Iteration 60: Train Loss = 0.4204; Validation Loss = 0.4356\n",
      "Iteration 61: Train Loss = 0.4195; Validation Loss = 0.4348\n",
      "Iteration 62: Train Loss = 0.4188; Validation Loss = 0.4343\n",
      "Iteration 63: Train Loss = 0.4182; Validation Loss = 0.4338\n",
      "Iteration 64: Train Loss = 0.4175; Validation Loss = 0.4332\n",
      "Iteration 65: Train Loss = 0.4168; Validation Loss = 0.4326\n",
      "Iteration 66: Train Loss = 0.4162; Validation Loss = 0.4322\n",
      "Iteration 67: Train Loss = 0.4155; Validation Loss = 0.4316\n",
      "Iteration 68: Train Loss = 0.4150; Validation Loss = 0.4312\n",
      "Iteration 69: Train Loss = 0.4145; Validation Loss = 0.4308\n",
      "Iteration 70: Train Loss = 0.4139; Validation Loss = 0.4304\n",
      "Iteration 71: Train Loss = 0.4134; Validation Loss = 0.4300\n",
      "Iteration 72: Train Loss = 0.4130; Validation Loss = 0.4296\n",
      "Iteration 73: Train Loss = 0.4125; Validation Loss = 0.4292\n",
      "Iteration 74: Train Loss = 0.4121; Validation Loss = 0.4289\n",
      "Iteration 75: Train Loss = 0.4117; Validation Loss = 0.4286\n",
      "Iteration 76: Train Loss = 0.4114; Validation Loss = 0.4284\n",
      "Iteration 77: Train Loss = 0.4110; Validation Loss = 0.4281\n",
      "Iteration 78: Train Loss = 0.4108; Validation Loss = 0.4279\n",
      "Iteration 79: Train Loss = 0.4106; Validation Loss = 0.4278\n",
      "Iteration 80: Train Loss = 0.4104; Validation Loss = 0.4277\n",
      "Iteration 81: Train Loss = 0.4102; Validation Loss = 0.4274\n",
      "Iteration 82: Train Loss = 0.4099; Validation Loss = 0.4272\n",
      "Iteration 83: Train Loss = 0.4095; Validation Loss = 0.4269\n",
      "Iteration 84: Train Loss = 0.4091; Validation Loss = 0.4267\n",
      "Iteration 85: Train Loss = 0.4089; Validation Loss = 0.4265\n",
      "Iteration 86: Train Loss = 0.4085; Validation Loss = 0.4263\n",
      "Iteration 87: Train Loss = 0.4083; Validation Loss = 0.4261\n",
      "Iteration 88: Train Loss = 0.4080; Validation Loss = 0.4258\n",
      "Iteration 89: Train Loss = 0.4077; Validation Loss = 0.4256\n",
      "Iteration 90: Train Loss = 0.4074; Validation Loss = 0.4253\n",
      "Iteration 91: Train Loss = 0.4073; Validation Loss = 0.4253\n",
      "Iteration 92: Train Loss = 0.4072; Validation Loss = 0.4251\n",
      "Iteration 93: Train Loss = 0.4068; Validation Loss = 0.4248\n",
      "Iteration 94: Train Loss = 0.4065; Validation Loss = 0.4246\n",
      "Iteration 95: Train Loss = 0.4063; Validation Loss = 0.4245\n",
      "Iteration 96: Train Loss = 0.4060; Validation Loss = 0.4243\n",
      "Iteration 97: Train Loss = 0.4058; Validation Loss = 0.4242\n",
      "Iteration 98: Train Loss = 0.4056; Validation Loss = 0.4241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:12,294] Trial 41 finished with value: 0.9647640399742249 and parameters: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 100, 'learning_rate': 0.04862432712397138, 'subsample': 0.589126773864923}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Train Loss = 0.4053; Validation Loss = 0.4238\n",
      "Iteration 100: Train Loss = 0.4051; Validation Loss = 0.4236\n",
      "Iteration 1: Train Loss = 0.6720; Validation Loss = 0.6731\n",
      "Iteration 2: Train Loss = 0.6528; Validation Loss = 0.6548\n",
      "Iteration 3: Train Loss = 0.6353; Validation Loss = 0.6380\n",
      "Iteration 4: Train Loss = 0.6191; Validation Loss = 0.6225\n",
      "Iteration 5: Train Loss = 0.6042; Validation Loss = 0.6085\n",
      "Iteration 6: Train Loss = 0.5905; Validation Loss = 0.5953\n",
      "Iteration 7: Train Loss = 0.5781; Validation Loss = 0.5834\n",
      "Iteration 8: Train Loss = 0.5665; Validation Loss = 0.5726\n",
      "Iteration 9: Train Loss = 0.5559; Validation Loss = 0.5626\n",
      "Iteration 10: Train Loss = 0.5461; Validation Loss = 0.5532\n",
      "Iteration 11: Train Loss = 0.5369; Validation Loss = 0.5445\n",
      "Iteration 12: Train Loss = 0.5282; Validation Loss = 0.5363\n",
      "Iteration 13: Train Loss = 0.5204; Validation Loss = 0.5290\n",
      "Iteration 14: Train Loss = 0.5132; Validation Loss = 0.5222\n",
      "Iteration 15: Train Loss = 0.5062; Validation Loss = 0.5156\n",
      "Iteration 16: Train Loss = 0.4998; Validation Loss = 0.5097\n",
      "Iteration 17: Train Loss = 0.4939; Validation Loss = 0.5042\n",
      "Iteration 18: Train Loss = 0.4884; Validation Loss = 0.4991\n",
      "Iteration 19: Train Loss = 0.4830; Validation Loss = 0.4942\n",
      "Iteration 20: Train Loss = 0.4783; Validation Loss = 0.4896\n",
      "Iteration 21: Train Loss = 0.4737; Validation Loss = 0.4854\n",
      "Iteration 22: Train Loss = 0.4694; Validation Loss = 0.4816\n",
      "Iteration 23: Train Loss = 0.4654; Validation Loss = 0.4779\n",
      "Iteration 24: Train Loss = 0.4615; Validation Loss = 0.4744\n",
      "Iteration 25: Train Loss = 0.4581; Validation Loss = 0.4714\n",
      "Iteration 26: Train Loss = 0.4547; Validation Loss = 0.4684\n",
      "Iteration 27: Train Loss = 0.4514; Validation Loss = 0.4654\n",
      "Iteration 28: Train Loss = 0.4486; Validation Loss = 0.4630\n",
      "Iteration 29: Train Loss = 0.4459; Validation Loss = 0.4605\n",
      "Iteration 30: Train Loss = 0.4434; Validation Loss = 0.4584\n",
      "Iteration 31: Train Loss = 0.4410; Validation Loss = 0.4563\n",
      "Iteration 32: Train Loss = 0.4387; Validation Loss = 0.4543\n",
      "Iteration 33: Train Loss = 0.4365; Validation Loss = 0.4525\n",
      "Iteration 34: Train Loss = 0.4345; Validation Loss = 0.4509\n",
      "Iteration 35: Train Loss = 0.4327; Validation Loss = 0.4491\n",
      "Iteration 36: Train Loss = 0.4309; Validation Loss = 0.4476\n",
      "Iteration 37: Train Loss = 0.4292; Validation Loss = 0.4462\n",
      "Iteration 38: Train Loss = 0.4277; Validation Loss = 0.4449\n",
      "Iteration 39: Train Loss = 0.4261; Validation Loss = 0.4436\n",
      "Iteration 40: Train Loss = 0.4247; Validation Loss = 0.4423\n",
      "Iteration 41: Train Loss = 0.4234; Validation Loss = 0.4414\n",
      "Iteration 42: Train Loss = 0.4222; Validation Loss = 0.4404\n",
      "Iteration 43: Train Loss = 0.4210; Validation Loss = 0.4395\n",
      "Iteration 44: Train Loss = 0.4199; Validation Loss = 0.4385\n",
      "Iteration 45: Train Loss = 0.4187; Validation Loss = 0.4375\n",
      "Iteration 46: Train Loss = 0.4176; Validation Loss = 0.4367\n",
      "Iteration 47: Train Loss = 0.4165; Validation Loss = 0.4358\n",
      "Iteration 48: Train Loss = 0.4156; Validation Loss = 0.4351\n",
      "Iteration 49: Train Loss = 0.4146; Validation Loss = 0.4342\n",
      "Iteration 50: Train Loss = 0.4139; Validation Loss = 0.4336\n",
      "Iteration 51: Train Loss = 0.4131; Validation Loss = 0.4330\n",
      "Iteration 52: Train Loss = 0.4122; Validation Loss = 0.4323\n",
      "Iteration 53: Train Loss = 0.4115; Validation Loss = 0.4317\n",
      "Iteration 54: Train Loss = 0.4109; Validation Loss = 0.4311\n",
      "Iteration 55: Train Loss = 0.4101; Validation Loss = 0.4304\n",
      "Iteration 56: Train Loss = 0.4095; Validation Loss = 0.4300\n",
      "Iteration 57: Train Loss = 0.4088; Validation Loss = 0.4294\n",
      "Iteration 58: Train Loss = 0.4082; Validation Loss = 0.4289\n",
      "Iteration 59: Train Loss = 0.4078; Validation Loss = 0.4287\n",
      "Iteration 60: Train Loss = 0.4074; Validation Loss = 0.4284\n",
      "Iteration 61: Train Loss = 0.4069; Validation Loss = 0.4281\n",
      "Iteration 62: Train Loss = 0.4065; Validation Loss = 0.4277\n",
      "Iteration 63: Train Loss = 0.4062; Validation Loss = 0.4275\n",
      "Iteration 64: Train Loss = 0.4058; Validation Loss = 0.4273\n",
      "Iteration 65: Train Loss = 0.4055; Validation Loss = 0.4270\n",
      "Iteration 66: Train Loss = 0.4051; Validation Loss = 0.4268\n",
      "Iteration 67: Train Loss = 0.4047; Validation Loss = 0.4264\n",
      "Iteration 68: Train Loss = 0.4043; Validation Loss = 0.4262\n",
      "Iteration 69: Train Loss = 0.4039; Validation Loss = 0.4260\n",
      "Iteration 70: Train Loss = 0.4037; Validation Loss = 0.4258\n",
      "Iteration 71: Train Loss = 0.4033; Validation Loss = 0.4255\n",
      "Iteration 72: Train Loss = 0.4028; Validation Loss = 0.4252\n",
      "Iteration 73: Train Loss = 0.4025; Validation Loss = 0.4250\n",
      "Iteration 74: Train Loss = 0.4023; Validation Loss = 0.4249\n",
      "Iteration 75: Train Loss = 0.4020; Validation Loss = 0.4248\n",
      "Iteration 76: Train Loss = 0.4017; Validation Loss = 0.4245\n",
      "Iteration 77: Train Loss = 0.4014; Validation Loss = 0.4243\n",
      "Iteration 78: Train Loss = 0.4012; Validation Loss = 0.4241\n",
      "Iteration 79: Train Loss = 0.4010; Validation Loss = 0.4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:21,042] Trial 42 finished with value: 0.9628919163149287 and parameters: {'max_depth': 9, 'min_samples_leaf': 5, 'n_estimators': 80, 'learning_rate': 0.05969440095467086, 'subsample': 0.6334734820817073}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80: Train Loss = 0.4007; Validation Loss = 0.4237\n",
      "Iteration 1: Train Loss = 0.6804; Validation Loss = 0.6808\n",
      "Iteration 2: Train Loss = 0.6683; Validation Loss = 0.6691\n",
      "Iteration 3: Train Loss = 0.6569; Validation Loss = 0.6581\n",
      "Iteration 4: Train Loss = 0.6461; Validation Loss = 0.6476\n",
      "Iteration 5: Train Loss = 0.6356; Validation Loss = 0.6376\n",
      "Iteration 6: Train Loss = 0.6260; Validation Loss = 0.6282\n",
      "Iteration 7: Train Loss = 0.6167; Validation Loss = 0.6192\n",
      "Iteration 8: Train Loss = 0.6079; Validation Loss = 0.6108\n",
      "Iteration 9: Train Loss = 0.5995; Validation Loss = 0.6029\n",
      "Iteration 10: Train Loss = 0.5915; Validation Loss = 0.5952\n",
      "Iteration 11: Train Loss = 0.5840; Validation Loss = 0.5879\n",
      "Iteration 12: Train Loss = 0.5768; Validation Loss = 0.5809\n",
      "Iteration 13: Train Loss = 0.5699; Validation Loss = 0.5742\n",
      "Iteration 14: Train Loss = 0.5634; Validation Loss = 0.5679\n",
      "Iteration 15: Train Loss = 0.5573; Validation Loss = 0.5621\n",
      "Iteration 16: Train Loss = 0.5514; Validation Loss = 0.5564\n",
      "Iteration 17: Train Loss = 0.5459; Validation Loss = 0.5510\n",
      "Iteration 18: Train Loss = 0.5405; Validation Loss = 0.5458\n",
      "Iteration 19: Train Loss = 0.5354; Validation Loss = 0.5409\n",
      "Iteration 20: Train Loss = 0.5303; Validation Loss = 0.5360\n",
      "Iteration 21: Train Loss = 0.5256; Validation Loss = 0.5315\n",
      "Iteration 22: Train Loss = 0.5212; Validation Loss = 0.5273\n",
      "Iteration 23: Train Loss = 0.5168; Validation Loss = 0.5232\n",
      "Iteration 24: Train Loss = 0.5127; Validation Loss = 0.5193\n",
      "Iteration 25: Train Loss = 0.5088; Validation Loss = 0.5154\n",
      "Iteration 26: Train Loss = 0.5050; Validation Loss = 0.5118\n",
      "Iteration 27: Train Loss = 0.5015; Validation Loss = 0.5085\n",
      "Iteration 28: Train Loss = 0.4980; Validation Loss = 0.5051\n",
      "Iteration 29: Train Loss = 0.4947; Validation Loss = 0.5020\n",
      "Iteration 30: Train Loss = 0.4917; Validation Loss = 0.4991\n",
      "Iteration 31: Train Loss = 0.4887; Validation Loss = 0.4962\n",
      "Iteration 32: Train Loss = 0.4858; Validation Loss = 0.4935\n",
      "Iteration 33: Train Loss = 0.4830; Validation Loss = 0.4908\n",
      "Iteration 34: Train Loss = 0.4803; Validation Loss = 0.4884\n",
      "Iteration 35: Train Loss = 0.4778; Validation Loss = 0.4861\n",
      "Iteration 36: Train Loss = 0.4752; Validation Loss = 0.4836\n",
      "Iteration 37: Train Loss = 0.4730; Validation Loss = 0.4815\n",
      "Iteration 38: Train Loss = 0.4707; Validation Loss = 0.4793\n",
      "Iteration 39: Train Loss = 0.4686; Validation Loss = 0.4775\n",
      "Iteration 40: Train Loss = 0.4665; Validation Loss = 0.4755\n",
      "Iteration 41: Train Loss = 0.4645; Validation Loss = 0.4736\n",
      "Iteration 42: Train Loss = 0.4626; Validation Loss = 0.4719\n",
      "Iteration 43: Train Loss = 0.4607; Validation Loss = 0.4701\n",
      "Iteration 44: Train Loss = 0.4589; Validation Loss = 0.4686\n",
      "Iteration 45: Train Loss = 0.4571; Validation Loss = 0.4669\n",
      "Iteration 46: Train Loss = 0.4553; Validation Loss = 0.4653\n",
      "Iteration 47: Train Loss = 0.4537; Validation Loss = 0.4637\n",
      "Iteration 48: Train Loss = 0.4522; Validation Loss = 0.4623\n",
      "Iteration 49: Train Loss = 0.4506; Validation Loss = 0.4609\n",
      "Iteration 50: Train Loss = 0.4492; Validation Loss = 0.4597\n",
      "Iteration 51: Train Loss = 0.4478; Validation Loss = 0.4584\n",
      "Iteration 52: Train Loss = 0.4465; Validation Loss = 0.4573\n",
      "Iteration 53: Train Loss = 0.4452; Validation Loss = 0.4561\n",
      "Iteration 54: Train Loss = 0.4441; Validation Loss = 0.4550\n",
      "Iteration 55: Train Loss = 0.4428; Validation Loss = 0.4538\n",
      "Iteration 56: Train Loss = 0.4416; Validation Loss = 0.4527\n",
      "Iteration 57: Train Loss = 0.4405; Validation Loss = 0.4518\n",
      "Iteration 58: Train Loss = 0.4394; Validation Loss = 0.4508\n",
      "Iteration 59: Train Loss = 0.4383; Validation Loss = 0.4498\n",
      "Iteration 60: Train Loss = 0.4374; Validation Loss = 0.4489\n",
      "Iteration 61: Train Loss = 0.4366; Validation Loss = 0.4482\n",
      "Iteration 62: Train Loss = 0.4356; Validation Loss = 0.4473\n",
      "Iteration 63: Train Loss = 0.4347; Validation Loss = 0.4465\n",
      "Iteration 64: Train Loss = 0.4339; Validation Loss = 0.4457\n",
      "Iteration 65: Train Loss = 0.4330; Validation Loss = 0.4449\n",
      "Iteration 66: Train Loss = 0.4321; Validation Loss = 0.4442\n",
      "Iteration 67: Train Loss = 0.4313; Validation Loss = 0.4434\n",
      "Iteration 68: Train Loss = 0.4306; Validation Loss = 0.4427\n",
      "Iteration 69: Train Loss = 0.4299; Validation Loss = 0.4420\n",
      "Iteration 70: Train Loss = 0.4292; Validation Loss = 0.4414\n",
      "Iteration 71: Train Loss = 0.4286; Validation Loss = 0.4407\n",
      "Iteration 72: Train Loss = 0.4279; Validation Loss = 0.4402\n",
      "Iteration 73: Train Loss = 0.4272; Validation Loss = 0.4396\n",
      "Iteration 74: Train Loss = 0.4266; Validation Loss = 0.4391\n",
      "Iteration 75: Train Loss = 0.4259; Validation Loss = 0.4385\n",
      "Iteration 76: Train Loss = 0.4254; Validation Loss = 0.4381\n",
      "Iteration 77: Train Loss = 0.4250; Validation Loss = 0.4378\n",
      "Iteration 78: Train Loss = 0.4243; Validation Loss = 0.4372\n",
      "Iteration 79: Train Loss = 0.4237; Validation Loss = 0.4367\n",
      "Iteration 80: Train Loss = 0.4232; Validation Loss = 0.4362\n",
      "Iteration 81: Train Loss = 0.4228; Validation Loss = 0.4358\n",
      "Iteration 82: Train Loss = 0.4223; Validation Loss = 0.4355\n",
      "Iteration 83: Train Loss = 0.4219; Validation Loss = 0.4352\n",
      "Iteration 84: Train Loss = 0.4215; Validation Loss = 0.4348\n",
      "Iteration 85: Train Loss = 0.4211; Validation Loss = 0.4345\n",
      "Iteration 86: Train Loss = 0.4207; Validation Loss = 0.4342\n",
      "Iteration 87: Train Loss = 0.4202; Validation Loss = 0.4337\n",
      "Iteration 88: Train Loss = 0.4199; Validation Loss = 0.4334\n",
      "Iteration 89: Train Loss = 0.4195; Validation Loss = 0.4331\n",
      "Iteration 90: Train Loss = 0.4190; Validation Loss = 0.4326\n",
      "Iteration 91: Train Loss = 0.4188; Validation Loss = 0.4324\n",
      "Iteration 92: Train Loss = 0.4183; Validation Loss = 0.4320\n",
      "Iteration 93: Train Loss = 0.4180; Validation Loss = 0.4317\n",
      "Iteration 94: Train Loss = 0.4176; Validation Loss = 0.4314\n",
      "Iteration 95: Train Loss = 0.4173; Validation Loss = 0.4311\n",
      "Iteration 96: Train Loss = 0.4169; Validation Loss = 0.4308\n",
      "Iteration 97: Train Loss = 0.4166; Validation Loss = 0.4306\n",
      "Iteration 98: Train Loss = 0.4164; Validation Loss = 0.4305\n",
      "Iteration 99: Train Loss = 0.4161; Validation Loss = 0.4303\n",
      "Iteration 100: Train Loss = 0.4159; Validation Loss = 0.4301\n",
      "Iteration 101: Train Loss = 0.4156; Validation Loss = 0.4299\n",
      "Iteration 102: Train Loss = 0.4154; Validation Loss = 0.4297\n",
      "Iteration 103: Train Loss = 0.4152; Validation Loss = 0.4296\n",
      "Iteration 104: Train Loss = 0.4149; Validation Loss = 0.4294\n",
      "Iteration 105: Train Loss = 0.4147; Validation Loss = 0.4293\n",
      "Iteration 106: Train Loss = 0.4145; Validation Loss = 0.4292\n",
      "Iteration 107: Train Loss = 0.4142; Validation Loss = 0.4289\n",
      "Iteration 108: Train Loss = 0.4139; Validation Loss = 0.4287\n",
      "Iteration 109: Train Loss = 0.4137; Validation Loss = 0.4286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:30,625] Trial 43 finished with value: 0.9641474974167744 and parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'n_estimators': 110, 'learning_rate': 0.037679814922928205, 'subsample': 0.5550434823771905}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110: Train Loss = 0.4135; Validation Loss = 0.4284\n",
      "Iteration 1: Train Loss = 0.6413; Validation Loss = 0.6432\n",
      "Iteration 2: Train Loss = 0.6003; Validation Loss = 0.6037\n",
      "Iteration 3: Train Loss = 0.5677; Validation Loss = 0.5723\n",
      "Iteration 4: Train Loss = 0.5410; Validation Loss = 0.5470\n",
      "Iteration 5: Train Loss = 0.5195; Validation Loss = 0.5267\n",
      "Iteration 6: Train Loss = 0.5018; Validation Loss = 0.5094\n",
      "Iteration 7: Train Loss = 0.4874; Validation Loss = 0.4960\n",
      "Iteration 8: Train Loss = 0.4755; Validation Loss = 0.4849\n",
      "Iteration 9: Train Loss = 0.4655; Validation Loss = 0.4760\n",
      "Iteration 10: Train Loss = 0.4568; Validation Loss = 0.4677\n",
      "Iteration 11: Train Loss = 0.4496; Validation Loss = 0.4611\n",
      "Iteration 12: Train Loss = 0.4432; Validation Loss = 0.4550\n",
      "Iteration 13: Train Loss = 0.4384; Validation Loss = 0.4505\n",
      "Iteration 14: Train Loss = 0.4338; Validation Loss = 0.4466\n",
      "Iteration 15: Train Loss = 0.4299; Validation Loss = 0.4433\n",
      "Iteration 16: Train Loss = 0.4268; Validation Loss = 0.4406\n",
      "Iteration 17: Train Loss = 0.4239; Validation Loss = 0.4379\n",
      "Iteration 18: Train Loss = 0.4213; Validation Loss = 0.4359\n",
      "Iteration 19: Train Loss = 0.4191; Validation Loss = 0.4339\n",
      "Iteration 20: Train Loss = 0.4175; Validation Loss = 0.4325\n",
      "Iteration 21: Train Loss = 0.4160; Validation Loss = 0.4313\n",
      "Iteration 22: Train Loss = 0.4146; Validation Loss = 0.4301\n",
      "Iteration 23: Train Loss = 0.4136; Validation Loss = 0.4291\n",
      "Iteration 24: Train Loss = 0.4122; Validation Loss = 0.4278\n",
      "Iteration 25: Train Loss = 0.4111; Validation Loss = 0.4269\n",
      "Iteration 26: Train Loss = 0.4104; Validation Loss = 0.4262\n",
      "Iteration 27: Train Loss = 0.4095; Validation Loss = 0.4258\n",
      "Iteration 28: Train Loss = 0.4088; Validation Loss = 0.4253\n",
      "Iteration 29: Train Loss = 0.4078; Validation Loss = 0.4243\n",
      "Iteration 30: Train Loss = 0.4074; Validation Loss = 0.4241\n",
      "Iteration 31: Train Loss = 0.4066; Validation Loss = 0.4236\n",
      "Iteration 32: Train Loss = 0.4064; Validation Loss = 0.4234\n",
      "Iteration 33: Train Loss = 0.4061; Validation Loss = 0.4234\n",
      "Iteration 34: Train Loss = 0.4054; Validation Loss = 0.4232\n",
      "Iteration 35: Train Loss = 0.4052; Validation Loss = 0.4232\n",
      "Iteration 36: Train Loss = 0.4049; Validation Loss = 0.4231\n",
      "Iteration 37: Train Loss = 0.4045; Validation Loss = 0.4227\n",
      "Iteration 38: Train Loss = 0.4042; Validation Loss = 0.4226\n",
      "Iteration 39: Train Loss = 0.4039; Validation Loss = 0.4222\n",
      "Iteration 40: Train Loss = 0.4038; Validation Loss = 0.4224\n",
      "Iteration 41: Train Loss = 0.4035; Validation Loss = 0.4222\n",
      "Iteration 42: Train Loss = 0.4033; Validation Loss = 0.4222\n",
      "Iteration 43: Train Loss = 0.4032; Validation Loss = 0.4221\n",
      "Iteration 44: Train Loss = 0.4026; Validation Loss = 0.4216\n",
      "Iteration 45: Train Loss = 0.4023; Validation Loss = 0.4214\n",
      "Iteration 46: Train Loss = 0.4020; Validation Loss = 0.4215\n",
      "Iteration 47: Train Loss = 0.4018; Validation Loss = 0.4216\n",
      "Iteration 48: Train Loss = 0.4014; Validation Loss = 0.4214\n",
      "Iteration 49: Train Loss = 0.4014; Validation Loss = 0.4216\n",
      "Iteration 50: Train Loss = 0.4012; Validation Loss = 0.4214\n",
      "Iteration 51: Train Loss = 0.4010; Validation Loss = 0.4214\n",
      "Iteration 52: Train Loss = 0.4005; Validation Loss = 0.4211\n",
      "Iteration 53: Train Loss = 0.4000; Validation Loss = 0.4209\n",
      "Iteration 54: Train Loss = 0.3996; Validation Loss = 0.4209\n",
      "Iteration 55: Train Loss = 0.3995; Validation Loss = 0.4207\n",
      "Iteration 56: Train Loss = 0.3989; Validation Loss = 0.4204\n",
      "Iteration 57: Train Loss = 0.3986; Validation Loss = 0.4206\n",
      "Iteration 58: Train Loss = 0.3985; Validation Loss = 0.4204\n",
      "Iteration 59: Train Loss = 0.3984; Validation Loss = 0.4205\n",
      "Iteration 60: Train Loss = 0.3982; Validation Loss = 0.4206\n",
      "Iteration 61: Train Loss = 0.3978; Validation Loss = 0.4202\n",
      "Iteration 62: Train Loss = 0.3975; Validation Loss = 0.4201\n",
      "Iteration 63: Train Loss = 0.3974; Validation Loss = 0.4200\n",
      "Iteration 64: Train Loss = 0.3974; Validation Loss = 0.4201\n",
      "Iteration 65: Train Loss = 0.3972; Validation Loss = 0.4199\n",
      "Iteration 66: Train Loss = 0.3971; Validation Loss = 0.4200\n",
      "Iteration 67: Train Loss = 0.3969; Validation Loss = 0.4198\n",
      "Iteration 68: Train Loss = 0.3965; Validation Loss = 0.4198\n",
      "Iteration 69: Train Loss = 0.3964; Validation Loss = 0.4199\n",
      "Iteration 70: Train Loss = 0.3961; Validation Loss = 0.4199\n",
      "Iteration 71: Train Loss = 0.3959; Validation Loss = 0.4198\n",
      "Iteration 72: Train Loss = 0.3959; Validation Loss = 0.4199\n",
      "Iteration 73: Train Loss = 0.3957; Validation Loss = 0.4201\n",
      "Iteration 74: Train Loss = 0.3954; Validation Loss = 0.4199\n",
      "Iteration 75: Train Loss = 0.3952; Validation Loss = 0.4199\n",
      "Iteration 76: Train Loss = 0.3950; Validation Loss = 0.4197\n",
      "Iteration 77: Train Loss = 0.3949; Validation Loss = 0.4198\n",
      "Iteration 78: Train Loss = 0.3949; Validation Loss = 0.4200\n",
      "Iteration 79: Train Loss = 0.3948; Validation Loss = 0.4202\n",
      "Iteration 80: Train Loss = 0.3945; Validation Loss = 0.4200\n",
      "Iteration 81: Train Loss = 0.3943; Validation Loss = 0.4198\n",
      "Iteration 82: Train Loss = 0.3943; Validation Loss = 0.4199\n",
      "Iteration 83: Train Loss = 0.3940; Validation Loss = 0.4199\n",
      "Iteration 84: Train Loss = 0.3941; Validation Loss = 0.4200\n",
      "Iteration 85: Train Loss = 0.3941; Validation Loss = 0.4198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:38,540] Trial 44 finished with value: 0.9612737304065673 and parameters: {'max_depth': 8, 'min_samples_leaf': 6, 'n_estimators': 120, 'learning_rate': 0.15429908141922724, 'subsample': 0.5946474295402212}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86: Train Loss = 0.3941; Validation Loss = 0.4199\n",
      "Iteration 87: Train Loss = 0.3939; Validation Loss = 0.4198\n",
      "Early stopping at iteration 86\n",
      "Iteration 1: Train Loss = 0.6661; Validation Loss = 0.6675\n",
      "Iteration 2: Train Loss = 0.6420; Validation Loss = 0.6445\n",
      "Iteration 3: Train Loss = 0.6205; Validation Loss = 0.6240\n",
      "Iteration 4: Train Loss = 0.6010; Validation Loss = 0.6056\n",
      "Iteration 5: Train Loss = 0.5837; Validation Loss = 0.5892\n",
      "Iteration 6: Train Loss = 0.5677; Validation Loss = 0.5744\n",
      "Iteration 7: Train Loss = 0.5535; Validation Loss = 0.5609\n",
      "Iteration 8: Train Loss = 0.5409; Validation Loss = 0.5490\n",
      "Iteration 9: Train Loss = 0.5294; Validation Loss = 0.5385\n",
      "Iteration 10: Train Loss = 0.5190; Validation Loss = 0.5289\n",
      "Iteration 11: Train Loss = 0.5093; Validation Loss = 0.5197\n",
      "Iteration 12: Train Loss = 0.5005; Validation Loss = 0.5117\n",
      "Iteration 13: Train Loss = 0.4926; Validation Loss = 0.5044\n",
      "Iteration 14: Train Loss = 0.4854; Validation Loss = 0.4978\n",
      "Iteration 15: Train Loss = 0.4786; Validation Loss = 0.4915\n",
      "Iteration 16: Train Loss = 0.4727; Validation Loss = 0.4860\n",
      "Iteration 17: Train Loss = 0.4668; Validation Loss = 0.4806\n",
      "Iteration 18: Train Loss = 0.4618; Validation Loss = 0.4761\n",
      "Iteration 19: Train Loss = 0.4568; Validation Loss = 0.4717\n",
      "Iteration 20: Train Loss = 0.4523; Validation Loss = 0.4677\n",
      "Iteration 21: Train Loss = 0.4485; Validation Loss = 0.4642\n",
      "Iteration 22: Train Loss = 0.4450; Validation Loss = 0.4611\n",
      "Iteration 23: Train Loss = 0.4416; Validation Loss = 0.4577\n",
      "Iteration 24: Train Loss = 0.4384; Validation Loss = 0.4550\n",
      "Iteration 25: Train Loss = 0.4356; Validation Loss = 0.4526\n",
      "Iteration 26: Train Loss = 0.4329; Validation Loss = 0.4504\n",
      "Iteration 27: Train Loss = 0.4304; Validation Loss = 0.4483\n",
      "Iteration 28: Train Loss = 0.4280; Validation Loss = 0.4464\n",
      "Iteration 29: Train Loss = 0.4258; Validation Loss = 0.4444\n",
      "Iteration 30: Train Loss = 0.4240; Validation Loss = 0.4428\n",
      "Iteration 31: Train Loss = 0.4220; Validation Loss = 0.4414\n",
      "Iteration 32: Train Loss = 0.4204; Validation Loss = 0.4398\n",
      "Iteration 33: Train Loss = 0.4187; Validation Loss = 0.4384\n",
      "Iteration 34: Train Loss = 0.4174; Validation Loss = 0.4373\n",
      "Iteration 35: Train Loss = 0.4160; Validation Loss = 0.4361\n",
      "Iteration 36: Train Loss = 0.4147; Validation Loss = 0.4354\n",
      "Iteration 37: Train Loss = 0.4135; Validation Loss = 0.4344\n",
      "Iteration 38: Train Loss = 0.4125; Validation Loss = 0.4337\n",
      "Iteration 39: Train Loss = 0.4112; Validation Loss = 0.4328\n",
      "Iteration 40: Train Loss = 0.4102; Validation Loss = 0.4321\n",
      "Iteration 41: Train Loss = 0.4090; Validation Loss = 0.4312\n",
      "Iteration 42: Train Loss = 0.4082; Validation Loss = 0.4306\n",
      "Iteration 43: Train Loss = 0.4074; Validation Loss = 0.4301\n",
      "Iteration 44: Train Loss = 0.4067; Validation Loss = 0.4295\n",
      "Iteration 45: Train Loss = 0.4060; Validation Loss = 0.4290\n",
      "Iteration 46: Train Loss = 0.4054; Validation Loss = 0.4287\n",
      "Iteration 47: Train Loss = 0.4045; Validation Loss = 0.4280\n",
      "Iteration 48: Train Loss = 0.4041; Validation Loss = 0.4276\n",
      "Iteration 49: Train Loss = 0.4038; Validation Loss = 0.4272\n",
      "Iteration 50: Train Loss = 0.4033; Validation Loss = 0.4269\n",
      "Iteration 51: Train Loss = 0.4029; Validation Loss = 0.4267\n",
      "Iteration 52: Train Loss = 0.4024; Validation Loss = 0.4263\n",
      "Iteration 53: Train Loss = 0.4020; Validation Loss = 0.4260\n",
      "Iteration 54: Train Loss = 0.4015; Validation Loss = 0.4257\n",
      "Iteration 55: Train Loss = 0.4010; Validation Loss = 0.4254\n",
      "Iteration 56: Train Loss = 0.4006; Validation Loss = 0.4251\n",
      "Iteration 57: Train Loss = 0.4002; Validation Loss = 0.4248\n",
      "Iteration 58: Train Loss = 0.3999; Validation Loss = 0.4245\n",
      "Iteration 59: Train Loss = 0.3996; Validation Loss = 0.4242\n",
      "Iteration 60: Train Loss = 0.3994; Validation Loss = 0.4241\n",
      "Iteration 61: Train Loss = 0.3991; Validation Loss = 0.4238\n",
      "Iteration 62: Train Loss = 0.3989; Validation Loss = 0.4238\n",
      "Iteration 63: Train Loss = 0.3985; Validation Loss = 0.4234\n",
      "Iteration 64: Train Loss = 0.3982; Validation Loss = 0.4233\n",
      "Iteration 65: Train Loss = 0.3980; Validation Loss = 0.4232\n",
      "Iteration 66: Train Loss = 0.3978; Validation Loss = 0.4231\n",
      "Iteration 67: Train Loss = 0.3975; Validation Loss = 0.4231\n",
      "Iteration 68: Train Loss = 0.3973; Validation Loss = 0.4230\n",
      "Iteration 69: Train Loss = 0.3970; Validation Loss = 0.4227\n",
      "Iteration 70: Train Loss = 0.3968; Validation Loss = 0.4226\n",
      "Iteration 71: Train Loss = 0.3966; Validation Loss = 0.4226\n",
      "Iteration 72: Train Loss = 0.3965; Validation Loss = 0.4226\n",
      "Iteration 73: Train Loss = 0.3963; Validation Loss = 0.4226\n",
      "Iteration 74: Train Loss = 0.3961; Validation Loss = 0.4227\n",
      "Iteration 75: Train Loss = 0.3959; Validation Loss = 0.4225\n",
      "Iteration 76: Train Loss = 0.3958; Validation Loss = 0.4224\n",
      "Iteration 77: Train Loss = 0.3955; Validation Loss = 0.4221\n",
      "Iteration 78: Train Loss = 0.3954; Validation Loss = 0.4221\n",
      "Iteration 79: Train Loss = 0.3953; Validation Loss = 0.4221\n",
      "Iteration 80: Train Loss = 0.3951; Validation Loss = 0.4219\n",
      "Iteration 81: Train Loss = 0.3949; Validation Loss = 0.4218\n",
      "Iteration 82: Train Loss = 0.3947; Validation Loss = 0.4216\n",
      "Iteration 83: Train Loss = 0.3946; Validation Loss = 0.4216\n",
      "Iteration 84: Train Loss = 0.3945; Validation Loss = 0.4217\n",
      "Iteration 85: Train Loss = 0.3943; Validation Loss = 0.4216\n",
      "Iteration 86: Train Loss = 0.3941; Validation Loss = 0.4215\n",
      "Iteration 87: Train Loss = 0.3940; Validation Loss = 0.4215\n",
      "Iteration 88: Train Loss = 0.3938; Validation Loss = 0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:38:47,166] Trial 45 finished with value: 0.9611833651708741 and parameters: {'max_depth': 10, 'min_samples_leaf': 5, 'n_estimators': 90, 'learning_rate': 0.07679977197260056, 'subsample': 0.5034381018014492}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89: Train Loss = 0.3938; Validation Loss = 0.4214\n",
      "Iteration 90: Train Loss = 0.3934; Validation Loss = 0.4212\n",
      "Iteration 1: Train Loss = 0.6858; Validation Loss = 0.6860\n",
      "Iteration 2: Train Loss = 0.6786; Validation Loss = 0.6790\n",
      "Iteration 3: Train Loss = 0.6717; Validation Loss = 0.6723\n",
      "Iteration 4: Train Loss = 0.6650; Validation Loss = 0.6657\n",
      "Iteration 5: Train Loss = 0.6585; Validation Loss = 0.6594\n",
      "Iteration 6: Train Loss = 0.6523; Validation Loss = 0.6534\n",
      "Iteration 7: Train Loss = 0.6462; Validation Loss = 0.6475\n",
      "Iteration 8: Train Loss = 0.6403; Validation Loss = 0.6418\n",
      "Iteration 9: Train Loss = 0.6346; Validation Loss = 0.6362\n",
      "Iteration 10: Train Loss = 0.6290; Validation Loss = 0.6308\n",
      "Iteration 11: Train Loss = 0.6236; Validation Loss = 0.6255\n",
      "Iteration 12: Train Loss = 0.6184; Validation Loss = 0.6204\n",
      "Iteration 13: Train Loss = 0.6134; Validation Loss = 0.6156\n",
      "Iteration 14: Train Loss = 0.6086; Validation Loss = 0.6109\n",
      "Iteration 15: Train Loss = 0.6038; Validation Loss = 0.6063\n",
      "Iteration 16: Train Loss = 0.5991; Validation Loss = 0.6017\n",
      "Iteration 17: Train Loss = 0.5948; Validation Loss = 0.5975\n",
      "Iteration 18: Train Loss = 0.5904; Validation Loss = 0.5933\n",
      "Iteration 19: Train Loss = 0.5861; Validation Loss = 0.5892\n",
      "Iteration 20: Train Loss = 0.5819; Validation Loss = 0.5851\n",
      "Iteration 21: Train Loss = 0.5780; Validation Loss = 0.5812\n",
      "Iteration 22: Train Loss = 0.5741; Validation Loss = 0.5775\n",
      "Iteration 23: Train Loss = 0.5703; Validation Loss = 0.5738\n",
      "Iteration 24: Train Loss = 0.5667; Validation Loss = 0.5704\n",
      "Iteration 25: Train Loss = 0.5632; Validation Loss = 0.5669\n",
      "Iteration 26: Train Loss = 0.5597; Validation Loss = 0.5635\n",
      "Iteration 27: Train Loss = 0.5564; Validation Loss = 0.5603\n",
      "Iteration 28: Train Loss = 0.5532; Validation Loss = 0.5571\n",
      "Iteration 29: Train Loss = 0.5500; Validation Loss = 0.5540\n",
      "Iteration 30: Train Loss = 0.5470; Validation Loss = 0.5511\n",
      "Iteration 31: Train Loss = 0.5440; Validation Loss = 0.5482\n",
      "Iteration 32: Train Loss = 0.5411; Validation Loss = 0.5454\n",
      "Iteration 33: Train Loss = 0.5383; Validation Loss = 0.5427\n",
      "Iteration 34: Train Loss = 0.5354; Validation Loss = 0.5400\n",
      "Iteration 35: Train Loss = 0.5328; Validation Loss = 0.5374\n",
      "Iteration 36: Train Loss = 0.5301; Validation Loss = 0.5349\n",
      "Iteration 37: Train Loss = 0.5276; Validation Loss = 0.5324\n",
      "Iteration 38: Train Loss = 0.5251; Validation Loss = 0.5300\n",
      "Iteration 39: Train Loss = 0.5226; Validation Loss = 0.5276\n",
      "Iteration 40: Train Loss = 0.5203; Validation Loss = 0.5254\n",
      "Iteration 41: Train Loss = 0.5180; Validation Loss = 0.5232\n",
      "Iteration 42: Train Loss = 0.5158; Validation Loss = 0.5211\n",
      "Iteration 43: Train Loss = 0.5136; Validation Loss = 0.5190\n",
      "Iteration 44: Train Loss = 0.5115; Validation Loss = 0.5170\n",
      "Iteration 45: Train Loss = 0.5095; Validation Loss = 0.5149\n",
      "Iteration 46: Train Loss = 0.5075; Validation Loss = 0.5130\n",
      "Iteration 47: Train Loss = 0.5055; Validation Loss = 0.5111\n",
      "Iteration 48: Train Loss = 0.5036; Validation Loss = 0.5092\n",
      "Iteration 49: Train Loss = 0.5017; Validation Loss = 0.5074\n",
      "Iteration 50: Train Loss = 0.4998; Validation Loss = 0.5056\n",
      "Iteration 51: Train Loss = 0.4980; Validation Loss = 0.5040\n",
      "Iteration 52: Train Loss = 0.4963; Validation Loss = 0.5023\n",
      "Iteration 53: Train Loss = 0.4945; Validation Loss = 0.5006\n",
      "Iteration 54: Train Loss = 0.4928; Validation Loss = 0.4990\n",
      "Iteration 55: Train Loss = 0.4912; Validation Loss = 0.4975\n",
      "Iteration 56: Train Loss = 0.4895; Validation Loss = 0.4959\n",
      "Iteration 57: Train Loss = 0.4880; Validation Loss = 0.4945\n",
      "Iteration 58: Train Loss = 0.4865; Validation Loss = 0.4931\n",
      "Iteration 59: Train Loss = 0.4850; Validation Loss = 0.4917\n",
      "Iteration 60: Train Loss = 0.4835; Validation Loss = 0.4902\n",
      "Iteration 61: Train Loss = 0.4821; Validation Loss = 0.4889\n",
      "Iteration 62: Train Loss = 0.4808; Validation Loss = 0.4876\n",
      "Iteration 63: Train Loss = 0.4795; Validation Loss = 0.4863\n",
      "Iteration 64: Train Loss = 0.4781; Validation Loss = 0.4850\n",
      "Iteration 65: Train Loss = 0.4768; Validation Loss = 0.4838\n",
      "Iteration 66: Train Loss = 0.4756; Validation Loss = 0.4826\n",
      "Iteration 67: Train Loss = 0.4744; Validation Loss = 0.4814\n",
      "Iteration 68: Train Loss = 0.4732; Validation Loss = 0.4803\n",
      "Iteration 69: Train Loss = 0.4721; Validation Loss = 0.4793\n",
      "Iteration 70: Train Loss = 0.4709; Validation Loss = 0.4782\n",
      "Iteration 71: Train Loss = 0.4698; Validation Loss = 0.4772\n",
      "Iteration 72: Train Loss = 0.4687; Validation Loss = 0.4761\n",
      "Iteration 73: Train Loss = 0.4676; Validation Loss = 0.4751\n",
      "Iteration 74: Train Loss = 0.4665; Validation Loss = 0.4741\n",
      "Iteration 75: Train Loss = 0.4654; Validation Loss = 0.4731\n",
      "Iteration 76: Train Loss = 0.4644; Validation Loss = 0.4721\n",
      "Iteration 77: Train Loss = 0.4634; Validation Loss = 0.4713\n",
      "Iteration 78: Train Loss = 0.4625; Validation Loss = 0.4704\n",
      "Iteration 79: Train Loss = 0.4617; Validation Loss = 0.4696\n",
      "Iteration 80: Train Loss = 0.4608; Validation Loss = 0.4687\n",
      "Iteration 81: Train Loss = 0.4598; Validation Loss = 0.4678\n",
      "Iteration 82: Train Loss = 0.4589; Validation Loss = 0.4669\n",
      "Iteration 83: Train Loss = 0.4580; Validation Loss = 0.4661\n",
      "Iteration 84: Train Loss = 0.4571; Validation Loss = 0.4653\n",
      "Iteration 85: Train Loss = 0.4563; Validation Loss = 0.4645\n",
      "Iteration 86: Train Loss = 0.4555; Validation Loss = 0.4637\n",
      "Iteration 87: Train Loss = 0.4548; Validation Loss = 0.4630\n",
      "Iteration 88: Train Loss = 0.4540; Validation Loss = 0.4623\n",
      "Iteration 89: Train Loss = 0.4533; Validation Loss = 0.4616\n",
      "Iteration 90: Train Loss = 0.4525; Validation Loss = 0.4608\n",
      "Iteration 91: Train Loss = 0.4517; Validation Loss = 0.4601\n",
      "Iteration 92: Train Loss = 0.4510; Validation Loss = 0.4595\n",
      "Iteration 93: Train Loss = 0.4503; Validation Loss = 0.4588\n",
      "Iteration 94: Train Loss = 0.4496; Validation Loss = 0.4582\n",
      "Iteration 95: Train Loss = 0.4489; Validation Loss = 0.4575\n",
      "Iteration 96: Train Loss = 0.4483; Validation Loss = 0.4569\n",
      "Iteration 97: Train Loss = 0.4478; Validation Loss = 0.4564\n",
      "Iteration 98: Train Loss = 0.4472; Validation Loss = 0.4559\n",
      "Iteration 99: Train Loss = 0.4465; Validation Loss = 0.4553\n",
      "Iteration 100: Train Loss = 0.4459; Validation Loss = 0.4547\n",
      "Iteration 101: Train Loss = 0.4453; Validation Loss = 0.4542\n",
      "Iteration 102: Train Loss = 0.4447; Validation Loss = 0.4536\n",
      "Iteration 103: Train Loss = 0.4441; Validation Loss = 0.4531\n",
      "Iteration 104: Train Loss = 0.4436; Validation Loss = 0.4526\n",
      "Iteration 105: Train Loss = 0.4430; Validation Loss = 0.4521\n",
      "Iteration 106: Train Loss = 0.4425; Validation Loss = 0.4516\n",
      "Iteration 107: Train Loss = 0.4420; Validation Loss = 0.4512\n",
      "Iteration 108: Train Loss = 0.4415; Validation Loss = 0.4507\n",
      "Iteration 109: Train Loss = 0.4410; Validation Loss = 0.4502\n",
      "Iteration 110: Train Loss = 0.4405; Validation Loss = 0.4497\n",
      "Iteration 111: Train Loss = 0.4400; Validation Loss = 0.4493\n",
      "Iteration 112: Train Loss = 0.4395; Validation Loss = 0.4489\n",
      "Iteration 113: Train Loss = 0.4391; Validation Loss = 0.4485\n",
      "Iteration 114: Train Loss = 0.4386; Validation Loss = 0.4481\n",
      "Iteration 115: Train Loss = 0.4382; Validation Loss = 0.4477\n",
      "Iteration 116: Train Loss = 0.4378; Validation Loss = 0.4473\n",
      "Iteration 117: Train Loss = 0.4373; Validation Loss = 0.4469\n",
      "Iteration 118: Train Loss = 0.4369; Validation Loss = 0.4465\n",
      "Iteration 119: Train Loss = 0.4365; Validation Loss = 0.4461\n",
      "Iteration 120: Train Loss = 0.4361; Validation Loss = 0.4457\n",
      "Iteration 121: Train Loss = 0.4356; Validation Loss = 0.4452\n",
      "Iteration 122: Train Loss = 0.4352; Validation Loss = 0.4448\n",
      "Iteration 123: Train Loss = 0.4348; Validation Loss = 0.4445\n",
      "Iteration 124: Train Loss = 0.4345; Validation Loss = 0.4442\n",
      "Iteration 125: Train Loss = 0.4340; Validation Loss = 0.4437\n",
      "Iteration 126: Train Loss = 0.4337; Validation Loss = 0.4434\n",
      "Iteration 127: Train Loss = 0.4333; Validation Loss = 0.4431\n",
      "Iteration 128: Train Loss = 0.4329; Validation Loss = 0.4427\n",
      "Iteration 129: Train Loss = 0.4326; Validation Loss = 0.4424\n",
      "Iteration 130: Train Loss = 0.4321; Validation Loss = 0.4420\n",
      "Iteration 131: Train Loss = 0.4318; Validation Loss = 0.4417\n",
      "Iteration 132: Train Loss = 0.4315; Validation Loss = 0.4414\n",
      "Iteration 133: Train Loss = 0.4311; Validation Loss = 0.4411\n",
      "Iteration 134: Train Loss = 0.4307; Validation Loss = 0.4407\n",
      "Iteration 135: Train Loss = 0.4304; Validation Loss = 0.4404\n",
      "Iteration 136: Train Loss = 0.4301; Validation Loss = 0.4402\n",
      "Iteration 137: Train Loss = 0.4298; Validation Loss = 0.4399\n",
      "Iteration 138: Train Loss = 0.4295; Validation Loss = 0.4396\n",
      "Iteration 139: Train Loss = 0.4292; Validation Loss = 0.4394\n",
      "Iteration 140: Train Loss = 0.4289; Validation Loss = 0.4392\n",
      "Iteration 141: Train Loss = 0.4286; Validation Loss = 0.4389\n",
      "Iteration 142: Train Loss = 0.4284; Validation Loss = 0.4387\n",
      "Iteration 143: Train Loss = 0.4281; Validation Loss = 0.4385\n",
      "Iteration 144: Train Loss = 0.4279; Validation Loss = 0.4383\n",
      "Iteration 145: Train Loss = 0.4277; Validation Loss = 0.4381\n",
      "Iteration 146: Train Loss = 0.4275; Validation Loss = 0.4379\n",
      "Iteration 147: Train Loss = 0.4272; Validation Loss = 0.4376\n",
      "Iteration 148: Train Loss = 0.4270; Validation Loss = 0.4374\n",
      "Iteration 149: Train Loss = 0.4266; Validation Loss = 0.4371\n",
      "Iteration 150: Train Loss = 0.4264; Validation Loss = 0.4369\n",
      "Iteration 151: Train Loss = 0.4262; Validation Loss = 0.4368\n",
      "Iteration 152: Train Loss = 0.4260; Validation Loss = 0.4366\n",
      "Iteration 153: Train Loss = 0.4258; Validation Loss = 0.4363\n",
      "Iteration 154: Train Loss = 0.4256; Validation Loss = 0.4362\n",
      "Iteration 155: Train Loss = 0.4253; Validation Loss = 0.4359\n",
      "Iteration 156: Train Loss = 0.4252; Validation Loss = 0.4358\n",
      "Iteration 157: Train Loss = 0.4250; Validation Loss = 0.4356\n",
      "Iteration 158: Train Loss = 0.4247; Validation Loss = 0.4354\n",
      "Iteration 159: Train Loss = 0.4245; Validation Loss = 0.4352\n",
      "Iteration 160: Train Loss = 0.4243; Validation Loss = 0.4350\n",
      "Iteration 161: Train Loss = 0.4240; Validation Loss = 0.4348\n",
      "Iteration 162: Train Loss = 0.4239; Validation Loss = 0.4347\n",
      "Iteration 163: Train Loss = 0.4238; Validation Loss = 0.4346\n",
      "Iteration 164: Train Loss = 0.4236; Validation Loss = 0.4344\n",
      "Iteration 165: Train Loss = 0.4235; Validation Loss = 0.4343\n",
      "Iteration 166: Train Loss = 0.4232; Validation Loss = 0.4341\n",
      "Iteration 167: Train Loss = 0.4230; Validation Loss = 0.4339\n",
      "Iteration 168: Train Loss = 0.4228; Validation Loss = 0.4338\n",
      "Iteration 169: Train Loss = 0.4226; Validation Loss = 0.4336\n",
      "Iteration 170: Train Loss = 0.4225; Validation Loss = 0.4335\n",
      "Iteration 171: Train Loss = 0.4223; Validation Loss = 0.4333\n",
      "Iteration 172: Train Loss = 0.4221; Validation Loss = 0.4331\n",
      "Iteration 173: Train Loss = 0.4220; Validation Loss = 0.4330\n",
      "Iteration 174: Train Loss = 0.4218; Validation Loss = 0.4329\n",
      "Iteration 175: Train Loss = 0.4217; Validation Loss = 0.4327\n",
      "Iteration 176: Train Loss = 0.4215; Validation Loss = 0.4326\n",
      "Iteration 177: Train Loss = 0.4214; Validation Loss = 0.4325\n",
      "Iteration 178: Train Loss = 0.4213; Validation Loss = 0.4324\n",
      "Iteration 179: Train Loss = 0.4212; Validation Loss = 0.4323\n",
      "Iteration 180: Train Loss = 0.4210; Validation Loss = 0.4321\n",
      "Iteration 181: Train Loss = 0.4208; Validation Loss = 0.4320\n",
      "Iteration 182: Train Loss = 0.4207; Validation Loss = 0.4319\n",
      "Iteration 183: Train Loss = 0.4206; Validation Loss = 0.4318\n",
      "Iteration 184: Train Loss = 0.4204; Validation Loss = 0.4316\n",
      "Iteration 185: Train Loss = 0.4203; Validation Loss = 0.4315\n",
      "Iteration 186: Train Loss = 0.4202; Validation Loss = 0.4314\n",
      "Iteration 187: Train Loss = 0.4201; Validation Loss = 0.4313\n",
      "Iteration 188: Train Loss = 0.4199; Validation Loss = 0.4312\n",
      "Iteration 189: Train Loss = 0.4198; Validation Loss = 0.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:39:06,357] Trial 46 finished with value: 0.9649623096896698 and parameters: {'max_depth': 6, 'min_samples_leaf': 4, 'n_estimators': 190, 'learning_rate': 0.022165749829091915, 'subsample': 0.6622829253122787}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190: Train Loss = 0.4197; Validation Loss = 0.4310\n",
      "Iteration 1: Train Loss = 0.6871; Validation Loss = 0.6873\n",
      "Iteration 2: Train Loss = 0.6813; Validation Loss = 0.6816\n",
      "Iteration 3: Train Loss = 0.6756; Validation Loss = 0.6760\n",
      "Iteration 4: Train Loss = 0.6700; Validation Loss = 0.6707\n",
      "Iteration 5: Train Loss = 0.6646; Validation Loss = 0.6654\n",
      "Iteration 6: Train Loss = 0.6594; Validation Loss = 0.6603\n",
      "Iteration 7: Train Loss = 0.6543; Validation Loss = 0.6553\n",
      "Iteration 8: Train Loss = 0.6494; Validation Loss = 0.6505\n",
      "Iteration 9: Train Loss = 0.6446; Validation Loss = 0.6458\n",
      "Iteration 10: Train Loss = 0.6400; Validation Loss = 0.6412\n",
      "Iteration 11: Train Loss = 0.6355; Validation Loss = 0.6369\n",
      "Iteration 12: Train Loss = 0.6311; Validation Loss = 0.6326\n",
      "Iteration 13: Train Loss = 0.6268; Validation Loss = 0.6284\n",
      "Iteration 14: Train Loss = 0.6227; Validation Loss = 0.6244\n",
      "Iteration 15: Train Loss = 0.6187; Validation Loss = 0.6204\n",
      "Iteration 16: Train Loss = 0.6148; Validation Loss = 0.6166\n",
      "Iteration 17: Train Loss = 0.6109; Validation Loss = 0.6128\n",
      "Iteration 18: Train Loss = 0.6072; Validation Loss = 0.6092\n",
      "Iteration 19: Train Loss = 0.6036; Validation Loss = 0.6057\n",
      "Iteration 20: Train Loss = 0.6001; Validation Loss = 0.6023\n",
      "Iteration 21: Train Loss = 0.5966; Validation Loss = 0.5988\n",
      "Iteration 22: Train Loss = 0.5932; Validation Loss = 0.5954\n",
      "Iteration 23: Train Loss = 0.5899; Validation Loss = 0.5923\n",
      "Iteration 24: Train Loss = 0.5868; Validation Loss = 0.5892\n",
      "Iteration 25: Train Loss = 0.5838; Validation Loss = 0.5862\n",
      "Iteration 26: Train Loss = 0.5807; Validation Loss = 0.5832\n",
      "Iteration 27: Train Loss = 0.5778; Validation Loss = 0.5804\n",
      "Iteration 28: Train Loss = 0.5750; Validation Loss = 0.5777\n",
      "Iteration 29: Train Loss = 0.5723; Validation Loss = 0.5750\n",
      "Iteration 30: Train Loss = 0.5696; Validation Loss = 0.5724\n",
      "Iteration 31: Train Loss = 0.5669; Validation Loss = 0.5697\n",
      "Iteration 32: Train Loss = 0.5644; Validation Loss = 0.5672\n",
      "Iteration 33: Train Loss = 0.5618; Validation Loss = 0.5647\n",
      "Iteration 34: Train Loss = 0.5593; Validation Loss = 0.5623\n",
      "Iteration 35: Train Loss = 0.5570; Validation Loss = 0.5600\n",
      "Iteration 36: Train Loss = 0.5546; Validation Loss = 0.5577\n",
      "Iteration 37: Train Loss = 0.5523; Validation Loss = 0.5555\n",
      "Iteration 38: Train Loss = 0.5501; Validation Loss = 0.5533\n",
      "Iteration 39: Train Loss = 0.5480; Validation Loss = 0.5512\n",
      "Iteration 40: Train Loss = 0.5458; Validation Loss = 0.5491\n",
      "Iteration 41: Train Loss = 0.5438; Validation Loss = 0.5471\n",
      "Iteration 42: Train Loss = 0.5417; Validation Loss = 0.5451\n",
      "Iteration 43: Train Loss = 0.5398; Validation Loss = 0.5432\n",
      "Iteration 44: Train Loss = 0.5379; Validation Loss = 0.5413\n",
      "Iteration 45: Train Loss = 0.5360; Validation Loss = 0.5395\n",
      "Iteration 46: Train Loss = 0.5342; Validation Loss = 0.5378\n",
      "Iteration 47: Train Loss = 0.5324; Validation Loss = 0.5360\n",
      "Iteration 48: Train Loss = 0.5307; Validation Loss = 0.5342\n",
      "Iteration 49: Train Loss = 0.5290; Validation Loss = 0.5325\n",
      "Iteration 50: Train Loss = 0.5273; Validation Loss = 0.5308\n",
      "Iteration 51: Train Loss = 0.5257; Validation Loss = 0.5292\n",
      "Iteration 52: Train Loss = 0.5241; Validation Loss = 0.5277\n",
      "Iteration 53: Train Loss = 0.5225; Validation Loss = 0.5262\n",
      "Iteration 54: Train Loss = 0.5211; Validation Loss = 0.5247\n",
      "Iteration 55: Train Loss = 0.5195; Validation Loss = 0.5233\n",
      "Iteration 56: Train Loss = 0.5181; Validation Loss = 0.5218\n",
      "Iteration 57: Train Loss = 0.5167; Validation Loss = 0.5205\n",
      "Iteration 58: Train Loss = 0.5153; Validation Loss = 0.5191\n",
      "Iteration 59: Train Loss = 0.5139; Validation Loss = 0.5177\n",
      "Iteration 60: Train Loss = 0.5126; Validation Loss = 0.5165\n",
      "Iteration 61: Train Loss = 0.5113; Validation Loss = 0.5152\n",
      "Iteration 62: Train Loss = 0.5100; Validation Loss = 0.5139\n",
      "Iteration 63: Train Loss = 0.5088; Validation Loss = 0.5127\n",
      "Iteration 64: Train Loss = 0.5076; Validation Loss = 0.5115\n",
      "Iteration 65: Train Loss = 0.5064; Validation Loss = 0.5104\n",
      "Iteration 66: Train Loss = 0.5052; Validation Loss = 0.5092\n",
      "Iteration 67: Train Loss = 0.5041; Validation Loss = 0.5082\n",
      "Iteration 68: Train Loss = 0.5030; Validation Loss = 0.5070\n",
      "Iteration 69: Train Loss = 0.5018; Validation Loss = 0.5059\n",
      "Iteration 70: Train Loss = 0.5007; Validation Loss = 0.5048\n",
      "Iteration 71: Train Loss = 0.4996; Validation Loss = 0.5037\n",
      "Iteration 72: Train Loss = 0.4985; Validation Loss = 0.5027\n",
      "Iteration 73: Train Loss = 0.4975; Validation Loss = 0.5017\n",
      "Iteration 74: Train Loss = 0.4964; Validation Loss = 0.5007\n",
      "Iteration 75: Train Loss = 0.4954; Validation Loss = 0.4997\n",
      "Iteration 76: Train Loss = 0.4945; Validation Loss = 0.4987\n",
      "Iteration 77: Train Loss = 0.4936; Validation Loss = 0.4978\n",
      "Iteration 78: Train Loss = 0.4927; Validation Loss = 0.4970\n",
      "Iteration 79: Train Loss = 0.4918; Validation Loss = 0.4961\n",
      "Iteration 80: Train Loss = 0.4909; Validation Loss = 0.4953\n",
      "Iteration 81: Train Loss = 0.4900; Validation Loss = 0.4943\n",
      "Iteration 82: Train Loss = 0.4891; Validation Loss = 0.4935\n",
      "Iteration 83: Train Loss = 0.4883; Validation Loss = 0.4927\n",
      "Iteration 84: Train Loss = 0.4875; Validation Loss = 0.4919\n",
      "Iteration 85: Train Loss = 0.4867; Validation Loss = 0.4911\n",
      "Iteration 86: Train Loss = 0.4858; Validation Loss = 0.4903\n",
      "Iteration 87: Train Loss = 0.4850; Validation Loss = 0.4895\n",
      "Iteration 88: Train Loss = 0.4843; Validation Loss = 0.4888\n",
      "Iteration 89: Train Loss = 0.4835; Validation Loss = 0.4880\n",
      "Iteration 90: Train Loss = 0.4828; Validation Loss = 0.4873\n",
      "Iteration 91: Train Loss = 0.4821; Validation Loss = 0.4866\n",
      "Iteration 92: Train Loss = 0.4813; Validation Loss = 0.4858\n",
      "Iteration 93: Train Loss = 0.4806; Validation Loss = 0.4852\n",
      "Iteration 94: Train Loss = 0.4800; Validation Loss = 0.4845\n",
      "Iteration 95: Train Loss = 0.4792; Validation Loss = 0.4838\n",
      "Iteration 96: Train Loss = 0.4786; Validation Loss = 0.4832\n",
      "Iteration 97: Train Loss = 0.4779; Validation Loss = 0.4825\n",
      "Iteration 98: Train Loss = 0.4772; Validation Loss = 0.4818\n",
      "Iteration 99: Train Loss = 0.4766; Validation Loss = 0.4812\n",
      "Iteration 100: Train Loss = 0.4760; Validation Loss = 0.4806\n",
      "Iteration 101: Train Loss = 0.4754; Validation Loss = 0.4800\n",
      "Iteration 102: Train Loss = 0.4748; Validation Loss = 0.4794\n",
      "Iteration 103: Train Loss = 0.4742; Validation Loss = 0.4788\n",
      "Iteration 104: Train Loss = 0.4737; Validation Loss = 0.4783\n",
      "Iteration 105: Train Loss = 0.4731; Validation Loss = 0.4778\n",
      "Iteration 106: Train Loss = 0.4726; Validation Loss = 0.4772\n",
      "Iteration 107: Train Loss = 0.4720; Validation Loss = 0.4767\n",
      "Iteration 108: Train Loss = 0.4714; Validation Loss = 0.4761\n",
      "Iteration 109: Train Loss = 0.4709; Validation Loss = 0.4756\n",
      "Iteration 110: Train Loss = 0.4703; Validation Loss = 0.4750\n",
      "Iteration 111: Train Loss = 0.4698; Validation Loss = 0.4744\n",
      "Iteration 112: Train Loss = 0.4693; Validation Loss = 0.4739\n",
      "Iteration 113: Train Loss = 0.4688; Validation Loss = 0.4734\n",
      "Iteration 114: Train Loss = 0.4683; Validation Loss = 0.4730\n",
      "Iteration 115: Train Loss = 0.4678; Validation Loss = 0.4724\n",
      "Iteration 116: Train Loss = 0.4673; Validation Loss = 0.4720\n",
      "Iteration 117: Train Loss = 0.4668; Validation Loss = 0.4715\n",
      "Iteration 118: Train Loss = 0.4664; Validation Loss = 0.4710\n",
      "Iteration 119: Train Loss = 0.4659; Validation Loss = 0.4706\n",
      "Iteration 120: Train Loss = 0.4654; Validation Loss = 0.4701\n",
      "Iteration 121: Train Loss = 0.4649; Validation Loss = 0.4696\n",
      "Iteration 122: Train Loss = 0.4645; Validation Loss = 0.4692\n",
      "Iteration 123: Train Loss = 0.4642; Validation Loss = 0.4689\n",
      "Iteration 124: Train Loss = 0.4637; Validation Loss = 0.4684\n",
      "Iteration 125: Train Loss = 0.4633; Validation Loss = 0.4680\n",
      "Iteration 126: Train Loss = 0.4630; Validation Loss = 0.4677\n",
      "Iteration 127: Train Loss = 0.4627; Validation Loss = 0.4673\n",
      "Iteration 128: Train Loss = 0.4623; Validation Loss = 0.4669\n",
      "Iteration 129: Train Loss = 0.4619; Validation Loss = 0.4665\n",
      "Iteration 130: Train Loss = 0.4615; Validation Loss = 0.4661\n",
      "Iteration 131: Train Loss = 0.4611; Validation Loss = 0.4657\n",
      "Iteration 132: Train Loss = 0.4607; Validation Loss = 0.4653\n",
      "Iteration 133: Train Loss = 0.4604; Validation Loss = 0.4650\n",
      "Iteration 134: Train Loss = 0.4601; Validation Loss = 0.4647\n",
      "Iteration 135: Train Loss = 0.4598; Validation Loss = 0.4644\n",
      "Iteration 136: Train Loss = 0.4593; Validation Loss = 0.4640\n",
      "Iteration 137: Train Loss = 0.4590; Validation Loss = 0.4637\n",
      "Iteration 138: Train Loss = 0.4587; Validation Loss = 0.4633\n",
      "Iteration 139: Train Loss = 0.4583; Validation Loss = 0.4630\n",
      "Iteration 140: Train Loss = 0.4579; Validation Loss = 0.4626\n",
      "Iteration 141: Train Loss = 0.4576; Validation Loss = 0.4622\n",
      "Iteration 142: Train Loss = 0.4573; Validation Loss = 0.4619\n",
      "Iteration 143: Train Loss = 0.4570; Validation Loss = 0.4616\n",
      "Iteration 144: Train Loss = 0.4566; Validation Loss = 0.4613\n",
      "Iteration 145: Train Loss = 0.4563; Validation Loss = 0.4609\n",
      "Iteration 146: Train Loss = 0.4560; Validation Loss = 0.4606\n",
      "Iteration 147: Train Loss = 0.4558; Validation Loss = 0.4604\n",
      "Iteration 148: Train Loss = 0.4555; Validation Loss = 0.4602\n",
      "Iteration 149: Train Loss = 0.4552; Validation Loss = 0.4599\n",
      "Iteration 150: Train Loss = 0.4550; Validation Loss = 0.4596\n",
      "Iteration 151: Train Loss = 0.4547; Validation Loss = 0.4594\n",
      "Iteration 152: Train Loss = 0.4544; Validation Loss = 0.4591\n",
      "Iteration 153: Train Loss = 0.4541; Validation Loss = 0.4588\n",
      "Iteration 154: Train Loss = 0.4539; Validation Loss = 0.4586\n",
      "Iteration 155: Train Loss = 0.4537; Validation Loss = 0.4584\n",
      "Iteration 156: Train Loss = 0.4536; Validation Loss = 0.4582\n",
      "Iteration 157: Train Loss = 0.4533; Validation Loss = 0.4580\n",
      "Iteration 158: Train Loss = 0.4531; Validation Loss = 0.4578\n",
      "Iteration 159: Train Loss = 0.4528; Validation Loss = 0.4575\n",
      "Iteration 160: Train Loss = 0.4527; Validation Loss = 0.4573\n",
      "Iteration 161: Train Loss = 0.4524; Validation Loss = 0.4571\n",
      "Iteration 162: Train Loss = 0.4522; Validation Loss = 0.4568\n",
      "Iteration 163: Train Loss = 0.4519; Validation Loss = 0.4566\n",
      "Iteration 164: Train Loss = 0.4517; Validation Loss = 0.4564\n",
      "Iteration 165: Train Loss = 0.4515; Validation Loss = 0.4561\n",
      "Iteration 166: Train Loss = 0.4513; Validation Loss = 0.4560\n",
      "Iteration 167: Train Loss = 0.4510; Validation Loss = 0.4557\n",
      "Iteration 168: Train Loss = 0.4508; Validation Loss = 0.4554\n",
      "Iteration 169: Train Loss = 0.4506; Validation Loss = 0.4553\n",
      "Iteration 170: Train Loss = 0.4504; Validation Loss = 0.4551\n",
      "Iteration 171: Train Loss = 0.4502; Validation Loss = 0.4549\n",
      "Iteration 172: Train Loss = 0.4501; Validation Loss = 0.4548\n",
      "Iteration 173: Train Loss = 0.4499; Validation Loss = 0.4546\n",
      "Iteration 174: Train Loss = 0.4497; Validation Loss = 0.4544\n",
      "Iteration 175: Train Loss = 0.4496; Validation Loss = 0.4543\n",
      "Iteration 176: Train Loss = 0.4494; Validation Loss = 0.4540\n",
      "Iteration 177: Train Loss = 0.4492; Validation Loss = 0.4538\n",
      "Iteration 178: Train Loss = 0.4490; Validation Loss = 0.4536\n",
      "Iteration 179: Train Loss = 0.4488; Validation Loss = 0.4535\n",
      "Iteration 180: Train Loss = 0.4487; Validation Loss = 0.4533\n",
      "Iteration 181: Train Loss = 0.4485; Validation Loss = 0.4532\n",
      "Iteration 182: Train Loss = 0.4484; Validation Loss = 0.4531\n",
      "Iteration 183: Train Loss = 0.4483; Validation Loss = 0.4530\n",
      "Iteration 184: Train Loss = 0.4482; Validation Loss = 0.4529\n",
      "Iteration 185: Train Loss = 0.4479; Validation Loss = 0.4527\n",
      "Iteration 186: Train Loss = 0.4478; Validation Loss = 0.4525\n",
      "Iteration 187: Train Loss = 0.4476; Validation Loss = 0.4523\n",
      "Iteration 188: Train Loss = 0.4474; Validation Loss = 0.4521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:39:23,232] Trial 47 finished with value: 0.9548787695076429 and parameters: {'max_depth': 3, 'min_samples_leaf': 2, 'n_estimators': 190, 'learning_rate': 0.021022653955736537, 'subsample': 0.7156084057520535}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 189: Train Loss = 0.4474; Validation Loss = 0.4521\n",
      "Iteration 190: Train Loss = 0.4472; Validation Loss = 0.4519\n",
      "Iteration 1: Train Loss = 0.6883; Validation Loss = 0.6885\n",
      "Iteration 2: Train Loss = 0.6837; Validation Loss = 0.6839\n",
      "Iteration 3: Train Loss = 0.6791; Validation Loss = 0.6794\n",
      "Iteration 4: Train Loss = 0.6746; Validation Loss = 0.6750\n",
      "Iteration 5: Train Loss = 0.6702; Validation Loss = 0.6707\n",
      "Iteration 6: Train Loss = 0.6658; Validation Loss = 0.6665\n",
      "Iteration 7: Train Loss = 0.6616; Validation Loss = 0.6624\n",
      "Iteration 8: Train Loss = 0.6574; Validation Loss = 0.6583\n",
      "Iteration 9: Train Loss = 0.6533; Validation Loss = 0.6543\n",
      "Iteration 10: Train Loss = 0.6493; Validation Loss = 0.6504\n",
      "Iteration 11: Train Loss = 0.6454; Validation Loss = 0.6467\n",
      "Iteration 12: Train Loss = 0.6416; Validation Loss = 0.6430\n",
      "Iteration 13: Train Loss = 0.6378; Validation Loss = 0.6393\n",
      "Iteration 14: Train Loss = 0.6341; Validation Loss = 0.6357\n",
      "Iteration 15: Train Loss = 0.6305; Validation Loss = 0.6322\n",
      "Iteration 16: Train Loss = 0.6270; Validation Loss = 0.6287\n",
      "Iteration 17: Train Loss = 0.6235; Validation Loss = 0.6253\n",
      "Iteration 18: Train Loss = 0.6201; Validation Loss = 0.6221\n",
      "Iteration 19: Train Loss = 0.6168; Validation Loss = 0.6189\n",
      "Iteration 20: Train Loss = 0.6135; Validation Loss = 0.6157\n",
      "Iteration 21: Train Loss = 0.6103; Validation Loss = 0.6126\n",
      "Iteration 22: Train Loss = 0.6072; Validation Loss = 0.6096\n",
      "Iteration 23: Train Loss = 0.6042; Validation Loss = 0.6066\n",
      "Iteration 24: Train Loss = 0.6012; Validation Loss = 0.6036\n",
      "Iteration 25: Train Loss = 0.5982; Validation Loss = 0.6007\n",
      "Iteration 26: Train Loss = 0.5953; Validation Loss = 0.5979\n",
      "Iteration 27: Train Loss = 0.5924; Validation Loss = 0.5952\n",
      "Iteration 28: Train Loss = 0.5897; Validation Loss = 0.5925\n",
      "Iteration 29: Train Loss = 0.5869; Validation Loss = 0.5898\n",
      "Iteration 30: Train Loss = 0.5842; Validation Loss = 0.5871\n",
      "Iteration 31: Train Loss = 0.5816; Validation Loss = 0.5845\n",
      "Iteration 32: Train Loss = 0.5790; Validation Loss = 0.5820\n",
      "Iteration 33: Train Loss = 0.5765; Validation Loss = 0.5795\n",
      "Iteration 34: Train Loss = 0.5739; Validation Loss = 0.5771\n",
      "Iteration 35: Train Loss = 0.5715; Validation Loss = 0.5747\n",
      "Iteration 36: Train Loss = 0.5691; Validation Loss = 0.5725\n",
      "Iteration 37: Train Loss = 0.5668; Validation Loss = 0.5702\n",
      "Iteration 38: Train Loss = 0.5645; Validation Loss = 0.5680\n",
      "Iteration 39: Train Loss = 0.5621; Validation Loss = 0.5658\n",
      "Iteration 40: Train Loss = 0.5599; Validation Loss = 0.5637\n",
      "Iteration 41: Train Loss = 0.5578; Validation Loss = 0.5616\n",
      "Iteration 42: Train Loss = 0.5556; Validation Loss = 0.5594\n",
      "Iteration 43: Train Loss = 0.5534; Validation Loss = 0.5574\n",
      "Iteration 44: Train Loss = 0.5514; Validation Loss = 0.5554\n",
      "Iteration 45: Train Loss = 0.5493; Validation Loss = 0.5534\n",
      "Iteration 46: Train Loss = 0.5473; Validation Loss = 0.5514\n",
      "Iteration 47: Train Loss = 0.5454; Validation Loss = 0.5496\n",
      "Iteration 48: Train Loss = 0.5434; Validation Loss = 0.5478\n",
      "Iteration 49: Train Loss = 0.5415; Validation Loss = 0.5459\n",
      "Iteration 50: Train Loss = 0.5396; Validation Loss = 0.5442\n",
      "Iteration 51: Train Loss = 0.5378; Validation Loss = 0.5424\n",
      "Iteration 52: Train Loss = 0.5360; Validation Loss = 0.5407\n",
      "Iteration 53: Train Loss = 0.5342; Validation Loss = 0.5389\n",
      "Iteration 54: Train Loss = 0.5325; Validation Loss = 0.5373\n",
      "Iteration 55: Train Loss = 0.5308; Validation Loss = 0.5356\n",
      "Iteration 56: Train Loss = 0.5291; Validation Loss = 0.5339\n",
      "Iteration 57: Train Loss = 0.5274; Validation Loss = 0.5323\n",
      "Iteration 58: Train Loss = 0.5258; Validation Loss = 0.5308\n",
      "Iteration 59: Train Loss = 0.5242; Validation Loss = 0.5292\n",
      "Iteration 60: Train Loss = 0.5227; Validation Loss = 0.5278\n",
      "Iteration 61: Train Loss = 0.5212; Validation Loss = 0.5263\n",
      "Iteration 62: Train Loss = 0.5197; Validation Loss = 0.5248\n",
      "Iteration 63: Train Loss = 0.5182; Validation Loss = 0.5234\n",
      "Iteration 64: Train Loss = 0.5167; Validation Loss = 0.5220\n",
      "Iteration 65: Train Loss = 0.5153; Validation Loss = 0.5206\n",
      "Iteration 66: Train Loss = 0.5139; Validation Loss = 0.5192\n",
      "Iteration 67: Train Loss = 0.5125; Validation Loss = 0.5179\n",
      "Iteration 68: Train Loss = 0.5111; Validation Loss = 0.5165\n",
      "Iteration 69: Train Loss = 0.5097; Validation Loss = 0.5152\n",
      "Iteration 70: Train Loss = 0.5083; Validation Loss = 0.5139\n",
      "Iteration 71: Train Loss = 0.5070; Validation Loss = 0.5126\n",
      "Iteration 72: Train Loss = 0.5057; Validation Loss = 0.5114\n",
      "Iteration 73: Train Loss = 0.5045; Validation Loss = 0.5101\n",
      "Iteration 74: Train Loss = 0.5032; Validation Loss = 0.5090\n",
      "Iteration 75: Train Loss = 0.5020; Validation Loss = 0.5078\n",
      "Iteration 76: Train Loss = 0.5008; Validation Loss = 0.5066\n",
      "Iteration 77: Train Loss = 0.4996; Validation Loss = 0.5055\n",
      "Iteration 78: Train Loss = 0.4984; Validation Loss = 0.5044\n",
      "Iteration 79: Train Loss = 0.4972; Validation Loss = 0.5033\n",
      "Iteration 80: Train Loss = 0.4961; Validation Loss = 0.5022\n",
      "Iteration 81: Train Loss = 0.4950; Validation Loss = 0.5011\n",
      "Iteration 82: Train Loss = 0.4939; Validation Loss = 0.5000\n",
      "Iteration 83: Train Loss = 0.4928; Validation Loss = 0.4990\n",
      "Iteration 84: Train Loss = 0.4918; Validation Loss = 0.4980\n",
      "Iteration 85: Train Loss = 0.4907; Validation Loss = 0.4970\n",
      "Iteration 86: Train Loss = 0.4897; Validation Loss = 0.4960\n",
      "Iteration 87: Train Loss = 0.4887; Validation Loss = 0.4951\n",
      "Iteration 88: Train Loss = 0.4878; Validation Loss = 0.4941\n",
      "Iteration 89: Train Loss = 0.4868; Validation Loss = 0.4932\n",
      "Iteration 90: Train Loss = 0.4858; Validation Loss = 0.4923\n",
      "Iteration 91: Train Loss = 0.4849; Validation Loss = 0.4914\n",
      "Iteration 92: Train Loss = 0.4840; Validation Loss = 0.4905\n",
      "Iteration 93: Train Loss = 0.4830; Validation Loss = 0.4896\n",
      "Iteration 94: Train Loss = 0.4822; Validation Loss = 0.4888\n",
      "Iteration 95: Train Loss = 0.4813; Validation Loss = 0.4879\n",
      "Iteration 96: Train Loss = 0.4804; Validation Loss = 0.4871\n",
      "Iteration 97: Train Loss = 0.4795; Validation Loss = 0.4862\n",
      "Iteration 98: Train Loss = 0.4786; Validation Loss = 0.4853\n",
      "Iteration 99: Train Loss = 0.4778; Validation Loss = 0.4845\n",
      "Iteration 100: Train Loss = 0.4770; Validation Loss = 0.4837\n",
      "Iteration 101: Train Loss = 0.4761; Validation Loss = 0.4829\n",
      "Iteration 102: Train Loss = 0.4753; Validation Loss = 0.4822\n",
      "Iteration 103: Train Loss = 0.4745; Validation Loss = 0.4814\n",
      "Iteration 104: Train Loss = 0.4738; Validation Loss = 0.4807\n",
      "Iteration 105: Train Loss = 0.4730; Validation Loss = 0.4799\n",
      "Iteration 106: Train Loss = 0.4723; Validation Loss = 0.4792\n",
      "Iteration 107: Train Loss = 0.4715; Validation Loss = 0.4785\n",
      "Iteration 108: Train Loss = 0.4708; Validation Loss = 0.4778\n",
      "Iteration 109: Train Loss = 0.4700; Validation Loss = 0.4772\n",
      "Iteration 110: Train Loss = 0.4693; Validation Loss = 0.4765\n",
      "Iteration 111: Train Loss = 0.4687; Validation Loss = 0.4759\n",
      "Iteration 112: Train Loss = 0.4679; Validation Loss = 0.4752\n",
      "Iteration 113: Train Loss = 0.4672; Validation Loss = 0.4746\n",
      "Iteration 114: Train Loss = 0.4665; Validation Loss = 0.4739\n",
      "Iteration 115: Train Loss = 0.4658; Validation Loss = 0.4732\n",
      "Iteration 116: Train Loss = 0.4652; Validation Loss = 0.4726\n",
      "Iteration 117: Train Loss = 0.4645; Validation Loss = 0.4720\n",
      "Iteration 118: Train Loss = 0.4639; Validation Loss = 0.4714\n",
      "Iteration 119: Train Loss = 0.4633; Validation Loss = 0.4709\n",
      "Iteration 120: Train Loss = 0.4627; Validation Loss = 0.4703\n",
      "Iteration 121: Train Loss = 0.4621; Validation Loss = 0.4697\n",
      "Iteration 122: Train Loss = 0.4615; Validation Loss = 0.4691\n",
      "Iteration 123: Train Loss = 0.4609; Validation Loss = 0.4686\n",
      "Iteration 124: Train Loss = 0.4603; Validation Loss = 0.4681\n",
      "Iteration 125: Train Loss = 0.4597; Validation Loss = 0.4676\n",
      "Iteration 126: Train Loss = 0.4592; Validation Loss = 0.4670\n",
      "Iteration 127: Train Loss = 0.4586; Validation Loss = 0.4665\n",
      "Iteration 128: Train Loss = 0.4580; Validation Loss = 0.4660\n",
      "Iteration 129: Train Loss = 0.4575; Validation Loss = 0.4655\n",
      "Iteration 130: Train Loss = 0.4569; Validation Loss = 0.4649\n",
      "Iteration 131: Train Loss = 0.4564; Validation Loss = 0.4644\n",
      "Iteration 132: Train Loss = 0.4558; Validation Loss = 0.4639\n",
      "Iteration 133: Train Loss = 0.4553; Validation Loss = 0.4634\n",
      "Iteration 134: Train Loss = 0.4548; Validation Loss = 0.4629\n",
      "Iteration 135: Train Loss = 0.4543; Validation Loss = 0.4625\n",
      "Iteration 136: Train Loss = 0.4538; Validation Loss = 0.4620\n",
      "Iteration 137: Train Loss = 0.4533; Validation Loss = 0.4615\n",
      "Iteration 138: Train Loss = 0.4528; Validation Loss = 0.4611\n",
      "Iteration 139: Train Loss = 0.4523; Validation Loss = 0.4606\n",
      "Iteration 140: Train Loss = 0.4518; Validation Loss = 0.4602\n",
      "Iteration 141: Train Loss = 0.4513; Validation Loss = 0.4597\n",
      "Iteration 142: Train Loss = 0.4509; Validation Loss = 0.4592\n",
      "Iteration 143: Train Loss = 0.4504; Validation Loss = 0.4588\n",
      "Iteration 144: Train Loss = 0.4500; Validation Loss = 0.4584\n",
      "Iteration 145: Train Loss = 0.4495; Validation Loss = 0.4580\n",
      "Iteration 146: Train Loss = 0.4491; Validation Loss = 0.4575\n",
      "Iteration 147: Train Loss = 0.4486; Validation Loss = 0.4571\n",
      "Iteration 148: Train Loss = 0.4482; Validation Loss = 0.4568\n",
      "Iteration 149: Train Loss = 0.4478; Validation Loss = 0.4563\n",
      "Iteration 150: Train Loss = 0.4473; Validation Loss = 0.4559\n",
      "Iteration 151: Train Loss = 0.4468; Validation Loss = 0.4555\n",
      "Iteration 152: Train Loss = 0.4464; Validation Loss = 0.4551\n",
      "Iteration 153: Train Loss = 0.4459; Validation Loss = 0.4546\n",
      "Iteration 154: Train Loss = 0.4456; Validation Loss = 0.4543\n",
      "Iteration 155: Train Loss = 0.4452; Validation Loss = 0.4539\n",
      "Iteration 156: Train Loss = 0.4448; Validation Loss = 0.4536\n",
      "Iteration 157: Train Loss = 0.4444; Validation Loss = 0.4532\n",
      "Iteration 158: Train Loss = 0.4441; Validation Loss = 0.4530\n",
      "Iteration 159: Train Loss = 0.4437; Validation Loss = 0.4526\n",
      "Iteration 160: Train Loss = 0.4434; Validation Loss = 0.4523\n",
      "Iteration 161: Train Loss = 0.4431; Validation Loss = 0.4520\n",
      "Iteration 162: Train Loss = 0.4427; Validation Loss = 0.4516\n",
      "Iteration 163: Train Loss = 0.4424; Validation Loss = 0.4514\n",
      "Iteration 164: Train Loss = 0.4420; Validation Loss = 0.4511\n",
      "Iteration 165: Train Loss = 0.4417; Validation Loss = 0.4507\n",
      "Iteration 166: Train Loss = 0.4413; Validation Loss = 0.4504\n",
      "Iteration 167: Train Loss = 0.4410; Validation Loss = 0.4501\n",
      "Iteration 168: Train Loss = 0.4407; Validation Loss = 0.4498\n",
      "Iteration 169: Train Loss = 0.4404; Validation Loss = 0.4495\n",
      "Iteration 170: Train Loss = 0.4401; Validation Loss = 0.4492\n",
      "Iteration 171: Train Loss = 0.4398; Validation Loss = 0.4490\n",
      "Iteration 172: Train Loss = 0.4395; Validation Loss = 0.4487\n",
      "Iteration 173: Train Loss = 0.4392; Validation Loss = 0.4484\n",
      "Iteration 174: Train Loss = 0.4389; Validation Loss = 0.4481\n",
      "Iteration 175: Train Loss = 0.4385; Validation Loss = 0.4478\n",
      "Iteration 176: Train Loss = 0.4382; Validation Loss = 0.4476\n",
      "Iteration 177: Train Loss = 0.4379; Validation Loss = 0.4473\n",
      "Iteration 178: Train Loss = 0.4377; Validation Loss = 0.4471\n",
      "Iteration 179: Train Loss = 0.4374; Validation Loss = 0.4468\n",
      "Iteration 180: Train Loss = 0.4371; Validation Loss = 0.4466\n",
      "Iteration 181: Train Loss = 0.4368; Validation Loss = 0.4463\n",
      "Iteration 182: Train Loss = 0.4365; Validation Loss = 0.4461\n",
      "Iteration 183: Train Loss = 0.4362; Validation Loss = 0.4458\n",
      "Iteration 184: Train Loss = 0.4360; Validation Loss = 0.4456\n",
      "Iteration 185: Train Loss = 0.4357; Validation Loss = 0.4453\n",
      "Iteration 186: Train Loss = 0.4354; Validation Loss = 0.4450\n",
      "Iteration 187: Train Loss = 0.4351; Validation Loss = 0.4448\n",
      "Iteration 188: Train Loss = 0.4348; Validation Loss = 0.4445\n",
      "Iteration 189: Train Loss = 0.4345; Validation Loss = 0.4443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:39:44,643] Trial 48 finished with value: 0.962992195036394 and parameters: {'max_depth': 6, 'min_samples_leaf': 4, 'n_estimators': 190, 'learning_rate': 0.014401572473367343, 'subsample': 0.7609744822137375}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190: Train Loss = 0.4342; Validation Loss = 0.4440\n",
      "Iteration 1: Train Loss = 0.6900; Validation Loss = 0.6901\n",
      "Iteration 2: Train Loss = 0.6870; Validation Loss = 0.6871\n",
      "Iteration 3: Train Loss = 0.6840; Validation Loss = 0.6842\n",
      "Iteration 4: Train Loss = 0.6810; Validation Loss = 0.6813\n",
      "Iteration 5: Train Loss = 0.6781; Validation Loss = 0.6785\n",
      "Iteration 6: Train Loss = 0.6752; Validation Loss = 0.6756\n",
      "Iteration 7: Train Loss = 0.6723; Validation Loss = 0.6728\n",
      "Iteration 8: Train Loss = 0.6695; Validation Loss = 0.6701\n",
      "Iteration 9: Train Loss = 0.6667; Validation Loss = 0.6674\n",
      "Iteration 10: Train Loss = 0.6639; Validation Loss = 0.6647\n",
      "Iteration 11: Train Loss = 0.6612; Validation Loss = 0.6620\n",
      "Iteration 12: Train Loss = 0.6586; Validation Loss = 0.6594\n",
      "Iteration 13: Train Loss = 0.6559; Validation Loss = 0.6568\n",
      "Iteration 14: Train Loss = 0.6533; Validation Loss = 0.6542\n",
      "Iteration 15: Train Loss = 0.6507; Validation Loss = 0.6517\n",
      "Iteration 16: Train Loss = 0.6481; Validation Loss = 0.6492\n",
      "Iteration 17: Train Loss = 0.6456; Validation Loss = 0.6468\n",
      "Iteration 18: Train Loss = 0.6432; Validation Loss = 0.6444\n",
      "Iteration 19: Train Loss = 0.6408; Validation Loss = 0.6421\n",
      "Iteration 20: Train Loss = 0.6384; Validation Loss = 0.6398\n",
      "Iteration 21: Train Loss = 0.6360; Validation Loss = 0.6375\n",
      "Iteration 22: Train Loss = 0.6337; Validation Loss = 0.6352\n",
      "Iteration 23: Train Loss = 0.6313; Validation Loss = 0.6329\n",
      "Iteration 24: Train Loss = 0.6291; Validation Loss = 0.6307\n",
      "Iteration 25: Train Loss = 0.6268; Validation Loss = 0.6285\n",
      "Iteration 26: Train Loss = 0.6246; Validation Loss = 0.6263\n",
      "Iteration 27: Train Loss = 0.6224; Validation Loss = 0.6242\n",
      "Iteration 28: Train Loss = 0.6203; Validation Loss = 0.6221\n",
      "Iteration 29: Train Loss = 0.6181; Validation Loss = 0.6201\n",
      "Iteration 30: Train Loss = 0.6160; Validation Loss = 0.6180\n",
      "Iteration 31: Train Loss = 0.6139; Validation Loss = 0.6160\n",
      "Iteration 32: Train Loss = 0.6119; Validation Loss = 0.6140\n",
      "Iteration 33: Train Loss = 0.6098; Validation Loss = 0.6120\n",
      "Iteration 34: Train Loss = 0.6078; Validation Loss = 0.6100\n",
      "Iteration 35: Train Loss = 0.6058; Validation Loss = 0.6081\n",
      "Iteration 36: Train Loss = 0.6038; Validation Loss = 0.6062\n",
      "Iteration 37: Train Loss = 0.6019; Validation Loss = 0.6043\n",
      "Iteration 38: Train Loss = 0.6000; Validation Loss = 0.6024\n",
      "Iteration 39: Train Loss = 0.5981; Validation Loss = 0.6006\n",
      "Iteration 40: Train Loss = 0.5962; Validation Loss = 0.5988\n",
      "Iteration 41: Train Loss = 0.5943; Validation Loss = 0.5969\n",
      "Iteration 42: Train Loss = 0.5925; Validation Loss = 0.5952\n",
      "Iteration 43: Train Loss = 0.5907; Validation Loss = 0.5934\n",
      "Iteration 44: Train Loss = 0.5889; Validation Loss = 0.5917\n",
      "Iteration 45: Train Loss = 0.5872; Validation Loss = 0.5900\n",
      "Iteration 46: Train Loss = 0.5854; Validation Loss = 0.5883\n",
      "Iteration 47: Train Loss = 0.5837; Validation Loss = 0.5867\n",
      "Iteration 48: Train Loss = 0.5820; Validation Loss = 0.5850\n",
      "Iteration 49: Train Loss = 0.5803; Validation Loss = 0.5834\n",
      "Iteration 50: Train Loss = 0.5787; Validation Loss = 0.5818\n",
      "Iteration 51: Train Loss = 0.5771; Validation Loss = 0.5802\n",
      "Iteration 52: Train Loss = 0.5754; Validation Loss = 0.5787\n",
      "Iteration 53: Train Loss = 0.5739; Validation Loss = 0.5771\n",
      "Iteration 54: Train Loss = 0.5723; Validation Loss = 0.5756\n",
      "Iteration 55: Train Loss = 0.5707; Validation Loss = 0.5741\n",
      "Iteration 56: Train Loss = 0.5692; Validation Loss = 0.5726\n",
      "Iteration 57: Train Loss = 0.5677; Validation Loss = 0.5711\n",
      "Iteration 58: Train Loss = 0.5661; Validation Loss = 0.5697\n",
      "Iteration 59: Train Loss = 0.5646; Validation Loss = 0.5682\n",
      "Iteration 60: Train Loss = 0.5632; Validation Loss = 0.5668\n",
      "Iteration 61: Train Loss = 0.5617; Validation Loss = 0.5654\n",
      "Iteration 62: Train Loss = 0.5603; Validation Loss = 0.5640\n",
      "Iteration 63: Train Loss = 0.5589; Validation Loss = 0.5627\n",
      "Iteration 64: Train Loss = 0.5575; Validation Loss = 0.5614\n",
      "Iteration 65: Train Loss = 0.5561; Validation Loss = 0.5601\n",
      "Iteration 66: Train Loss = 0.5547; Validation Loss = 0.5587\n",
      "Iteration 67: Train Loss = 0.5533; Validation Loss = 0.5574\n",
      "Iteration 68: Train Loss = 0.5520; Validation Loss = 0.5561\n",
      "Iteration 69: Train Loss = 0.5506; Validation Loss = 0.5548\n",
      "Iteration 70: Train Loss = 0.5493; Validation Loss = 0.5536\n",
      "Iteration 71: Train Loss = 0.5480; Validation Loss = 0.5523\n",
      "Iteration 72: Train Loss = 0.5467; Validation Loss = 0.5511\n",
      "Iteration 73: Train Loss = 0.5455; Validation Loss = 0.5499\n",
      "Iteration 74: Train Loss = 0.5443; Validation Loss = 0.5487\n",
      "Iteration 75: Train Loss = 0.5430; Validation Loss = 0.5475\n",
      "Iteration 76: Train Loss = 0.5418; Validation Loss = 0.5464\n",
      "Iteration 77: Train Loss = 0.5406; Validation Loss = 0.5452\n",
      "Iteration 78: Train Loss = 0.5394; Validation Loss = 0.5441\n",
      "Iteration 79: Train Loss = 0.5382; Validation Loss = 0.5430\n",
      "Iteration 80: Train Loss = 0.5371; Validation Loss = 0.5419\n",
      "Iteration 81: Train Loss = 0.5359; Validation Loss = 0.5407\n",
      "Iteration 82: Train Loss = 0.5347; Validation Loss = 0.5396\n",
      "Iteration 83: Train Loss = 0.5336; Validation Loss = 0.5385\n",
      "Iteration 84: Train Loss = 0.5325; Validation Loss = 0.5375\n",
      "Iteration 85: Train Loss = 0.5314; Validation Loss = 0.5365\n",
      "Iteration 86: Train Loss = 0.5304; Validation Loss = 0.5354\n",
      "Iteration 87: Train Loss = 0.5293; Validation Loss = 0.5344\n",
      "Iteration 88: Train Loss = 0.5283; Validation Loss = 0.5334\n",
      "Iteration 89: Train Loss = 0.5272; Validation Loss = 0.5324\n",
      "Iteration 90: Train Loss = 0.5262; Validation Loss = 0.5314\n",
      "Iteration 91: Train Loss = 0.5251; Validation Loss = 0.5304\n",
      "Iteration 92: Train Loss = 0.5241; Validation Loss = 0.5294\n",
      "Iteration 93: Train Loss = 0.5231; Validation Loss = 0.5284\n",
      "Iteration 94: Train Loss = 0.5221; Validation Loss = 0.5274\n",
      "Iteration 95: Train Loss = 0.5211; Validation Loss = 0.5265\n",
      "Iteration 96: Train Loss = 0.5201; Validation Loss = 0.5256\n",
      "Iteration 97: Train Loss = 0.5191; Validation Loss = 0.5246\n",
      "Iteration 98: Train Loss = 0.5182; Validation Loss = 0.5237\n",
      "Iteration 99: Train Loss = 0.5172; Validation Loss = 0.5228\n",
      "Iteration 100: Train Loss = 0.5163; Validation Loss = 0.5219\n",
      "Iteration 101: Train Loss = 0.5154; Validation Loss = 0.5210\n",
      "Iteration 102: Train Loss = 0.5144; Validation Loss = 0.5201\n",
      "Iteration 103: Train Loss = 0.5135; Validation Loss = 0.5192\n",
      "Iteration 104: Train Loss = 0.5126; Validation Loss = 0.5183\n",
      "Iteration 105: Train Loss = 0.5117; Validation Loss = 0.5175\n",
      "Iteration 106: Train Loss = 0.5108; Validation Loss = 0.5166\n",
      "Iteration 107: Train Loss = 0.5099; Validation Loss = 0.5158\n",
      "Iteration 108: Train Loss = 0.5091; Validation Loss = 0.5150\n",
      "Iteration 109: Train Loss = 0.5082; Validation Loss = 0.5141\n",
      "Iteration 110: Train Loss = 0.5074; Validation Loss = 0.5133\n",
      "Iteration 111: Train Loss = 0.5065; Validation Loss = 0.5125\n",
      "Iteration 112: Train Loss = 0.5057; Validation Loss = 0.5117\n",
      "Iteration 113: Train Loss = 0.5049; Validation Loss = 0.5109\n",
      "Iteration 114: Train Loss = 0.5041; Validation Loss = 0.5102\n",
      "Iteration 115: Train Loss = 0.5033; Validation Loss = 0.5094\n",
      "Iteration 116: Train Loss = 0.5025; Validation Loss = 0.5086\n",
      "Iteration 117: Train Loss = 0.5018; Validation Loss = 0.5079\n",
      "Iteration 118: Train Loss = 0.5010; Validation Loss = 0.5071\n",
      "Iteration 119: Train Loss = 0.5002; Validation Loss = 0.5064\n",
      "Iteration 120: Train Loss = 0.4994; Validation Loss = 0.5057\n",
      "Iteration 121: Train Loss = 0.4987; Validation Loss = 0.5049\n",
      "Iteration 122: Train Loss = 0.4980; Validation Loss = 0.5043\n",
      "Iteration 123: Train Loss = 0.4972; Validation Loss = 0.5036\n",
      "Iteration 124: Train Loss = 0.4965; Validation Loss = 0.5029\n",
      "Iteration 125: Train Loss = 0.4958; Validation Loss = 0.5022\n",
      "Iteration 126: Train Loss = 0.4951; Validation Loss = 0.5015\n",
      "Iteration 127: Train Loss = 0.4943; Validation Loss = 0.5008\n",
      "Iteration 128: Train Loss = 0.4936; Validation Loss = 0.5002\n",
      "Iteration 129: Train Loss = 0.4929; Validation Loss = 0.4995\n",
      "Iteration 130: Train Loss = 0.4922; Validation Loss = 0.4989\n",
      "Iteration 131: Train Loss = 0.4916; Validation Loss = 0.4982\n",
      "Iteration 132: Train Loss = 0.4909; Validation Loss = 0.4976\n",
      "Iteration 133: Train Loss = 0.4902; Validation Loss = 0.4969\n",
      "Iteration 134: Train Loss = 0.4895; Validation Loss = 0.4963\n",
      "Iteration 135: Train Loss = 0.4889; Validation Loss = 0.4957\n",
      "Iteration 136: Train Loss = 0.4883; Validation Loss = 0.4951\n",
      "Iteration 137: Train Loss = 0.4876; Validation Loss = 0.4945\n",
      "Iteration 138: Train Loss = 0.4870; Validation Loss = 0.4939\n",
      "Iteration 139: Train Loss = 0.4864; Validation Loss = 0.4932\n",
      "Iteration 140: Train Loss = 0.4857; Validation Loss = 0.4927\n",
      "Iteration 141: Train Loss = 0.4851; Validation Loss = 0.4921\n",
      "Iteration 142: Train Loss = 0.4845; Validation Loss = 0.4915\n",
      "Iteration 143: Train Loss = 0.4839; Validation Loss = 0.4909\n",
      "Iteration 144: Train Loss = 0.4833; Validation Loss = 0.4903\n",
      "Iteration 145: Train Loss = 0.4827; Validation Loss = 0.4898\n",
      "Iteration 146: Train Loss = 0.4821; Validation Loss = 0.4892\n",
      "Iteration 147: Train Loss = 0.4816; Validation Loss = 0.4887\n",
      "Iteration 148: Train Loss = 0.4810; Validation Loss = 0.4881\n",
      "Iteration 149: Train Loss = 0.4804; Validation Loss = 0.4876\n",
      "Iteration 150: Train Loss = 0.4799; Validation Loss = 0.4871\n",
      "Iteration 151: Train Loss = 0.4793; Validation Loss = 0.4865\n",
      "Iteration 152: Train Loss = 0.4788; Validation Loss = 0.4860\n",
      "Iteration 153: Train Loss = 0.4782; Validation Loss = 0.4855\n",
      "Iteration 154: Train Loss = 0.4777; Validation Loss = 0.4850\n",
      "Iteration 155: Train Loss = 0.4772; Validation Loss = 0.4845\n",
      "Iteration 156: Train Loss = 0.4766; Validation Loss = 0.4840\n",
      "Iteration 157: Train Loss = 0.4761; Validation Loss = 0.4835\n",
      "Iteration 158: Train Loss = 0.4756; Validation Loss = 0.4830\n",
      "Iteration 159: Train Loss = 0.4751; Validation Loss = 0.4826\n",
      "Iteration 160: Train Loss = 0.4745; Validation Loss = 0.4821\n",
      "Iteration 161: Train Loss = 0.4740; Validation Loss = 0.4816\n",
      "Iteration 162: Train Loss = 0.4735; Validation Loss = 0.4811\n",
      "Iteration 163: Train Loss = 0.4730; Validation Loss = 0.4806\n",
      "Iteration 164: Train Loss = 0.4725; Validation Loss = 0.4801\n",
      "Iteration 165: Train Loss = 0.4720; Validation Loss = 0.4797\n",
      "Iteration 166: Train Loss = 0.4715; Validation Loss = 0.4792\n",
      "Iteration 167: Train Loss = 0.4710; Validation Loss = 0.4787\n",
      "Iteration 168: Train Loss = 0.4706; Validation Loss = 0.4783\n",
      "Iteration 169: Train Loss = 0.4701; Validation Loss = 0.4779\n",
      "Iteration 170: Train Loss = 0.4696; Validation Loss = 0.4774\n",
      "Iteration 171: Train Loss = 0.4692; Validation Loss = 0.4770\n",
      "Iteration 172: Train Loss = 0.4687; Validation Loss = 0.4766\n",
      "Iteration 173: Train Loss = 0.4682; Validation Loss = 0.4761\n",
      "Iteration 174: Train Loss = 0.4678; Validation Loss = 0.4757\n",
      "Iteration 175: Train Loss = 0.4673; Validation Loss = 0.4753\n",
      "Iteration 176: Train Loss = 0.4669; Validation Loss = 0.4749\n",
      "Iteration 177: Train Loss = 0.4664; Validation Loss = 0.4744\n",
      "Iteration 178: Train Loss = 0.4660; Validation Loss = 0.4740\n",
      "Iteration 179: Train Loss = 0.4656; Validation Loss = 0.4737\n",
      "Iteration 180: Train Loss = 0.4652; Validation Loss = 0.4733\n",
      "Iteration 181: Train Loss = 0.4648; Validation Loss = 0.4729\n",
      "Iteration 182: Train Loss = 0.4644; Validation Loss = 0.4725\n",
      "Iteration 183: Train Loss = 0.4640; Validation Loss = 0.4721\n",
      "Iteration 184: Train Loss = 0.4636; Validation Loss = 0.4717\n",
      "Iteration 185: Train Loss = 0.4631; Validation Loss = 0.4713\n",
      "Iteration 186: Train Loss = 0.4627; Validation Loss = 0.4709\n",
      "Iteration 187: Train Loss = 0.4623; Validation Loss = 0.4705\n",
      "Iteration 188: Train Loss = 0.4619; Validation Loss = 0.4702\n",
      "Iteration 189: Train Loss = 0.4615; Validation Loss = 0.4698\n",
      "Iteration 190: Train Loss = 0.4612; Validation Loss = 0.4695\n",
      "Iteration 191: Train Loss = 0.4608; Validation Loss = 0.4691\n",
      "Iteration 192: Train Loss = 0.4604; Validation Loss = 0.4687\n",
      "Iteration 193: Train Loss = 0.4600; Validation Loss = 0.4684\n",
      "Iteration 194: Train Loss = 0.4596; Validation Loss = 0.4680\n",
      "Iteration 195: Train Loss = 0.4593; Validation Loss = 0.4677\n",
      "Iteration 196: Train Loss = 0.4589; Validation Loss = 0.4673\n",
      "Iteration 197: Train Loss = 0.4585; Validation Loss = 0.4670\n",
      "Iteration 198: Train Loss = 0.4581; Validation Loss = 0.4666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 00:40:04,770] Trial 49 finished with value: 0.9604173577510112 and parameters: {'max_depth': 6, 'min_samples_leaf': 4, 'n_estimators': 200, 'learning_rate': 0.009304689075338247, 'subsample': 0.6661965667760874}. Best is trial 7 with value: 0.9650713580331645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199: Train Loss = 0.4578; Validation Loss = 0.4663\n",
      "Iteration 200: Train Loss = 0.4574; Validation Loss = 0.4660\n",
      "Лучшие параметры: {'max_depth': 8, 'min_samples_leaf': 5, 'n_estimators': 200, 'learning_rate': 0.031227245340080506, 'subsample': 0.9780260989099555}\n",
      "Параметры:\n",
      "    max_depth: 8\n",
      "    min_samples_leaf: 5\n",
      "    n_estimators: 200\n",
      "    learning_rate: 0.031227245340080506\n",
      "    subsample: 0.9780260989099555\n"
     ]
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    base_model_params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    boosting_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200, step=10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'early_stopping_rounds': 10,\n",
    "    }\n",
    "\n",
    "    model = Boosting(base_model_params=base_model_params, **boosting_params, plot=False)\n",
    "\n",
    "    model.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "    valid_auc = model.score(x_valid, y_valid)\n",
    "\n",
    "    return valid_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "\n",
    "print(\"Параметры:\")\n",
    "for key, val in best_params.items():\n",
    "    print(f\"    {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGk-Wt_slWlV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 4. Интерпретация градиентного бустинга [1 балл]\n",
    "\n",
    "Постройте калибровочную кривую для вашей лучшей модели градиентного бустинга и оцените, насколько точно модель предсказывает вероятности.\n",
    "\n",
    "**Инструкция:**\n",
    "1. Постройте калибровочную кривую для лучшей модели градиентного бустинга.\n",
    "2. Постройте аналогичную кривую для логистической регрессии.\n",
    "3. Сравните полученные результаты и проанализируйте, насколько хорошо каждая модель оценивает вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LdJG4bHClWlV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "best_params = {\n",
    "    \"max_depth\": 7,\n",
    "    \"min_samples_leaf\": 5,\n",
    "    \"n_estimators\": 160,\n",
    "    \"learning_rate\": 0.05140680815313123,\n",
    "    \"subsample\": 0.8297819104403557\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6756; Validation Loss = 0.6762\n",
      "Iteration 2: Train Loss = 0.6594; Validation Loss = 0.6607\n",
      "Iteration 3: Train Loss = 0.6444; Validation Loss = 0.6462\n",
      "Iteration 4: Train Loss = 0.6306; Validation Loss = 0.6330\n",
      "Iteration 5: Train Loss = 0.6177; Validation Loss = 0.6208\n",
      "Iteration 6: Train Loss = 0.6057; Validation Loss = 0.6091\n",
      "Iteration 7: Train Loss = 0.5945; Validation Loss = 0.5983\n",
      "Iteration 8: Train Loss = 0.5840; Validation Loss = 0.5882\n",
      "Iteration 9: Train Loss = 0.5741; Validation Loss = 0.5787\n",
      "Iteration 10: Train Loss = 0.5650; Validation Loss = 0.5699\n",
      "Iteration 11: Train Loss = 0.5562; Validation Loss = 0.5617\n",
      "Iteration 12: Train Loss = 0.5483; Validation Loss = 0.5540\n",
      "Iteration 13: Train Loss = 0.5409; Validation Loss = 0.5469\n",
      "Iteration 14: Train Loss = 0.5339; Validation Loss = 0.5403\n",
      "Iteration 15: Train Loss = 0.5273; Validation Loss = 0.5339\n",
      "Iteration 16: Train Loss = 0.5212; Validation Loss = 0.5281\n",
      "Iteration 17: Train Loss = 0.5154; Validation Loss = 0.5227\n",
      "Iteration 18: Train Loss = 0.5098; Validation Loss = 0.5175\n",
      "Iteration 19: Train Loss = 0.5045; Validation Loss = 0.5123\n",
      "Iteration 20: Train Loss = 0.4997; Validation Loss = 0.5078\n",
      "Iteration 21: Train Loss = 0.4952; Validation Loss = 0.5035\n",
      "Iteration 22: Train Loss = 0.4907; Validation Loss = 0.4993\n",
      "Iteration 23: Train Loss = 0.4867; Validation Loss = 0.4955\n",
      "Iteration 24: Train Loss = 0.4829; Validation Loss = 0.4920\n",
      "Iteration 25: Train Loss = 0.4793; Validation Loss = 0.4886\n",
      "Iteration 26: Train Loss = 0.4759; Validation Loss = 0.4855\n",
      "Iteration 27: Train Loss = 0.4727; Validation Loss = 0.4824\n",
      "Iteration 28: Train Loss = 0.4696; Validation Loss = 0.4797\n",
      "Iteration 29: Train Loss = 0.4668; Validation Loss = 0.4770\n",
      "Iteration 30: Train Loss = 0.4640; Validation Loss = 0.4745\n",
      "Iteration 31: Train Loss = 0.4615; Validation Loss = 0.4722\n",
      "Iteration 32: Train Loss = 0.4590; Validation Loss = 0.4699\n",
      "Iteration 33: Train Loss = 0.4564; Validation Loss = 0.4675\n",
      "Iteration 34: Train Loss = 0.4542; Validation Loss = 0.4653\n",
      "Iteration 35: Train Loss = 0.4520; Validation Loss = 0.4633\n",
      "Iteration 36: Train Loss = 0.4498; Validation Loss = 0.4612\n",
      "Iteration 37: Train Loss = 0.4479; Validation Loss = 0.4595\n",
      "Iteration 38: Train Loss = 0.4459; Validation Loss = 0.4577\n",
      "Iteration 39: Train Loss = 0.4443; Validation Loss = 0.4561\n",
      "Iteration 40: Train Loss = 0.4426; Validation Loss = 0.4546\n",
      "Iteration 41: Train Loss = 0.4411; Validation Loss = 0.4533\n",
      "Iteration 42: Train Loss = 0.4396; Validation Loss = 0.4518\n",
      "Iteration 43: Train Loss = 0.4383; Validation Loss = 0.4507\n",
      "Iteration 44: Train Loss = 0.4369; Validation Loss = 0.4495\n",
      "Iteration 45: Train Loss = 0.4355; Validation Loss = 0.4483\n",
      "Iteration 46: Train Loss = 0.4344; Validation Loss = 0.4473\n",
      "Iteration 47: Train Loss = 0.4332; Validation Loss = 0.4463\n",
      "Iteration 48: Train Loss = 0.4321; Validation Loss = 0.4453\n",
      "Iteration 49: Train Loss = 0.4310; Validation Loss = 0.4445\n",
      "Iteration 50: Train Loss = 0.4300; Validation Loss = 0.4435\n",
      "Iteration 51: Train Loss = 0.4290; Validation Loss = 0.4426\n",
      "Iteration 52: Train Loss = 0.4281; Validation Loss = 0.4418\n",
      "Iteration 53: Train Loss = 0.4271; Validation Loss = 0.4409\n",
      "Iteration 54: Train Loss = 0.4262; Validation Loss = 0.4402\n",
      "Iteration 55: Train Loss = 0.4254; Validation Loss = 0.4394\n",
      "Iteration 56: Train Loss = 0.4246; Validation Loss = 0.4387\n",
      "Iteration 57: Train Loss = 0.4239; Validation Loss = 0.4381\n",
      "Iteration 58: Train Loss = 0.4232; Validation Loss = 0.4376\n",
      "Iteration 59: Train Loss = 0.4225; Validation Loss = 0.4370\n",
      "Iteration 60: Train Loss = 0.4218; Validation Loss = 0.4364\n",
      "Iteration 61: Train Loss = 0.4213; Validation Loss = 0.4359\n",
      "Iteration 62: Train Loss = 0.4207; Validation Loss = 0.4354\n",
      "Iteration 63: Train Loss = 0.4202; Validation Loss = 0.4349\n",
      "Iteration 64: Train Loss = 0.4196; Validation Loss = 0.4345\n",
      "Iteration 65: Train Loss = 0.4191; Validation Loss = 0.4342\n",
      "Iteration 66: Train Loss = 0.4185; Validation Loss = 0.4336\n",
      "Iteration 67: Train Loss = 0.4178; Validation Loss = 0.4331\n",
      "Iteration 68: Train Loss = 0.4173; Validation Loss = 0.4328\n",
      "Iteration 69: Train Loss = 0.4168; Validation Loss = 0.4324\n",
      "Iteration 70: Train Loss = 0.4162; Validation Loss = 0.4319\n",
      "Iteration 71: Train Loss = 0.4157; Validation Loss = 0.4316\n",
      "Iteration 72: Train Loss = 0.4154; Validation Loss = 0.4313\n",
      "Iteration 73: Train Loss = 0.4151; Validation Loss = 0.4311\n",
      "Iteration 74: Train Loss = 0.4148; Validation Loss = 0.4309\n",
      "Iteration 75: Train Loss = 0.4143; Validation Loss = 0.4305\n",
      "Iteration 76: Train Loss = 0.4139; Validation Loss = 0.4301\n",
      "Iteration 77: Train Loss = 0.4134; Validation Loss = 0.4296\n",
      "Iteration 78: Train Loss = 0.4130; Validation Loss = 0.4294\n",
      "Iteration 79: Train Loss = 0.4127; Validation Loss = 0.4292\n",
      "Iteration 80: Train Loss = 0.4123; Validation Loss = 0.4288\n",
      "Iteration 81: Train Loss = 0.4119; Validation Loss = 0.4284\n",
      "Iteration 82: Train Loss = 0.4116; Validation Loss = 0.4281\n",
      "Iteration 83: Train Loss = 0.4113; Validation Loss = 0.4279\n",
      "Iteration 84: Train Loss = 0.4110; Validation Loss = 0.4277\n",
      "Iteration 85: Train Loss = 0.4108; Validation Loss = 0.4274\n",
      "Iteration 86: Train Loss = 0.4107; Validation Loss = 0.4273\n",
      "Iteration 87: Train Loss = 0.4104; Validation Loss = 0.4270\n",
      "Iteration 88: Train Loss = 0.4101; Validation Loss = 0.4269\n",
      "Iteration 89: Train Loss = 0.4100; Validation Loss = 0.4269\n",
      "Iteration 90: Train Loss = 0.4098; Validation Loss = 0.4267\n",
      "Iteration 91: Train Loss = 0.4096; Validation Loss = 0.4266\n",
      "Iteration 92: Train Loss = 0.4093; Validation Loss = 0.4263\n",
      "Iteration 93: Train Loss = 0.4091; Validation Loss = 0.4262\n",
      "Iteration 94: Train Loss = 0.4090; Validation Loss = 0.4261\n",
      "Iteration 95: Train Loss = 0.4089; Validation Loss = 0.4261\n",
      "Iteration 96: Train Loss = 0.4087; Validation Loss = 0.4258\n",
      "Iteration 97: Train Loss = 0.4085; Validation Loss = 0.4257\n",
      "Iteration 98: Train Loss = 0.4084; Validation Loss = 0.4256\n",
      "Iteration 99: Train Loss = 0.4082; Validation Loss = 0.4255\n",
      "Iteration 100: Train Loss = 0.4081; Validation Loss = 0.4254\n",
      "Iteration 101: Train Loss = 0.4079; Validation Loss = 0.4253\n",
      "Iteration 102: Train Loss = 0.4078; Validation Loss = 0.4252\n",
      "Iteration 103: Train Loss = 0.4076; Validation Loss = 0.4251\n",
      "Iteration 104: Train Loss = 0.4075; Validation Loss = 0.4251\n",
      "Iteration 105: Train Loss = 0.4074; Validation Loss = 0.4250\n",
      "Iteration 106: Train Loss = 0.4073; Validation Loss = 0.4249\n",
      "Iteration 107: Train Loss = 0.4072; Validation Loss = 0.4248\n",
      "Iteration 108: Train Loss = 0.4071; Validation Loss = 0.4247\n",
      "Iteration 109: Train Loss = 0.4068; Validation Loss = 0.4245\n",
      "Iteration 110: Train Loss = 0.4067; Validation Loss = 0.4244\n",
      "Iteration 111: Train Loss = 0.4066; Validation Loss = 0.4244\n",
      "Iteration 112: Train Loss = 0.4065; Validation Loss = 0.4242\n",
      "Iteration 113: Train Loss = 0.4063; Validation Loss = 0.4240\n",
      "Iteration 114: Train Loss = 0.4062; Validation Loss = 0.4240\n",
      "Iteration 115: Train Loss = 0.4061; Validation Loss = 0.4239\n",
      "Iteration 116: Train Loss = 0.4060; Validation Loss = 0.4239\n",
      "Iteration 117: Train Loss = 0.4059; Validation Loss = 0.4238\n",
      "Iteration 118: Train Loss = 0.4057; Validation Loss = 0.4237\n",
      "Iteration 119: Train Loss = 0.4056; Validation Loss = 0.4237\n",
      "Iteration 120: Train Loss = 0.4055; Validation Loss = 0.4236\n",
      "Iteration 121: Train Loss = 0.4054; Validation Loss = 0.4235\n",
      "Iteration 122: Train Loss = 0.4053; Validation Loss = 0.4234\n",
      "Iteration 123: Train Loss = 0.4052; Validation Loss = 0.4234\n",
      "Iteration 124: Train Loss = 0.4051; Validation Loss = 0.4233\n",
      "Iteration 125: Train Loss = 0.4050; Validation Loss = 0.4232\n",
      "Iteration 126: Train Loss = 0.4048; Validation Loss = 0.4231\n",
      "Iteration 127: Train Loss = 0.4047; Validation Loss = 0.4230\n",
      "Iteration 128: Train Loss = 0.4046; Validation Loss = 0.4229\n",
      "Iteration 129: Train Loss = 0.4045; Validation Loss = 0.4229\n",
      "Iteration 130: Train Loss = 0.4044; Validation Loss = 0.4228\n",
      "Iteration 131: Train Loss = 0.4042; Validation Loss = 0.4228\n",
      "Iteration 132: Train Loss = 0.4042; Validation Loss = 0.4227\n",
      "Iteration 133: Train Loss = 0.4041; Validation Loss = 0.4227\n",
      "Iteration 134: Train Loss = 0.4039; Validation Loss = 0.4226\n",
      "Iteration 135: Train Loss = 0.4038; Validation Loss = 0.4225\n",
      "Iteration 136: Train Loss = 0.4037; Validation Loss = 0.4224\n",
      "Iteration 137: Train Loss = 0.4037; Validation Loss = 0.4224\n",
      "Iteration 138: Train Loss = 0.4036; Validation Loss = 0.4225\n",
      "Iteration 139: Train Loss = 0.4036; Validation Loss = 0.4225\n",
      "Iteration 140: Train Loss = 0.4034; Validation Loss = 0.4225\n",
      "Iteration 141: Train Loss = 0.4033; Validation Loss = 0.4225\n",
      "Iteration 142: Train Loss = 0.4031; Validation Loss = 0.4223\n",
      "Iteration 143: Train Loss = 0.4031; Validation Loss = 0.4223\n",
      "Iteration 144: Train Loss = 0.4030; Validation Loss = 0.4222\n",
      "Iteration 145: Train Loss = 0.4029; Validation Loss = 0.4221\n",
      "Iteration 146: Train Loss = 0.4028; Validation Loss = 0.4221\n",
      "Iteration 147: Train Loss = 0.4028; Validation Loss = 0.4221\n",
      "Iteration 148: Train Loss = 0.4028; Validation Loss = 0.4222\n",
      "Iteration 149: Train Loss = 0.4026; Validation Loss = 0.4221\n",
      "Iteration 150: Train Loss = 0.4025; Validation Loss = 0.4220\n",
      "Iteration 151: Train Loss = 0.4025; Validation Loss = 0.4219\n",
      "Iteration 152: Train Loss = 0.4024; Validation Loss = 0.4219\n",
      "Iteration 153: Train Loss = 0.4023; Validation Loss = 0.4219\n",
      "Iteration 154: Train Loss = 0.4022; Validation Loss = 0.4219\n",
      "Iteration 155: Train Loss = 0.4021; Validation Loss = 0.4217\n",
      "Iteration 156: Train Loss = 0.4021; Validation Loss = 0.4217\n",
      "Iteration 157: Train Loss = 0.4019; Validation Loss = 0.4217\n",
      "Iteration 158: Train Loss = 0.4018; Validation Loss = 0.4216\n",
      "Iteration 159: Train Loss = 0.4017; Validation Loss = 0.4216\n",
      "Iteration 160: Train Loss = 0.4016; Validation Loss = 0.4215\n"
     ]
    }
   ],
   "source": [
    "best_boosting = Boosting(\n",
    "    base_model_params={\n",
    "        'max_depth': best_params['max_depth'],\n",
    "        'min_samples_leaf': best_params['min_samples_leaf']\n",
    "    },\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    early_stopping_rounds=10,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "best_boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "y_pred_proba_boost = best_boosting.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=42)\n",
    "logistic_regression.fit(x_train, y_train)\n",
    "\n",
    "prob_gb = best_boosting.predict_proba(x_test)[:, 1]\n",
    "prob_lr = logistic_regression.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fraction_of_positives_gb, mean_predicted_value_gb = calibration_curve(y_test, prob_gb, n_bins=10)\n",
    "fraction_of_positives_lr, mean_predicted_value_lr = calibration_curve(y_test, prob_lr, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_predicted_value_gb, fraction_of_positives_gb, label='Gradient Boosting')\n",
    "plt.plot(mean_predicted_value_lr, fraction_of_positives_lr, label='Logistic Regression')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O6bjp2MlWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь оценим важность признаков для градиентного бустинга.\n",
    "\n",
    "**Задание:**\n",
    "1. Поскольку базовая модель — дерево из `sklearn`, вычислите важность каждого признака для каждого дерева, используя атрибут `feature_importances_` у `DecisionTreeRegressor`.\n",
    "2. Усредните значения важности по всем деревьям и нормализуйте их так, чтобы сумма была равна единице (убедитесь, что значения неотрицательны).\n",
    "3. Дополните вашу реализацию бустинга, добавив метод `feature_importances_`, который будет возвращать усредненные и нормализованные важности признаков.\n",
    "\n",
    "**Построение графиков:**\n",
    "1. Постройте столбчатую диаграмму важности признаков для градиентного бустинга.\n",
    "2. На соседнем графике изобразите важность признаков для логистической регрессии, используя модули весов.\n",
    "3. Сравните графики и проанализируйте полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y0JNgBk_lWlW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Feature Index')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "coef_lr = np.abs(logistic_regression.coef_[0])\n",
    "coef_lr_norm = coef_lr / coef_lr.sum()\n",
    "\n",
    "feat_imp_boost = best_boosting.feature_importances_\n",
    "feat_imp_lr = coef_lr_norm\n",
    "\n",
    "features = np.arange(len(feat_imp_boost))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "axes[0].bar(features, feat_imp_boost)\n",
    "axes[0].set_title(\"Boosting Feature Importances\")\n",
    "axes[0].set_xlabel(\"Feature Index\")\n",
    "axes[0].set_ylabel(\"Importance\")\n",
    "\n",
    "axes[1].bar(features, feat_imp_lr)\n",
    "axes[1].set_title(\"Logistic Regression Coefficients (abs, normalized)\")\n",
    "axes[1].set_xlabel(\"Feature Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xheSi2IolWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обычно избыточные признаки могут негативно влиять на качество бустинга. Попробуйте следующее:\n",
    "\n",
    "1. **Отфильтруйте неважные признаки:** Используйте построенную диаграмму важности признаков, чтобы отобрать наиболее незначительные признаки.\n",
    "2. **Обучите модель повторно:** Обучите модель на основе оставшихся признаков с теми же гиперпараметрами.\n",
    "3. **Оцените качество модели:** Сравните результаты новой модели с исходной. Улучшилось ли качество после отфильтровывания незначительных признаков?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Rw9W9MEYlWlW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen features (indexes): [  0   1   2   4   5   7   9  10  12  15  20  21  24  25  27  35  36  65\n",
      "  69  71  76  81  83  92 121 135 142 143 149 154 155 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168]\n",
      "Count of chosen features: 43 from 169\n",
      "Shape of filtered x_train: (18825, 43)\n",
      "Shape of filtered x_valid: (2354, 43)\n",
      "Shape of filtered x_test: (2353, 43)\n"
     ]
    }
   ],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ\n",
    "\n",
    "threshold = np.percentile(feat_imp_boost, 75)\n",
    "\n",
    "important_features = np.where(feat_imp_boost >= threshold)[0]\n",
    "\n",
    "print(f\"Chosen features (indexes): {important_features}\")\n",
    "print(f\"Count of chosen features: {len(important_features)} from {len(feat_imp_boost)}\")\n",
    "\n",
    "selected_features = important_features\n",
    "\n",
    "x_train_filtered = x_train[:, selected_features]\n",
    "x_valid_filtered = x_valid[:, selected_features]\n",
    "x_test_filtered = x_test[:, selected_features]\n",
    "\n",
    "print(f\"Shape of filtered x_train: {x_train_filtered.shape}\")\n",
    "print(f\"Shape of filtered x_valid: {x_valid_filtered.shape}\")\n",
    "print(f\"Shape of filtered x_test: {x_test_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Train Loss = 0.6757; Validation Loss = 0.6764\n",
      "Iteration 2: Train Loss = 0.6596; Validation Loss = 0.6608\n",
      "Iteration 3: Train Loss = 0.6447; Validation Loss = 0.6462\n",
      "Iteration 4: Train Loss = 0.6307; Validation Loss = 0.6329\n",
      "Iteration 5: Train Loss = 0.6179; Validation Loss = 0.6203\n",
      "Iteration 6: Train Loss = 0.6058; Validation Loss = 0.6086\n",
      "Iteration 7: Train Loss = 0.5947; Validation Loss = 0.5977\n",
      "Iteration 8: Train Loss = 0.5844; Validation Loss = 0.5878\n",
      "Iteration 9: Train Loss = 0.5746; Validation Loss = 0.5783\n",
      "Iteration 10: Train Loss = 0.5654; Validation Loss = 0.5695\n",
      "Iteration 11: Train Loss = 0.5569; Validation Loss = 0.5615\n",
      "Iteration 12: Train Loss = 0.5489; Validation Loss = 0.5540\n",
      "Iteration 13: Train Loss = 0.5415; Validation Loss = 0.5469\n",
      "Iteration 14: Train Loss = 0.5346; Validation Loss = 0.5404\n",
      "Iteration 15: Train Loss = 0.5278; Validation Loss = 0.5342\n",
      "Iteration 16: Train Loss = 0.5218; Validation Loss = 0.5283\n",
      "Iteration 17: Train Loss = 0.5159; Validation Loss = 0.5227\n",
      "Iteration 18: Train Loss = 0.5104; Validation Loss = 0.5174\n",
      "Iteration 19: Train Loss = 0.5052; Validation Loss = 0.5124\n",
      "Iteration 20: Train Loss = 0.5005; Validation Loss = 0.5080\n",
      "Iteration 21: Train Loss = 0.4959; Validation Loss = 0.5038\n",
      "Iteration 22: Train Loss = 0.4917; Validation Loss = 0.4997\n",
      "Iteration 23: Train Loss = 0.4877; Validation Loss = 0.4961\n",
      "Iteration 24: Train Loss = 0.4838; Validation Loss = 0.4923\n",
      "Iteration 25: Train Loss = 0.4802; Validation Loss = 0.4890\n",
      "Iteration 26: Train Loss = 0.4768; Validation Loss = 0.4859\n",
      "Iteration 27: Train Loss = 0.4736; Validation Loss = 0.4828\n",
      "Iteration 28: Train Loss = 0.4706; Validation Loss = 0.4800\n",
      "Iteration 29: Train Loss = 0.4677; Validation Loss = 0.4772\n",
      "Iteration 30: Train Loss = 0.4649; Validation Loss = 0.4745\n",
      "Iteration 31: Train Loss = 0.4623; Validation Loss = 0.4720\n",
      "Iteration 32: Train Loss = 0.4596; Validation Loss = 0.4698\n",
      "Iteration 33: Train Loss = 0.4573; Validation Loss = 0.4675\n",
      "Iteration 34: Train Loss = 0.4550; Validation Loss = 0.4653\n",
      "Iteration 35: Train Loss = 0.4530; Validation Loss = 0.4634\n",
      "Iteration 36: Train Loss = 0.4509; Validation Loss = 0.4614\n",
      "Iteration 37: Train Loss = 0.4490; Validation Loss = 0.4596\n",
      "Iteration 38: Train Loss = 0.4471; Validation Loss = 0.4580\n",
      "Iteration 39: Train Loss = 0.4453; Validation Loss = 0.4564\n",
      "Iteration 40: Train Loss = 0.4435; Validation Loss = 0.4549\n",
      "Iteration 41: Train Loss = 0.4419; Validation Loss = 0.4535\n",
      "Iteration 42: Train Loss = 0.4405; Validation Loss = 0.4521\n",
      "Iteration 43: Train Loss = 0.4389; Validation Loss = 0.4507\n",
      "Iteration 44: Train Loss = 0.4376; Validation Loss = 0.4496\n",
      "Iteration 45: Train Loss = 0.4363; Validation Loss = 0.4483\n",
      "Iteration 46: Train Loss = 0.4351; Validation Loss = 0.4472\n",
      "Iteration 47: Train Loss = 0.4338; Validation Loss = 0.4461\n",
      "Iteration 48: Train Loss = 0.4327; Validation Loss = 0.4450\n",
      "Iteration 49: Train Loss = 0.4314; Validation Loss = 0.4440\n",
      "Iteration 50: Train Loss = 0.4305; Validation Loss = 0.4432\n",
      "Iteration 51: Train Loss = 0.4295; Validation Loss = 0.4423\n",
      "Iteration 52: Train Loss = 0.4284; Validation Loss = 0.4414\n",
      "Iteration 53: Train Loss = 0.4275; Validation Loss = 0.4407\n",
      "Iteration 54: Train Loss = 0.4267; Validation Loss = 0.4399\n",
      "Iteration 55: Train Loss = 0.4258; Validation Loss = 0.4392\n",
      "Iteration 56: Train Loss = 0.4251; Validation Loss = 0.4385\n",
      "Iteration 57: Train Loss = 0.4244; Validation Loss = 0.4380\n",
      "Iteration 58: Train Loss = 0.4236; Validation Loss = 0.4372\n",
      "Iteration 59: Train Loss = 0.4230; Validation Loss = 0.4365\n",
      "Iteration 60: Train Loss = 0.4223; Validation Loss = 0.4360\n",
      "Iteration 61: Train Loss = 0.4216; Validation Loss = 0.4355\n",
      "Iteration 62: Train Loss = 0.4209; Validation Loss = 0.4348\n",
      "Iteration 63: Train Loss = 0.4202; Validation Loss = 0.4342\n",
      "Iteration 64: Train Loss = 0.4195; Validation Loss = 0.4336\n",
      "Iteration 65: Train Loss = 0.4190; Validation Loss = 0.4331\n",
      "Iteration 66: Train Loss = 0.4185; Validation Loss = 0.4327\n",
      "Iteration 67: Train Loss = 0.4181; Validation Loss = 0.4323\n",
      "Iteration 68: Train Loss = 0.4176; Validation Loss = 0.4319\n",
      "Iteration 69: Train Loss = 0.4171; Validation Loss = 0.4315\n",
      "Iteration 70: Train Loss = 0.4167; Validation Loss = 0.4312\n",
      "Iteration 71: Train Loss = 0.4162; Validation Loss = 0.4308\n",
      "Iteration 72: Train Loss = 0.4157; Validation Loss = 0.4303\n",
      "Iteration 73: Train Loss = 0.4153; Validation Loss = 0.4299\n",
      "Iteration 74: Train Loss = 0.4149; Validation Loss = 0.4297\n",
      "Iteration 75: Train Loss = 0.4145; Validation Loss = 0.4293\n",
      "Iteration 76: Train Loss = 0.4141; Validation Loss = 0.4289\n",
      "Iteration 77: Train Loss = 0.4138; Validation Loss = 0.4286\n",
      "Iteration 78: Train Loss = 0.4133; Validation Loss = 0.4282\n",
      "Iteration 79: Train Loss = 0.4129; Validation Loss = 0.4279\n",
      "Iteration 80: Train Loss = 0.4125; Validation Loss = 0.4276\n",
      "Iteration 81: Train Loss = 0.4122; Validation Loss = 0.4274\n",
      "Iteration 82: Train Loss = 0.4119; Validation Loss = 0.4272\n",
      "Iteration 83: Train Loss = 0.4115; Validation Loss = 0.4269\n",
      "Iteration 84: Train Loss = 0.4112; Validation Loss = 0.4267\n",
      "Iteration 85: Train Loss = 0.4110; Validation Loss = 0.4266\n",
      "Iteration 86: Train Loss = 0.4107; Validation Loss = 0.4263\n",
      "Iteration 87: Train Loss = 0.4103; Validation Loss = 0.4261\n",
      "Iteration 88: Train Loss = 0.4100; Validation Loss = 0.4257\n",
      "Iteration 89: Train Loss = 0.4096; Validation Loss = 0.4254\n",
      "Iteration 90: Train Loss = 0.4093; Validation Loss = 0.4252\n",
      "Iteration 91: Train Loss = 0.4090; Validation Loss = 0.4249\n",
      "Iteration 92: Train Loss = 0.4089; Validation Loss = 0.4248\n",
      "Iteration 93: Train Loss = 0.4087; Validation Loss = 0.4247\n",
      "Iteration 94: Train Loss = 0.4085; Validation Loss = 0.4245\n",
      "Iteration 95: Train Loss = 0.4083; Validation Loss = 0.4244\n",
      "Iteration 96: Train Loss = 0.4081; Validation Loss = 0.4242\n",
      "Iteration 97: Train Loss = 0.4080; Validation Loss = 0.4242\n",
      "Iteration 98: Train Loss = 0.4077; Validation Loss = 0.4240\n",
      "Iteration 99: Train Loss = 0.4074; Validation Loss = 0.4238\n",
      "Iteration 100: Train Loss = 0.4072; Validation Loss = 0.4237\n",
      "Iteration 101: Train Loss = 0.4070; Validation Loss = 0.4236\n",
      "Iteration 102: Train Loss = 0.4068; Validation Loss = 0.4234\n",
      "Iteration 103: Train Loss = 0.4066; Validation Loss = 0.4233\n",
      "Iteration 104: Train Loss = 0.4064; Validation Loss = 0.4232\n",
      "Iteration 105: Train Loss = 0.4063; Validation Loss = 0.4231\n",
      "Iteration 106: Train Loss = 0.4061; Validation Loss = 0.4230\n",
      "Iteration 107: Train Loss = 0.4060; Validation Loss = 0.4229\n",
      "Iteration 108: Train Loss = 0.4058; Validation Loss = 0.4228\n",
      "Iteration 109: Train Loss = 0.4056; Validation Loss = 0.4228\n",
      "Iteration 110: Train Loss = 0.4055; Validation Loss = 0.4227\n",
      "Iteration 111: Train Loss = 0.4053; Validation Loss = 0.4225\n",
      "Iteration 112: Train Loss = 0.4051; Validation Loss = 0.4224\n",
      "Iteration 113: Train Loss = 0.4050; Validation Loss = 0.4223\n",
      "Iteration 114: Train Loss = 0.4048; Validation Loss = 0.4222\n",
      "Iteration 115: Train Loss = 0.4047; Validation Loss = 0.4221\n",
      "Iteration 116: Train Loss = 0.4046; Validation Loss = 0.4220\n",
      "Iteration 117: Train Loss = 0.4045; Validation Loss = 0.4220\n",
      "Iteration 118: Train Loss = 0.4043; Validation Loss = 0.4219\n",
      "Iteration 119: Train Loss = 0.4042; Validation Loss = 0.4218\n",
      "Iteration 120: Train Loss = 0.4041; Validation Loss = 0.4218\n",
      "Iteration 121: Train Loss = 0.4040; Validation Loss = 0.4218\n",
      "Iteration 122: Train Loss = 0.4039; Validation Loss = 0.4217\n",
      "Iteration 123: Train Loss = 0.4038; Validation Loss = 0.4216\n",
      "Iteration 124: Train Loss = 0.4036; Validation Loss = 0.4215\n",
      "Iteration 125: Train Loss = 0.4035; Validation Loss = 0.4216\n",
      "Iteration 126: Train Loss = 0.4034; Validation Loss = 0.4215\n",
      "Iteration 127: Train Loss = 0.4032; Validation Loss = 0.4215\n",
      "Iteration 128: Train Loss = 0.4031; Validation Loss = 0.4214\n",
      "Iteration 129: Train Loss = 0.4030; Validation Loss = 0.4214\n",
      "Iteration 130: Train Loss = 0.4029; Validation Loss = 0.4213\n",
      "Iteration 131: Train Loss = 0.4027; Validation Loss = 0.4213\n",
      "Iteration 132: Train Loss = 0.4025; Validation Loss = 0.4212\n",
      "Iteration 133: Train Loss = 0.4025; Validation Loss = 0.4212\n",
      "Iteration 134: Train Loss = 0.4023; Validation Loss = 0.4212\n",
      "Iteration 135: Train Loss = 0.4022; Validation Loss = 0.4210\n",
      "Iteration 136: Train Loss = 0.4020; Validation Loss = 0.4209\n",
      "Iteration 137: Train Loss = 0.4020; Validation Loss = 0.4209\n",
      "Iteration 138: Train Loss = 0.4020; Validation Loss = 0.4210\n",
      "Iteration 139: Train Loss = 0.4018; Validation Loss = 0.4210\n",
      "Iteration 140: Train Loss = 0.4017; Validation Loss = 0.4208\n",
      "Iteration 141: Train Loss = 0.4016; Validation Loss = 0.4207\n",
      "Iteration 142: Train Loss = 0.4014; Validation Loss = 0.4207\n",
      "Iteration 143: Train Loss = 0.4013; Validation Loss = 0.4206\n",
      "Iteration 144: Train Loss = 0.4011; Validation Loss = 0.4206\n",
      "Iteration 145: Train Loss = 0.4011; Validation Loss = 0.4206\n",
      "Iteration 146: Train Loss = 0.4009; Validation Loss = 0.4205\n",
      "Iteration 147: Train Loss = 0.4008; Validation Loss = 0.4205\n",
      "Iteration 148: Train Loss = 0.4007; Validation Loss = 0.4205\n",
      "Iteration 149: Train Loss = 0.4006; Validation Loss = 0.4204\n",
      "Iteration 150: Train Loss = 0.4006; Validation Loss = 0.4204\n",
      "Iteration 151: Train Loss = 0.4005; Validation Loss = 0.4204\n",
      "Iteration 152: Train Loss = 0.4005; Validation Loss = 0.4204\n",
      "Iteration 153: Train Loss = 0.4003; Validation Loss = 0.4203\n",
      "Iteration 154: Train Loss = 0.4003; Validation Loss = 0.4204\n",
      "Iteration 155: Train Loss = 0.4002; Validation Loss = 0.4204\n",
      "Iteration 156: Train Loss = 0.4001; Validation Loss = 0.4204\n",
      "Iteration 157: Train Loss = 0.4000; Validation Loss = 0.4203\n",
      "Iteration 158: Train Loss = 0.4000; Validation Loss = 0.4203\n",
      "Iteration 159: Train Loss = 0.3999; Validation Loss = 0.4203\n",
      "Iteration 160: Train Loss = 0.3998; Validation Loss = 0.4203\n"
     ]
    }
   ],
   "source": [
    "boosting_filtered = Boosting(\n",
    "    base_model_params={\n",
    "        \"max_depth\": best_params['max_depth'],\n",
    "        \"min_samples_leaf\": best_params['min_samples_leaf']\n",
    "    },\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    early_stopping_rounds=10,\n",
    "    plot=False\n",
    "    )\n",
    "\n",
    "boosting_filtered.fit(x_train_filtered, y_train, x_valid_filtered, y_valid)\n",
    "\n",
    "y_pred_proba_boost_filtered = boosting_filtered.predict_proba(x_test_filtered)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Boosting Test ROC-AUC: 0.9640\n",
      "Reduced Boosting Test ROC-AUC: 0.9636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_default = best_boosting.score(x_test, y_test)\n",
    "\n",
    "roc_auc_filtered = roc_auc_score(y_test, y_pred_proba_boost_filtered)\n",
    "\n",
    "print(f\"Original Boosting Test ROC-AUC: {roc_auc_default:.4f}\")\n",
    "print(f\"Reduced Boosting Test ROC-AUC: {roc_auc_filtered:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH3489RklWlW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 5 (бонус). Блендинговое [0.5 балла]\n",
    "\n",
    "Реализуйте блендинг над вашей лучшей моделью и логистической регрессией. Улучшилось ли качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWcDMMVilWlW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eEF0yJ4vlWlX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 6 (бонус). Катбустовое [0.5 балла]\n",
    "\n",
    "Запустите [CatBoost](https://catboost.ai/en/docs/concepts/python-quickstart) на наших данных, сравните с вашей реализацией. Где получилось лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lYGhc_clWlX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqHzTXCllZO8"
   },
   "source": [
    "Оставьте пожалуйста отзыв о курсе!\n",
    "\n",
    "https://forms.gle/LajA3Xrps6u96Q5A8\n",
    "\n",
    "\n",
    "Это очень важно. Благодаря обратной связи мы будем двигаться в сторону антиградиента)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "492px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
